////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////


:org-name: Application Services
:community:
:imagesdir: ./images
:product-version: 1
:product-long: OpenShift Streams for Apache Kafka
:product: Streams for Apache Kafka
:registry-product-long: OpenShift Service Registry
:registry: Service Registry
// Placeholder URL, when we get a HOST UI for the service we can put it here properly
:service-url: https://console.redhat.com/application-services/streams/
:registry-url: https://console.redhat.com/beta/application-services/service-registry/
:property-file-name: app-services.properties

// Other upstream project names
:samples-git-repo: https://github.com/redhat-developer/app-services-guides

//URL components for cross refs
:base-url: https://github.com/redhat-developer/app-services-guides/blob/main/
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:getting-started-url: getting-started/README.adoc
:getting-started-service-registry-url: getting-started-service-registry/README.adoc
:kafka-bin-scripts-url: kafka-bin-scripts/README.adoc
:kafkacat-url: kafkacat/README.adoc
:quarkus-url: quarkus/README.adoc
:quarkus-service-registry-url: quarkus-service-registry/README.adoc
:rhoas-cli-url: rhoas-cli/README.adoc
:rhoas-cli-kafka-url: rhoas-cli-kafka/README.adoc
:rhoas-cli-service-registry-url: rhoas-cli-service-registry/README.adoc
:rhoas-cli-ref-url: commands
:topic-config-url: topic-configuration/README.adoc
:consumer-config-url: consumer-configuration/README.adoc
:service-binding-url: service-discovery/README.adoc
:access-mgmt-url: access-mgmt/README.adoc
:access-mgmt-service-registry-url: access-mgmt-service-registry/README.adoc

////
END GENERATED ATTRIBUTES
////

[id="chap-monitoring-metrics"]
= Monitoring metrics in {product-long}
ifdef::context[:parent-context: {context}]
:context: monitoring-metrics

// Purpose statement for the assembly
[role="_abstract"]
As a developer or administrator, you can view metrics in {product} to visualize the performance and data usage for Kafka instances and topics that you have access to. You can view metrics directly in the {product} web console, or use the metrics API endpoint provided by {product} to import the data into your own metrics monitoring tool, such as Prometheus.

//Additional line break to resolve mod docs generation error, not sure why. Leaving for now. (Stetson, 20 May 2021)

[id="ref-supported-metrics_{context}"]
== Supported metrics in {product}

{product} supports the following metrics for Kafka instances and topics. In the {product} web console, a subset of these metrics is displayed in the *Dashboard* page of each Kafka instance.

Broker metrics::
+
--
* `kafka_broker_quota_softlimitbytes`: Maximum amount of broker storage before producers are throttled

* `kafka_broker_quota_totalstorageusedbytes`: Total storage used by the broker

* `kafka_controller_kafkacontroller_global_partition_count`: Number of partitions across all topics in the cluster

* `kafka_controller_kafkacontroller_offline_partitions_count`: Number of partitions without an active leader and that are therefore not writable or readable

* `kafka_namespace:haproxy_server_bytes_in_total:rate5m`: *<@SME: Description?>*

* `kafka_namespace:haproxy_server_bytes_out_total:rate5m`: *<@SME: Description?>*

* `kafka_namespace:kafka_server_socket_server_metrics_connection_count:sum`: Number of client connections to the broker

* `kafka_namespace:kafka_server_socket_server_metrics_connection_creation_rate:sum`: Number of attempted client connections per second for the broker

* `kubelet_volume_stats_available_bytes`: Available disk space in the broker

* `kubelet_volume_stats_used_bytes`: Used disk space in the broker
--

Topic metrics::
+
--
* `kafka_server_brokertopicmetrics_bytes_in_total`: Number of incoming bytes to topics in the instance

* `kafka_server_brokertopicmetrics_bytes_out_total`: Number of outgoing bytes from topics in the instance

* `kafka_server_brokertopicmetrics_messages_in_total`: Number of messages per second received by one or more topics in the instance

* `kafka_topic:kafka_log_log_size:sum`: Log size for each partition in a topic *<@SME: This correct?>*

* `kafka_topic:kafka_topic_partitions:count`: Number of partitions in a topic *<@SME: This correct?>*

* `kafka_topic:kafka_topic_partitions:sum`: *<@SME: Description? I don't see the diff between this and "partitions:count".>*
--

Scrape metrics::
+
--
*<SME: Description for these? I can kind of guess on a couple, but not clear.>*

* `scrape_duration_seconds`:

* `scrape_samples_post_metric_relabeling`:

* `scrape_samples_scraped`:

* `scrape_series_added`:
--

[id="proc-viewing-metrics_{context}"]
== Viewing metrics for a Kafka instance in {product}

After you produce and consume messages in your services using methods such as link:https://kafka.apache.org/downloads[Kafka] scripts, link:https://github.com/edenhill/kcat[Kafkacat], or a link:https://quarkus.io/[Quarkus] application, you can return to the Kafka instance in the web console and use the *Dashboard* page to view metrics for the instance and topics. The metrics help you understand the performance and data usage for your Kafka instance and topics.

.Prerequisites
* You have access to a Kafka instance in {product} that contains topics. For more information about access management in {product}, see {base-url}{access-mgmt-url}[_Managing account access in {product-long}_^].
* You or other users have produced and consumed messages in the Kafka topics. For example, to use Kafka scripts to produce and consume messages, see {base-url}{kafka-bin-scripts-url}[_Configuring and connecting Kafka scripts with {product-long}_^].

.Procedure
* In the *Kafka Instances* page of the web console, click the name of the Kafka instance and select the *Dashboard* tab.
+
--
When you create your Kafka instance and topics, the *Dashboard* page is initially empty. After you start producing and consuming messages in your services, you can return to this page to view related metrics.

NOTE: In some cases, after you start producing and consuming messages, you might need to wait several minutes for the latest metrics to appear. You might also need to wait until your instance and topics contain enough data for metrics to appear.

--

[id="proc-configuring-metrics-prometheus_{context}"]
== Configuring metrics monitoring for a Kafka instance in Prometheus

As an alternative to viewing metrics for a Kafka instance in the {product} web console, you can export your metrics to https://prometheus.io/docs/introduction/overview/[Prometheus] and integrate the metrics with your own metrics monitoring platform. {product} provides a `kafkas/{id}/metrics/federate` API endpoint that you can configure as a scrape target for Prometheus to use to collect and store metrics. You can then access the metrics in the https://prometheus.io/docs/visualization/browser/[Prometheus expression browser] or in a data-graphing tool such as https://prometheus.io/docs/visualization/grafana/[Grafana].

This procedure follows the https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/additional-scrape-config.md#additional-scrape-configuration[Additional Scrape Configuration] method defined by Prometheus for integrating third-party metrics.

.Prerequisites
* You have access to a Kafka instance that contains topics in {product}. For more information about access management in {product}, see {base-url}{access-mgmt-url}[_Managing account access in {product-long}_^].
* You have the generated credentials for your service account that has access to the Kafka instance. To regenerate and re-copy the credentials, use the *Service Accounts* page in the {product} web console to find your service account and update the credentials. For more information about service accounts, see {base-url}{getting-started-url}#proc-creating-service-account_getting-started[_Getting started with {product-long}_^].
* You or other users have produced and consumed messages in the Kafka topics. For example, to use Kafka scripts to produce and consume messages, see {base-url}{kafka-bin-scripts-url}[_Configuring and connecting Kafka scripts with {product-long}_^].
* You've installed a Prometheus instance. For installation instructions, see https://prometheus.io/docs/prometheus/latest/getting_started/[Getting Started] in the Prometheus documentation.
* You've installed the https://kubernetes.io/docs/tasks/tools/#kubectl[kubectl] Kubernetes command-line tool.

.Procedure
. In your Prometheus instance, create the following `kafka-federate.yaml` file. Replace the values with your own Kafka instance and credential information.
+
--
The `<kafka_instance_id>` is the ID of the Kafka instance. The `<client_id>` and `<client_secret>` are the generated credentials for your service account.

.Example `kafka-federate.yaml` file
[source,yaml,subs="+quotes"]
----
- job_name: "kafka-federate"
  static_configs:
  - targets: ["api.openshift.com"]
  scheme: "https"
  metrics_path: "/api/kafkas_mgmt/v1/kafkas/__<kafka_instance_id>__/metrics/federate"
  oauth2:
    client_id: "__<client_id>__"
    client_secret: "__<client_secret>__"
    token_url: "https://identity.api.openshift.com/auth/realms/rhoas/protocol/openid-connect/token"
----

Alternatively, you can add this information to your Prometheus configuration file, as shown in the following example. For more information about this method, see https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file[Configuration File] in the Prometheus documentation.

.Alternative method to update Prometheus configuration
[source,yaml,subs="+quotes"]
----
- job_name: "kafka-federate"
  static_configs:
  - targets: ["api.openshift.com"]
  scheme: "https"
  metrics_path: "/api/kafkas_mgmt/v1/kafkas/__<kafka_instance_id>__/metrics/federate"
  oauth2:
    client_id: "__<client_id>__"
    client_secret: "__<client_secret>__"
    token_url: "https://identity.api.openshift.com/auth/realms/rhoas/protocol/openid-connect/token"
----
--
. Create a Kubernetes secret that contains the new `kafka-federate.yaml` file, as shown in the following example. Replace `<namespace>` with the value of your own Kubernetes namespace.
+
.Example command to create a Kubernetes secret
[source,subs="+quotes"]
----
kubectl create secret generic additional-scrape-configs --from-file=kafka-federate.yaml --dry-run -o yaml \
kubectl apply -f - -n __<namespace>__
----
. In the Prometheus custom resource of your Kubernetes cluster, reference the new secret as shown in the following example.
+
--
.Example Prometheus custom resource with new secret
[source,subs="+quotes"]
----
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
    ...
spec:
    ...
    additionalScrapeConfigs:
        name: additional-scrape-configs
        key: kafka-federate.yaml
----

The new scrape target becomes available after the configuration has reloaded.

You can view your collected metrics in the Prometheus expression browser at `http://__<host>__:__<port>__/graph`, or integrate your Prometheus data source with a data-graphing tool such as Grafana. For information about Prometheus metrics in Grafana, see https://prometheus.io/docs/visualization/grafana/[Grafana Support for Prometheus] in the Grafana documentation.
--

[role="_additional-resources"]
.Additional resources
* {base-url}{getting-started-url}[_Getting started with {product-long}_^]
* {base-url}{rhoas-cli-kafka-url}[_Getting started with the `rhoas` CLI for {product-long}_^]
* {base-url-cli}{rhoas-cli-ref-url}[_CLI command reference (rhoas)_^]
* https://prometheus.io/docs/prometheus/latest/getting_started/[Getting Started] in the Prometheus documentation
* https://prometheus.io/docs/visualization/grafana/[Grafana Support for Prometheus]
* https://grafana.com/docs/grafana/latest/datasources/prometheus/[Prometheus Data Source] in the Grafana documentation

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
