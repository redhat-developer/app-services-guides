////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////


:org-name: Application Services
:community:
:imagesdir: ./images
:product-version: 1
:product-long: OpenShift Streams for Apache Kafka
:product: Streams for Apache Kafka
:registry-product-long: OpenShift Service Registry
:registry: Service Registry
// Placeholder URL, when we get a HOST UI for the service we can put it here properly
:service-url: https://console.redhat.com/application-services/streams/
:registry-url: https://console.redhat.com/application-services/service-registry/
:property-file-name: app-services.properties

// Other upstream project names
:samples-git-repo: https://github.com/redhat-developer/app-services-guides

//URL components for cross refs
:base-url: https://github.com/redhat-developer/app-services-guides/blob/main/
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:getting-started-url: getting-started/README.adoc
:getting-started-service-registry-url: getting-started-service-registry/README.adoc
:kafka-bin-scripts-url: kafka-bin-scripts/README.adoc
:kafkacat-url: kafkacat/README.adoc
:quarkus-url: quarkus/README.adoc
:quarkus-service-registry-url: quarkus-service-registry/README.adoc
:rhoas-cli-url: rhoas-cli/README.adoc
:rhoas-cli-kafka-url: rhoas-cli-kafka/README.adoc
:rhoas-cli-service-registry-url: rhoas-cli-service-registry/README.adoc
:rhoas-cli-ref-url: commands
:topic-config-url: topic-configuration/README.adoc
:consumer-config-url: consumer-configuration/README.adoc
:service-binding-url: service-discovery/README.adoc
:access-mgmt-url: access-mgmt/README.adoc
:access-mgmt-service-registry-url: access-mgmt-service-registry/README.adoc

////
END GENERATED ATTRIBUTES
////

[id="chap-monitoring-metrics"]
= Monitoring metrics in {product-long}
ifdef::context[:parent-context: {context}]
:context: monitoring-metrics

// Purpose statement for the assembly
[role="_abstract"]
As a developer or administrator, you can view metrics in {product} to visualize the performance and data usage for Kafka instances and topics that you have access to. You can view metrics directly in the {product} web console, or use the metrics API endpoint provided by {product} to import the data into your own metrics monitoring tool, such as Prometheus.

//Additional line break to resolve mod docs generation error, not sure why. Leaving for now. (Stetson, 20 May 2021)

[id="ref-supported-metrics_{context}"]
== Supported metrics in {product}

{product} supports the following metrics for Kafka instances and topics. In the {product} web console, a subset of these metrics is displayed in the *Dashboard* page of each Kafka instance.

Broker metrics::
+
--
* `kafka_broker_quota_softlimitbytes`: Maximum amount of broker storage before producers are throttled

* `kafka_broker_quota_totalstorageusedbytes`: Total storage used by the broker

* `kafka_controller_kafkacontroller_global_partition_count`: Number of partitions for each broker in the instance

* `kafka_controller_kafkacontroller_offline_partitions_count`: Number of partitions without an active leader and that are therefore not writable or readable

* `kafka_namespace:haproxy_server_bytes_in_total:rate5m`: Incoming byte rate for the instance in the last five minutes

* `kafka_namespace:haproxy_server_bytes_out_total:rate5m`: Outgoing byte rate for the instance in the last five minutes

* `kafka_namespace:kafka_server_socket_server_metrics_connection_count:sum`: Number of client connections to the instance

* `kafka_namespace:kafka_server_socket_server_metrics_connection_creation_rate:sum`: Number of attempted client connections per second for the instance

* `kubelet_volume_stats_available_bytes`: Available disk space in the instance

* `kubelet_volume_stats_used_bytes`: Used disk space in the instance
--

Topic metrics::
+
--
* `kafka_server_brokertopicmetrics_bytes_in_total`: Number of incoming bytes to topics in the instance

* `kafka_server_brokertopicmetrics_bytes_out_total`: Number of outgoing bytes from topics in the instance

* `kafka_server_brokertopicmetrics_messages_in_total`: Number of messages per second received by one or more topics in the instance

* `kafka_topic:kafka_log_log_size:sum`: Log size of all topics in the instance

* `kafka_topic:kafka_topic_partitions:count`: Number of topics in the instance

* `kafka_topic:kafka_topic_partitions:sum`: Number of partitions across all topics in the instance
--

[id="proc-viewing-metrics_{context}"]
== Viewing metrics for a Kafka instance in {product}

After you produce and consume messages in your services using methods such as link:https://kafka.apache.org/downloads[Kafka] scripts, link:https://github.com/edenhill/kcat[Kafkacat], or a link:https://quarkus.io/[Quarkus] application, you can return to the Kafka instance in the web console and use the *Dashboard* page to view metrics for the instance and topics. The metrics help you understand the performance and data usage for your Kafka instance and topics.

.Prerequisites
* You have access to a Kafka instance in {product} that contains topics. For more information about access management in {product}, see {base-url}{access-mgmt-url}[_Managing account access in {product-long}_^].

.Procedure
* In the *Kafka Instances* page of the web console, click the name of the Kafka instance and select the *Dashboard* tab.
+
--
When you create a Kafka instance and add new topics, the *Dashboard* page is initially empty. After you start producing and consuming messages in your services, you can return to this page to view related metrics. For example, to use Kafka scripts to produce and consume messages, see {base-url}{kafka-bin-scripts-url}[_Configuring and connecting Kafka scripts with {product-long}_^].

NOTE: In some cases, after you start producing and consuming messages, you might need to wait several minutes for the latest metrics to appear. You might also need to wait until your instance and topics contain enough data for metrics to appear.

--

[id="proc-configuring-metrics-prometheus_{context}"]
== Configuring metrics monitoring for a Kafka instance in Prometheus

As an alternative to viewing metrics for a Kafka instance in the {product} web console, you can export your metrics to https://prometheus.io/docs/introduction/overview/[Prometheus] and integrate the metrics with your own metrics monitoring platform. {product} provides a `kafkas/{id}/metrics/federate` API endpoint that you can configure as a scrape target for Prometheus to use to collect and store metrics. You can then access the metrics in the https://prometheus.io/docs/visualization/browser/[Prometheus expression browser] or in a data-graphing tool such as https://prometheus.io/docs/visualization/grafana/[Grafana].

This procedure follows the https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file[Configuration File] method defined by Prometheus for integrating third-party metrics. If you use the Prometheus Operator in your monitoring environment, you can also follow the https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/additional-scrape-config.md#additional-scrape-configuration[Additional Scrape Configuration] method.

.Prerequisites
* You have access to a Kafka instance that contains topics in {product}. For more information about access management in {product}, see {base-url}{access-mgmt-url}[_Managing account access in {product-long}_^].
* You have the generated credentials for your service account that has access to the Kafka instance. To regenerate and re-copy the credentials, use the *Service Accounts* page in the {product} web console to find your service account and update the credentials. For more information about service accounts, see {base-url}{getting-started-url}#proc-creating-service-account_getting-started[_Getting started with {product-long}_^].
* You've installed a Prometheus instance in your monitoring environment. For installation instructions, see https://prometheus.io/docs/prometheus/latest/getting_started/[Getting Started] in the Prometheus documentation.

.Procedure
* In your Prometheus configuration file, add the following information. Replace `<kafka_instance_id>` with the ID of the Kafka instance. Replace `<client_id>` and `<client_secret>` with the generated credentials for your service account.
+
--
.Required information for Prometheus configuration file
[source,yaml,subs="+quotes"]
----
- job_name: "kafka-federate"
  static_configs:
  - targets: ["api.openshift.com"]
  scheme: "https"
  metrics_path: "/api/kafkas_mgmt/v1/kafkas/__<kafka_instance_id>__/metrics/federate"
  oauth2:
    client_id: "__<client_id>__"
    client_secret: "__<client_secret>__"
    token_url: "https://identity.api.openshift.com/auth/realms/rhoas/protocol/openid-connect/token"
----

The new scrape target becomes available after the configuration has reloaded.

You can view your collected metrics in the Prometheus expression browser at `http://__<host>__:__<port>__/graph`, or integrate your Prometheus data source with a data-graphing tool such as Grafana. For information about Prometheus metrics in Grafana, see https://prometheus.io/docs/visualization/grafana/[Grafana Support for Prometheus] in the Grafana documentation.

When you create a Kafka instance and add new topics, the metrics are initially empty. After you start producing and consuming messages in your services, you can return to your monitoring tool to view related metrics. For example, to use Kafka scripts to produce and consume messages, see {base-url}{kafka-bin-scripts-url}[_Configuring and connecting Kafka scripts with {product-long}_^].

NOTE: In some cases, after you start producing and consuming messages, you might need to wait several minutes for the latest metrics to appear. You might also need to wait until your instance and topics contain enough data for metrics to appear.

[NOTE]
====
If you use the Prometheus Operator in your monitoring environment, you can alternatively create a `kafka-federate.yaml` file as an additional scrape configuration in your Prometheus custom resource as shown in the following example commands. For more information about this method, see https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/additional-scrape-config.md#additional-scrape-configuration[Additional Scrape Configuration] in the Prometheus documentation.

.Example `kafka-federate.yaml` file
[source,yaml,subs="+quotes"]
----
- job_name: "kafka-federate"
  static_configs:
  - targets: ["api.openshift.com"]
  scheme: "https"
  metrics_path: "/api/kafkas_mgmt/v1/kafkas/__<kafka_instance_id>__/metrics/federate"
  oauth2:
    client_id: "__<client_id>__"
    client_secret: "__<client_secret>__"
    token_url: "https://identity.api.openshift.com/auth/realms/rhoas/protocol/openid-connect/token"
----

.Example command to create and apply a Kubernetes secret
[source,subs="+quotes"]
----
kubectl create secret generic additional-scrape-configs --from-file=__<~/kafka-federate.yaml>__ --dry-run -o yaml \
kubectl apply -f - -n __<namespace>__
----

.Example Prometheus custom resource with new secret
[source,subs="+quotes"]
----
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
    ...
spec:
    ...
    additionalScrapeConfigs:
        name: additional-scrape-configs
        key: kafka-federate.yaml
----
====

--

[role="_additional-resources"]
.Additional resources
* {base-url}{getting-started-url}[_Getting started with {product-long}_^]
* {base-url}{rhoas-cli-kafka-url}[_Getting started with the `rhoas` CLI for {product-long}_^]
* {base-url-cli}{rhoas-cli-ref-url}[_CLI command reference (rhoas)_^]
* https://prometheus.io/docs/prometheus/latest/getting_started/[Getting Started] in the Prometheus documentation
* https://prometheus.io/docs/visualization/grafana/[Grafana Support for Prometheus]
* https://grafana.com/docs/grafana/latest/datasources/prometheus/[Prometheus Data Source] in the Grafana documentation

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
