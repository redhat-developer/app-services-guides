[id="chap-using-servicediscovery"]
=  Connecting {product-long} Service to your OpenShift cluster
ifdef::context[:parent-context: {context}]
:context: using-servicediscovery

////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////


:community:
:imagesdir: ./images
:product-long: Application Services
:product: App Services
// Placeholder URL, when we get a HOST UI for the service we can put it here properly
:service-url: https://cloud.redhat.com/beta/application-services/streams/
:property-file-name: app-services.properties

// Other upstream project names
:samples-git-repo: https://github.com/redhat-developer/app-services-guides

////
END GENERATED ATTRIBUTES
////

[IMPORTANT]
====
{product-long} is currently available for Development Preview. Development Preview releases provide early access to a limited set of features that might not be fully tested and that might change in the final GA version. Users should not use Development Preview software in production or for business-critical workloads. Limited documentation is available for Development Preview releases and is typically focused on fundamental user goals.
====

// Purpose statement for the assembly
[role="_abstract"]
As a developer of applications and services, you can connect Quarkus applications to Kafka instances in {product-long}. https://quarkus.io/[Quarkus^] is a Kubernetes-native Java framework made for Java virtual machines (JVMs) and native compilation, and optimized for serverless, cloud, and Kubernetes environments. Quarkus is designed to work with popular Java standards, frameworks, and libraries like Eclipse MicroProfile and Spring, as well as Apache Kafka, RESTEasy (JAX-RS), Hibernate ORM (JPA), Infinispan, Camel, and many more.

OpenShift is an Enteprise Kuberentes Platform. When deploying Quarkus and any other type of Applications developers can use 
https://developers.redhat.com/blog/2019/12/19/introducing-the-service-binding-operator/[ServiceBinding] capability that would let 
their applications to discover configuration automatically inside their OpenShift or Kubernetes instance.

RHOAS CLI contains set of commands that provide capability to connect and bind your OpenShift deployed applications with {product-long} services.
You can review all possible commands in https://github.com/redhat-developer/app-services-cli/blob/main/docs/commands/rhoas_cluster.adoc[cli documentation].

This quick start shows you how to connect Red Hat OpenShift Streams for Apache Kafka to OpenShift using the RHOAS CLI.
    
In this Quick Start, you'll connect your OpenShift Streams for Apache Kafka cloud service to your OpenShift cluster using the Red Hat OpenShift Application Services. (RHOAS) Operator and CLI. For more information on Red Hat OpenShift Application Services, please visit [cloud.redhat.com.](https://cloud.redhat.com/application-services)

.Prerequisites
ifndef::community[]
* You have a Red Hat account.
endif::[]
* You have a running Kafka instance in {product}.

// Condition out QS-only content so that it doesn't appear in docs.
// All QS anchor IDs must be in this alternate anchor ID format `[#anchor-id]` because the ascii splitter relies on the other format `[id="anchor-id"]` to generate module files.
ifdef::qs[]
[#description]
Using the {product-long} services in your OpenShift cluster{product-long}.

[#introduction]
Welcome to the {product-long} Service Discovery quick start. In this quick start, you'll learn how to connect your {product} services with https://quarkus.io/[Quarkus^]. Tutorial will connect Kafka with your Application to produce and consume messages from your Kafka instances in {product}.
endif::[]

[id="proc-installing-cli_{context}"]
== Installing required cli tools

RHOAS operator allow you to represent {product} services as first class citizens in your OpenShift environment.
This enables you to work and integrate with these services using standard OpenShift/Kubernetes features and APIs.
    
To connect your OpenShift project to your Red Hat OpenShift Application Services, you can use the RHOAS CLI.  
In this step we going to install RHOAS and OpenShift cli. You can skip this step entirely if you have those CLI
already installed.

.Prerequisites
* You have bash/shell environment that can be used to download and install required tools

.Procedure
. Install oc command line by following https://docs.openshift.com/container-platform/4.7/cli_reference/openshift_cli/getting-started-cli.html#installing-openshift-cli[official guide]
. Install RHOAS CLI by downloading binaries from the latest release https://github.com/redhat-developer/app-services-cli/releases/latest

ifdef::qs[]
.Verification
* Do you see help printed when running oc command in your terminal
* Do you see help printed when running rhoas command in your terminal
endif::[]

[id="proc-installing-operator_{context}"]
== Installing RHOAS Operator on your OpenShift cluster

RHOAS Operator creates instances of {product} Services in your cluster. 
Operator connects with {product} API giving you ability to embed connection details to various services.

Operator currently enables developers to connect with their Red Hat OpenShift Streams for Apache Kafka (RHOSAK) instances.
Operator is used by RHOAS CLI to connect your services with your OpenShift cluster

[.diagram]
.Operator and CLI relationships
image::rhoas-operator.png[Operator and CLI relationships]

NOTE: RHOAS Operator is already installed on various environments like OpenShift Developer Sandbox: https://developers.redhat.com/developer-sandbox

NOTE: Installation procedure assumes that you have access to install operators in openshift-namespace on your cluster. 

.Prerequisites
* You have login access to OpenShift cluster
* You have cluster admin rights on OpenShift cluster in order to install Operator

.Procedure
. Please login to your OpenShift UI instance (if you do not have OpenShift instance you can use https://developers.redhat.com/developer-sandbox and skip this procedure entirely)

. Navigate to side menu and select `Administrator` > `Operators`> `Operator Hub`
. Type `RHOAS` in search tab 
. Select `RHOAS` tile 
+
[.instruction]
.Operator tab
image::operator-install.png[Operator]
. Leave select `All namespaces` option checked and Press install to install your operator
. Please wait for operator to install.

ifdef::qs[]
.Verification
* Is RHOAS Operator installation finished successfully
endif::[]

[id="proc-inspecting-operator_{context}"]
== Setting up RHOAS CLI to use RHOAS operator

RHOAS CLI `cluster` commands provide number of commands that work directly with your own OpenShift cluster.
To use cluster command developer needs to have https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/[kuberentesconfig] that can be set using oc or kubectl command.
This config contains information about your active cluster and namespace that will be used by RHOAS CLI to perform operations on the cluster

`rhoas cluster status` command can be used to verify RHOAS operator installation.

The **RHOAS** CLI uses the **oc** CLI to determine the OpenShift instance, and project, that the Streams for Apache Kafka instance should be connected to. Let's verify that your **oc** client is properly connected to your OpenShift project.

.Prerequisites
* You have login access to OpenShift cluster
* You have ability to create new namespace and deploy new applications to your OpenShift cluster

.Procedure
. Login to your OpenShift cluster using `oc login` command line tool
+
[source]
----
oc login
----
. Create new namespace `rhoas-quarkus` that will be used for this guide
+
[source]
----
oc new-project rhoas-quarkus
----
+
. Login to rhoas CLI
+
[source]
----
rhoas login
----
+
. On the command line, enter  `rhoas cluster status` command and execute check if CLI has proper connection to cluster
+
[source]
----
rhoas cluster status
----
 
ifdef::qs[]
.Verification
* Is RHOAS Operator installation finished successfully
* Please inspect output of the command and check if RHOAS Operator is installed.
* Output from the command contains: "RHOAS Operator: Installed"
* Namespace is `rhoas-quarkus`
endif::[]

[id="proc-connecting-kafka_{context}"]
== Connecting Apache Kafka Service to your cluster

Now we can connect one of our services to our cluster. 
`rhoas cluster connect` command will let us to connect our Kafka instance to our cluster. If you do not have any Kafka instance you can create new one using `rhoas kafka create` command.

.Procedure
. Execute
+
[source]
----
rhoas cluster connect --ignore-context
---- 
+
. You will be asked to select the Kafka instance you want to connect.
Select instance and Press **Enter** to continue.
. The CLI will print the **Connection Details** and asks you to confirm. 
. Verify if namespace that will be used to create service instance is `rhoas-quarkus`
. Type `y` and press **Enter** to continue.
. You will be asked to provide a token, which again can be retrieved from https://cloud.redhat.com/openshift/token . Navigate to this URL, copy the token to your clipboard, and copy it into your terminal. Press **Enter** to continue.
. You should see the message: **KafkaConnection successfully installed on your cluster.**
. To verify that the connection has been successfully created, execute the following **oc** command: `oc get KafkaConnection`. This should return a **KafkaConnection** with the name of your Kafka instance.

.Verification
ifdef::qs[]
* `oc get KafkaConnection` returned created kafka instance

In case of issues you can remove your KafkaConnection by executing 
`oc delete akc name-of-your-kafka` and retry operation
endif::[]

[id="proc-create-prices-topic_{context}"]
== Creating the prices Kafka topic in {product}

For this quick start, the Kafka topic that the Quarkus example application references is called `prices`. You need to create this topic in {product} so that the Quarkus application can interact with it.

.Prerequisites
* You've created a Kafka instance in {product} and the instance is in *Ready* state.

.Procedure
. In the *Streams for Apache Kafka* page of the web console, select the name of the Kafka instance that you want to add a topic to.
. Click *Create topic* and follow the guided steps to define the topic details. Click *Next* to complete each step and click *Finish* to complete the setup.
+
[.screencapture]
.Guided steps to define topic details
image::sak-create-topic.png[Image of wizard to create a topic]

. *Topic name*: Enter `prices` as the topic name.
. *Partitions*: Set the number of partitions for this topic. This example sets the partition to `1` for a single partition. Partitions are distinct lists of messages within a topic and enable parts of a topic to be distributed over multiple brokers in the cluster. A topic can contain one or more partitions, enabling producer and consumer loads to be scaled.
+
NOTE: You can increase the number of partitions later, but you cannot decrease them.
+

* *Message retention*: Set the message retention time to the relevant value and increment. This example sets the retention to `7 days`. Message retention time is the amount of time that messages are retained in a topic before they are deleted or compacted, depending on the cleanup policy.
+
After you complete the topic setup, the new Kafka topic is listed in the topics table. You can now run the Quarkus application to start producing and consuming messages to and from this topic.

.Verification
ifdef::qs[]
* Is the new Kafka topic `prices` listed in the topics table?
endif::[]
ifndef::qs[]
* Verify that the new Kafka topic `prices` is listed in the topics table.
endif::[]

[id="proc-running-quarkus-example-application_{context}"]
== Starting the Quarkus example application

In this section of the guide we going to deploy Quarkus Example application container image 
The Quarkus example application in this quick start will connect with Kafka prices topic and generate random numbers between 0 and 100 and produces it to a Kafka topic. Another part of the application consumes the number from the Kafka topic. Applicalition exposes the number as a REST UI (using Server Sent events).

.Prerequisites
* You've created the `prices` example Kafka topic.

.Procedure
. Execute quarkus application template that will be deployed to your namespace
+
[source]
----
oc apply -f https://raw.githubusercontent.com/redhat-developer/app-services-guides/main/code-examples/quarkus-kafka-quickstart/.kubernetes/kubernetes.yml
---- 
+
. Fetch route to the application by executing
+
[source]
----
oc get route
---- 
+
. Open url from **HOST/PORT** section in your default browser
. Verify that you see Quarkus landing page
. If the Quarkus application fails to run, review the error log in the terminal and address any problems. Also review the steps in this quick start to ensure that the Quarkus application and Kafka topic are configured correctly.
. Append `/prices.html` to url to see prices section.
. You should see `N/A` as price as we still did not connected our application to our Kafka instance

ifdef::qs[]
.Verification
* Did the Quarkus example application run without any errors?
* Can you view `prices.html` page
endif::[]

[id="proc-binding-kafka_{context}"]
== Connecting your service with running application

Once application is running we can now connect our service using `rhoas cluster bind` command. This command will let us inject credentials to our application as files. Credentials will be by default injected as volume into our kuberentes deployment. Quarkus Kuberentes Client will detect them and automatically configure our application for us.

Bind command will create following structure in your pod that many frameworks like quarkus can automatically read and autoconfigure your application to connect with the Kafka and other services.

[source]
----
/bindings/yourkafka
├── bootstrapServers
├── password
├── provider
├── saslMechanism
├── securityProtocol
├── type
└── user
----

.Procedure
. Execute 
+
[source]
----
rhoas cluster bind
---- 
+
Command will automatically detect our application and single Kafka service in our namespace and inject connection details to the running application.
. Select application we want to connect with.
. Press enter to select it on the list
. Verify that command finished with **Binding succeeded** message
. Please go back to your app `prices.html` page
. You should see prices changing on the webpage


ifdef::qs[]
.Verification
* Command should end up with **Binding succeeded**
* Can you view `prices.html` page and chaging prices
endif::[]

ifdef::qs[]
[#conclusion]
Congratulations! You successfully completed the {product} Service Discovery quick start, and are now ready to deploy and connect services to your own applications with {product}.
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
