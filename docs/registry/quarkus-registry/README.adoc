////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//All OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/

//OpenShift Application Services CLI
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:command-ref-url-cli: commands
:installation-guide-url-cli: rhoas/rhoas-cli-installation/README.adoc
:service-contexts-url-cli: rhoas/rhoas-service-contexts/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:getting-started-rhoas-cli-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc
:message-browsing-url-kafka: kafka/message-browsing-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:getting-started-rhoas-cli-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:product-long-connectors: OpenShift Connectors
:product-connectors: Connectors
:product-version-connectors: 1
:service-url-connectors: https://console.redhat.com/application-services/connectors
:getting-started-url-connectors: connectors/getting-started-connectors/README.adoc

//OpenShift API Designer
:product-long-api-designer: OpenShift API Designer
:product-api-designer: API Designer
:product-version-api-designer: 1
:service-url-api-designer: https://console.redhat.com/application-services/api-designer/
:getting-started-url-api-designer: api-designer/getting-started-api-designer/README.adoc

//OpenShift API Management
:product-long-api-management: OpenShift API Management
:product-api-management: API Management
:product-version-api-management: 1
:service-url-api-management: https://console.redhat.com/application-services/api-management/

////
END GENERATED ATTRIBUTES
////

[id="chap-using-quarkus-registry"]
= Using Quarkus applications with Kafka instances and {product-long-registry}
ifdef::context[:parent-context: {context}]
:context: quarkus-service-registry

// Purpose statement for the assembly
[role="_abstract"]
As a developer of applications and services, you can connect Quarkus applications to Kafka instances in {product-long-kafka} and {registry} instances in {product-long-registry}. This makes it easy for development teams to store and reuse schemas in event streaming architectures.

https://quarkus.io/[Quarkus^] is a Kubernetes-native Java framework made for Java virtual machines (JVMs) and native compilation, and optimized for serverless, cloud, and Kubernetes environments.

Quarkus is designed to work with popular Java standards, frameworks, and libraries like Eclipse MicroProfile and Spring, as well as Apache Kafka, RESTEasy (JAX-RS), Hibernate ORM (JPA), Infinispan, Camel, and many more.

.Prerequisites
ifndef::community[]
* You have a Red Hat account.
endif::[]
* You have a running Kafka instance in {product-kafka} (see {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^]).
* You have a running {registry} instance in {product-long-registry} (see {base-url}{getting-started-url-registry}[Getting started with {product-long-registry}^]).
* https://github.com/git-guides/[Git^] is installed.
* You have an IDE such as https://www.jetbrains.com/idea/download/[IntelliJ IDEA^], https://www.eclipse.org/downloads/[Eclipse^], or https://code.visualstudio.com/Download[VSCode^].
* https://adoptopenjdk.net/[OpenJDK^] 11 or later is installed on Linux or MacOS. (The latest LTS version of OpenJDK is recommended.)
* https://maven.apache.org/[Apache Maven^] 3.8.x or later is installed (for Quarkus 2.2.x).

// Condition out QS-only content so that it doesn't appear in docs.
// All QS anchor IDs must be in this alternate anchor ID format `[#anchor-id]` because the ascii splitter relies on the other format `[id="anchor-id"]` to generate module files.
ifdef::qs[]
[#description]
====
Learn how to use a Quarkus application that produces messages to and consume messages from a Kafka instance in {product-long-kafka} and manage the message schemas in {product-long-registry}.
====

[#introduction]
====
Welcome to the quick start for {product-long-registry} with Quarkus. In this quick start, you'll learn how to use https://quarkus.io/[Quarkus^] to produce messages to and consume messages from your Kafka instances in {product-kafka} and manage the message schemas in {product-long-registry}.
====
endif::[]

[id="proc-importing-quarkus-registry-sample-code_{context}"]
== Importing the Quarkus sample code

[role="_abstract"]
For this quick start, you'll use the Quarkus {registry} sample code from the App Services {samples-git-repo}[Guides and Samples^] repository in GitHub. After you understand the concepts and tasks in this quick start, you can use your own Quarkus applications with {product-kafka} and {registry} in the same way.

.Procedure
. On the command line, clone the App Services {samples-git-repo}[Guides and Samples^] repository from GitHub.
+
.Cloning the guides and samples repository
[source,subs="+attributes"]
----
git clone {samples-git-repo} app-services-guides
----
. In your IDE, open the `code-examples/quarkus-service-registry-quickstart` directory from the repository that you cloned.

ifdef::qs[]
.Verification
* Is the Quarkus example application accessible in your IDE?
endif::[]

[id="proc-configuring-quarkus-registry-app_{context}"]
== Configuring the Quarkus application to connect to Kafka and {registry} instances

[role="_abstract"]
To enable your Quarkus applications to access a Kafka instance, configure the connection properties using the Kafka bootstrap server endpoint. To access a {registry} instance, configure the registry endpoint connection property with the Core Registry API value.

Access to the {registry} and Kafka instances is managed using the same service account and SASL/OAUTHBEARER token endpoint. For Quarkus, you can configure all connection properties using the `application.properties` file. This example sets environment variables and references them in this file.

Quarkus applications use https://github.com/eclipse/microprofile-reactive-messaging[MicroProfile Reactive Messaging^] to produce messages to and consume messages from your Kafka instances in {product-kafka}. For details on configuration options, see the https://quarkus.io/guides/kafka[Apache Kafka Reference Guide^] in the Quarkus documentation.

This Quarkus example application includes producer and consumer processes that serialize/deserialize Kafka messages using a schema stored in {registry}.

.Prerequisites
* You have a service account with write access to Kafka and {registry} instances and have stored your credentials securely (see {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^] and {base-url}{getting-started-url-registry}[Getting started with {product-long-registry}^]).
* You have the Kafka bootstrap server endpoint for the Kafka instance. You copied this information previously for the Kafka instance in {product-kafka} by selecting the options menu (three vertical dots) and clicking *Connection*.
* You have the Core Registry API endpoint for the {registry} instance. You copied this information for the {registry} instance by selecting the options menu (three vertical dots) and clicking *Connection*. From the list of endpoints, you copied the *Core Registry API* endpoint supported by the Apicurio serializer/deserializer (SerDes) used in this example.
* You copied the *Token endpoint URL* value from the same list of endpoints to be used for the OAuth-based athentication method used in this example.

.Procedure
. On the command line, set the following environment variables to use your Kafka and {registry} instances with Quarkus or other applications. Replace the values with your own server and credential information:
+
* The `<bootstrap_server>` is the bootstrap server endpoint for your Kafka instance.
* The `<service_registry_url>` is the URL for your {registry} instance.
* The `<service_registry_core_path>` is the constant we use to for setting proper API path for service registry
* The `<oauth_token_endpoint_uri>` is the SASL/OAUTHBEARER token endpoint.
* The `<client_id>` and `<client_secret>` are the generated credentials for your service account.
+
.Setting environment variables for server and credentials
[source,subs="+quotes"]
----
$ export KAFKA_HOST=__<bootstrap_server>__
$ export SERVICE_REGISTRY_URL=__<service_registry_url>__
$ export SERVICE_REGISTRY_CORE_PATH=/apis/registry/v2
$ export RHOAS_OAUTH_TOKEN_URL=__<oauth_token_endpoint_uri>__
$ export RHOAS_CLIENT_ID=__<client_id>__
$ export RHOAS_CLIENT_SECRET=__<client_secret>__
----

. In the Quarkus example application, review the `/src/main/resources/application.properties` files in the `consumer` and `producer` sub-folders to understand how the environment variables you set in the previous step are used. This example uses the `dev` configuration profile in the `application.properties` files.

ifdef::qs[]
.Verification
* Did you set the required environment variables for accessing your services?
endif::[]

[id="proc-create-prices-topic-registry_{context}"]
== Creating the quotes Kafka topic in {product-kafka}

[role="_abstract"]
For this quick start, the Kafka topic that the Quarkus example application uses is called `quotes`. You must create this topic in {product-kafka} so that the Quarkus application can interact with it.

.Prerequisites
* You're logged in to the {product-kafka} web console at {service-url-kafka}[^].
* You've created a Kafka instance in {product-kafka} and the instance is in *Ready* state.

.Procedure
. In the {product-kafka} web console, go to *Streams for Apache Kafka* > *Kafka Instances* and click the name of the Kafka instance that you want to add a topic to.
. Select the *Topics* tab, click *Create topic*, and follow the guided steps to define the topic details. Click *Next* to complete each step and click *Finish* to complete the setup.
+
[.screencapture]
.Guided steps to define topic details
image::sak-create-topic.png[Image of wizard to create a topic]

* *Topic name*: Enter `quotes` as the topic name.
* *Partitions*: Set the number of partitions for this topic. This example sets the partition to `1` for a single partition. Partitions are distinct lists of messages in a topic and enable parts of a topic to be distributed over multiple brokers in the cluster. A topic can contain one or more partitions, enabling producer and consumer loads to be scaled.
* *Message retention*: Set the message retention time and size to the relevant value and increment. This example sets the retention time to `A week` and the retention size to `Unlimited`. Message retention time is the amount of time that messages are retained in a topic before they are deleted or compacted, depending on the cleanup policy. Retention size is the maximum total size of all log segments in a partition before they are deleted or compacted.
* *Replicas*: For this release of {product-kafka}, the replicas are preconfigured. The number of partition replicas for the topic is set to `3` and the minimum number of follower replicas that must be in sync with a partition leader is set to `2`.
+
Replicas are copies of partitions in a topic. Partition replicas are distributed over multiple brokers in the cluster to ensure topic availability if a broker fails. When a follower replica is in sync with a partition leader, the follower replica can become the new partition leader if needed.
+
After you complete the topic setup, the new Kafka topic is listed in the topics table. You can now run the Quarkus application to start producing and consuming messages using this topic.

.Verification
ifdef::qs[]
* Is the new `quotes` Kafka topic listed in the topics table?
endif::[]
ifndef::qs[]
* Verify that the new `quotes` Kafka topic is listed in the topics table.
endif::[]


[id="proc-running-quarkus-registry-example-app_{context}"]
== Running the Quarkus example application

[role="_abstract"]
After you configure your Quarkus application to connect to Kafka and {registry} instances, and you create the Kafka topic, you can run the Quarkus application to start producing and consuming messages to and from this topic.

The Quarkus application in this quick start consists of two processes:

* The consumer process is implemented by the `QuotesResource` class. This class exposes the `/quotes` REST endpoint that streams quotes from the `quotes` topic. This process also has a minimal frontend that streams quotes using Server-Sent Events to the web page.
* The producer process is implemented by the `QuotesProducer` class. This class produces a new quote periodically (every 5 seconds) with a random quote value that is published to the `quotes` topic.

.Prerequisites
* You've configured the Quarkus example application to connect to the Kafka and {registry} instances.
* You've created the Kafka `quotes` topic.
ifndef::qs[]
* You're logged in to the {registry} web console at {service-url-registry}[^].
endif::[]


.Procedure
. On the command line, change to the `code-examples/quarkus-service-registry-quickstart/consumer` directory that you imported and run the consumer process.
+
.Running the example consumer process
[source]
----
$ cd ~/code-examples/quarkus-service-registry-quickstart/consumer
$ mvn quarkus:dev
----
. After the consumer process is running, in a web browser, go to http://localhost:8080/quotes.html[^] and verify that this process is available.

. Leave the consumer process running, and run the producer process on a different terminal.
+
.Running the example producer process
[source]
----
$ cd ~/code-examples/quarkus-service-registry-quickstart/producer
$ mvn quarkus:dev
----

. When both the consumer and producer processes are running, view the generated quotes in the web browser at http://localhost:8080/quotes.html[^].

. In the web console, go to *{registry}* > *{registry} Instances*,  select your {registry} instance, and view the automatically generated schema for your application.


.What just happened?

* The Quarkus application is configured to use the `io.apicurio.registry.serde.avro.AvroKafkaSerializer` Java class for serializing and the `io.apicurio.registry.serde.avro.AvroKafkaDeserializer` class for deserializing messages to Avro format. This SerDes is configured to use remote schemas in {product-long-registry} rather than the local schemas in the application.

* Because there are no schemas in the {registry} instance, the SerDes published the schema for the `quotes` topic. The name of the schema is managed by the `TopicRecordIdStrategy` class, which uses the `topic_name-value` convention. You can find this schema in the {registry} instance and configure compatability rules to govern how the schema can evolve for future versions.

* If the Quarkus application fails to run, review the error log in the terminal and address any problems. Also review the steps in this quick start to ensure that the Quarkus application and Kafka topic are configured correctly.

ifdef::qs[]
.Verification
* Did the Quarkus example application run without any errors?
* Did you see the generated quotes at http://localhost:8080/quotes.html[^]?
* Did you see generated schemas in the {registry} instance?
endif::[]

ifdef::qs[]
[#conclusion]
====
Congratulations! You successfully completed the {product-kafka} and {registry} Quarkus quick start, and are now ready to use your own Quarkus application with {product-kafka} and {registry}.
====
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
