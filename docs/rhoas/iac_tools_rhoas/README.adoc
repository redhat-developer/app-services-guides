////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//All OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/
:sso-token-url: https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
:cloud-console-url: https://console.redhat.com/
:service-accounts-url: https://console.redhat.com/application-services/service-accounts

//OpenShift Application Services CLI
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:command-ref-url-cli: commands
:installation-guide-url-cli: rhoas/rhoas-cli-installation/README.adoc
:service-contexts-url-cli: rhoas/rhoas-service-contexts/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:getting-started-rhoas-cli-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc
:message-browsing-url-kafka: kafka/message-browsing-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:getting-started-rhoas-cli-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:connectors: Connectors
:product-long-connectors: OpenShift Connectors
:product-connectors: Connectors
:product-version-connectors: 1
:service-url-connectors: https://console.redhat.com/application-services/connectors
:getting-started-url-connectors: connectors/getting-started-connectors/README.adoc
:getting-started-rhoas-cli-url-connectors: connectors/rhoas-cli-getting-started-connectors/README.adoc

//OpenShift API Designer
:product-long-api-designer: OpenShift API Designer
:product-api-designer: API Designer
:product-version-api-designer: 1
:service-url-api-designer: https://console.redhat.com/application-services/api-designer/
:getting-started-url-api-designer: api-designer/getting-started-api-designer/README.adoc

//OpenShift API Management
:product-long-api-management: OpenShift API Management
:product-api-management: API Management
:product-version-api-management: 1
:service-url-api-management: https://console.redhat.com/application-services/api-management/

////
END GENERATED ATTRIBUTES
////

[id="chap-using-iac-tools"]
= Using Infrastructure as Code tools with Kafka instances in {product-long-kafka}
ifdef::context[:parent-context: {context}]
:context: using-iac-tools

[role="_abstract"]
As a developer of applications and services, you can use Infrastructure as Code (IaC) tools to automate deployment and management of architectures.

{product-long-kafka} provides two REST APIs, one for accessing the Kafka instance and one for managing resources such as Kafka instances and services accounts. This guide describes how to use different IaC tools to provision and manage your {product-kafka} instances. Information on Kafka resources is provided to IaC tools listed in this guide by the Kafka APIs.

For more information on creating and setting up Kafka instances, see {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^].


//Additional line break to resolve mod docs generation error

[id="con-ansible_{context}"]
== Ansible

[role="_abstract"]
Ansible is an open source configuration management and automation tool. To learn more about Ansible, including how to install it, see https://www.redhat.com/en/topics/automation/learning-ansible-tutorial[Learning Ansible Basics^].

Ansible automates the management of remote systems and controls their desired state. Ansible works by connecting to your nodes and pushing out small programs - called modules - to these nodes. Modules are the core functionality of Ansible and are used to execute tasks. The output of one module can be the input for the next one or they can be independent of each other.

One or more Ansible modules can be combined to make a play. Two or more plays can be combined to create an Ansible Playbook, a list of sequential tasks (written in YAML) that automatically executes against hosts. Groups of hosts form your Ansible inventory. You can use an Ansible Playbook to describe the desired state of your infrastructure and then have Ansible provision it.

Ansible roles are a special kind of playbook that is fully self-contained and portable with the tasks, variables, configuration templates, and other supporting files that are needed to complete a complex orchestration. Multiple roles can exist inside a collection allowing easy sharing of content through Automation Hub and https://galaxy.ansible.com[Ansible Galaxy^].

//Additional line break to resolve mod docs generation error

[id="proc-install-rhoas-ansible-collection_{context}"]
=== Installing the RHOAS Ansible collection

[role="_abstract"]
You can use the https://galaxy.ansible.com/rhoas/rhoas[RHOAS Ansible collection] to fully manage your Kafka environment with Ansible. You can create, retrieve info, configure, deploy, and delete the following {product-long-kafka} resources:

* Kafka instances
* Service accounts
* Kafka topics
* Access Control Lists (ACLs)

.Prerequisites

* Python 3.9 or later is installed.
* https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html?extIdCarryOver=true&sc_cid=701f2000001Css5AAC[Ansible^] 2.9 or later is installed.
* This collection requires some additional dependencies to be installed. To do this, enter the following command.
+
[source,shell]
----
$ pip install rhoas-sdks
pip install python-dotenv
----
NOTE: Use `pip install rhoas-sdks --force reinstall`  to ensure that the most up to date versions of the SDKs are installed.
+

* (Optional) This collection works best when used in a Python virtual environment. To create and activate a Python virtual environment, enter the following command:
+
[source,shell]
----
$ python3 -m venv rhoas
. rhoas/bin/activate
----
+

.Procedure

. Open a terminal and enter the following command:
+
[source,shell]
----
$ ansible-galaxy collection install rhoas.rhoas
----
+
By default, ansible-galaxy collection install uses https://galaxy.ansible.com as the Galaxy server (as listed in the ansible.cfg file under GALAXY_SERVER). You do not need any further configuration. The install command automatically appends the path `ansible_collections` to the one specified with the `-p` option unless the parent directory is already in a folder called `ansible_collections`.
. Once installed, you can reference a collection content by its fully qualified collection name (FQCN).

NOTE: The `create_kafka` command in the task needs to be the FQCN of the module for collection development, for example
`rhoas.rhoas.create_kafka`.


[id="proc-using-rhoas-ansible-collection_{context}"]
=== Using the RHOAS Ansible collection

.Prerequisites
* You have a Red Hat account.
* You have an offline token that is accessible to Ansible.

[NOTE]
The offline token is a refresh token with no expiry and can be used by non-interactive processes to provide an access token for {product-long-rhoas} to authenticate the user. The token is an OpenShift offline token and can be found at https://cloud.redhat.com/openshift/token.

* Two further environment variables are used for the collection to work. These environment variables can be placed in a `.env` file. If neither of these environment variables are set, the collection will default to the URLs shown in the examples.
- API_BASE_HOST - The base host for the API. This is the base URL for the API. For example, https://api.openshift.com.
- SSO_BASE_HOST - The base host for the SSO. This is the base URL for the SSO. For example, https://sso.redhat.com/auth/realms/redhat-external.

.Procedure

. Once the collection is installed, documentation for each module can be found by entering `ansible-doc rhoas.rhoas.<module_name>` and replacing <module_name> with the name of the module. The following example outputs the documentation for the `create_kafka` module. It shows the required (denoted by `=`) and optional parameters (denoted by `-`) for the module, along with examples of how to use the module, and the output of the module.
+
[source,shell]
----
$ ansible-doc rhoas.rhoas.create_kafka
----
. You can run a single module to complete one task like in the following example which creates a Kafka instance.
+
.Example create_kafka module
[source,shell]
----
$ ansible localhost -m rhoas.rhoas.create_kafka -a 'name=unique-kafka-name billing_model=standard cloud_provider=aws plan="developer.x1" region="us-east-1" openshift_offline_token=<OFFLINE_TOKEN>'
----
. Alternatively, you can run a playbook. Enter the following command to run the https://github.com/redhat-developer/app-services-ansible/blob/main/rhoas_test.yml[rhoas test playbook].
+
[source,shell]
----
$ ansible-playbook rhoas_test.yml
----
[NOTE]
The openshift_offline_token is used for authentication to enable communication with the Kafka Management API. If the token is not passed in as an argument per task, the module attempts to read it from the `_OFFLINE_TOKEN_` environment variable.

+
NOTE: The example playbook used in this section includes comments that indicate how to directly specify values rather than fetching them dynamically. For example, to specify a Kafka instance ID, a comment in the playbook states that you can include the following line:
+
[source]
----
kafka_id: <kafka_id>
----

+

This playbook, if run as is, will complete the following tasks using these corresponding modules:
+
.. Provision a {product-kafka} instance using the `create_kafka` module. The only required fields for this module are the `name`, `billing_model` and `cloud provider` fields. You can specify configuration options such as `billing_cloud_account_id` and the `openshift_offline_token`. All other information for the instance is provided by the Kafka APIs.
+
[NOTE]
When a module is run, Ansible saves the output of that command in a variable in the `register` field. In the following example, Ansible saves the created Kafka instance as `_kafka_req_resp_`.
+

.Example `create_kafka` module
[source,yaml]
----
- name: Create kafka
    rhoas.rhoas.create_kafka:
      name: "kafka-name"
      instance_type: "x1"
      billing_model: "standard"
      cloud_provider: "aws"
      region: "us-east-1"
      plan: "developer.x1"
      billing_cloud_account_id: "123456789"
      openshift_offline_token: "OPENSHIFT_OFFLINE_TOKEN"
    register:
      kafka_req_resp
----

.. Create a service account using the `create_service_account` module to manage a service account. The only required fields are the `name` and `short description` fields. Ansible populates the generated service account credentials in the `client_id` and `client_secret` fields once it creates the service account.
+
.Example `create_service_account` module
[source,yaml]
----
- name: Create Service Account
    create_service_account:
      name: "service-account-name"
      description: "This is a description of the service account"
      openshift_offline_token: "OPENSHIFT_OFFLINE_TOKEN"
    register:
      srvce_acc_resp_obj
----

.. Create an ACL for the service account with some default values, and bind that ACL to the Kafka instance and service account using the `create_kafka_acl_binding` module. You can get the Kafka ID from the `_kafka_req_resp_` variable or enter it in the kafka_id field. You can get the service account ID from the `_srvce_acc_resp_obj_` variable. The following list is a description of each field that must have a value in an ACL binding module.
** `resource_name`: the name of resource you want access to. This example uses the name that is passed when creating the topic.
** `resource_type`: The type of resource you want access to. This example uses *Topic*.
** `pattern_type`: The type of pattern of the ACL.
** `operation_type`: The type of operation that is allowed for the given user on this module.
** `permission_type`: Whether permission is given or taken away.
+
.Example `create_kafka_acl_binding` module
[source,yaml]
----
- name: Create kafka ACL Service Binding
    rhoas.rhoas.create_kafka_acl_binding:
      kafka_id: "{{ kafka_req_resp.kafka_id }}"
      # To hardcode the kafka_id, uncomment and use the following line:
      # kafka_id: "KAFKA_ID"
      principal: " {{ srvce_acc_resp_obj['client_id'] }}"
      # To hardcode the principal_id, uncomment and use the following line:
      # principal: "PRINCIPAL_ID"
      resource_name: "topic-name"
      resource_type: "Topic"
      pattern_type: "PREFIXED"
      operation_type: "all"
      permission_type: "allow"
      openshift_offline_token: "OPENSHIFT_OFFLINE_TOKEN"
    register: kafka_acl_resp

----

.. Create a Kafka topic using the `create_kafka_topic` module. The `kafka_id` field is a required field.
+
.Example `create_kafka_topic` module
[source,yaml]
----
- name: Create Kafka Topic
    create_kafka_topic:
      topic_name: "kafka-topic-name"
      kafka_id: "{{ kafka_req_resp.id }}"
      # To hardcode the kafka_id, uncomment and use the following line:
      # kafka_id: "KAFKA_ID"
      partitions: 1
      retention_period_ms: "86400000"
      retention_size_bytes: "1073741824"
      cleanup_policy: "compact"
      openshift_offline_token: "OPENSHIFT_OFFLINE_TOKEN"
    register:
      create_topic_res_obj
----
+
.. Update the configuration of the first topic using the `update_kafka_topic` module. In the following example, the cleanup policy has been updated from compact to delete by replacing `"compact"` with `"delete"` in the `cleanup_policy` field. You can also update the `retention_period_ms` and `retention_size_bytes` fields.
+
.Example `update_kafka_topic` module
[source,yaml]
----
- name: Update Kafka Topic
    update_kafka_topic:
      topic_name: "kafka-topic-name"
      kafka_id: "{{ kafka_req_resp.id }}"
      # To hardcode the kafka_id, uncomment and use the following line:
      # kafka_id: "KAFKA_ID"
      partitions: 1
      retention_period_ms: "86400000"
      retention_size_bytes: "1073741824"
      cleanup_policy: "delete"
      openshift_offline_token: "OPENSHIFT_OFFLINE_TOKEN"
    register:
      update_topic_res_obj
----
+

.. Delete both created topics using the `delete_kafka_topic` module.
+
.Example `delete_kafka_topic` module
[source,yaml]
----
- name: Delete Kafka Topic
   rhoas.rhoas.delete_kafka_topic:
     topic_name: "KAFKA_TOPIC_NAME"
      kafka_id: "{{ kafka_req_resp_obj['kafka_id'] }}"
      # To hardcode the kafka_id, uncomment and use the following line:
      # kafka_id: "KAFKA_ID"
     openshift_offline_token: "OPENSHIFT_OFFLINE_TOKEN"
----

.. Delete the service account using the `delete_service_account_by_id` module.
+
.Example `delete_service_account_by_id` module
[source,yaml]
----
- name: Delete Service Account
   rhoas.rhoas.delete_service_account_by_id:
   # service_account_id: "service_account_id"
  service_account_id: "{{ srvce_acc_resp_obj['client_id'] }}"

  # openshift_offline_token: "OFFLINE_TOKEN"
----

.. Deprovision and delete the {product-kafka} instance using the `delete_kafka_by_id` module.
+
.Example `delete_kafka_by_id` module
[source]
----
- name: Delete kafka instance by ID
    rhoas.rhoas.delete_kafka_by_id:
     kafka_id: "{{ kafka_req_resp_obj['kafka_id'] }}"
     openshift_offline_token: "offline_token"
----

As the playbook deletes all created resources, some alteration of the playbook is necessary to keep them. For more information on writing playbooks, see the https://docs.ansible.com/ansible/latest/user_guide/playbooks_best_practices.html#playbooks-tips-and-tricks[Ansible documentation].

[id="con-terraform_{context}"]
== Terraform

link:https://www.terraform.io/[HashiCorp Terraform^] is an infrastructure as code tool that lets you build, change, and version infrastructure safely and efficiently through human-readable configuration files that you can version, reuse, and share. You can then use a consistent workflow to provision and manage all of your infrastructure throughout its lifecycle.

The link:https://registry.terraform.io/providers/redhat-developer/rhoas/latest[RHOAS Terraform^] provider is available in the official link:https://www.terraform.io/[Terraform provider registry^] and includes resources to interact with {product-long-rhoas}.

You can fully manage your Kafka environment through your Terraform system using the RHOAS Terraform provider. You can create, retrieve info, configure, deploy, and delete the following {product-kafka} resources:

* Kafka instances
* Service accounts
* Kafka topics
* Access Control Lists (ACLs)


[id="proc-using-rhoas-terraform-provider_{context}"]
=== Installing the RHOAS Terraform provider

.Prerequisites
* You have a Red Hat account.
* https://www.terraform.io/downloads[Terraform^] v1.3.4 or later is installed.
* You have an offline token that is accessible to Terraform.

[NOTE]
The offline token is a refresh token with no expiry and can be used by non-interactive processes to provide an access token for {product-long-rhoas} to authenticate the user. The token is an OpenShift offline token and can be found at https://cloud.redhat.com/openshift/token. As the offline token is a sensitive value that varies between environments it is best specified through as `_OFFLINE_TOKEN_` environment variable when running `terraform apply` in a terminal.

.Procedure
. Open your browser of choice and navigate to the RHOAS Terraform provider.
. Click *Use Provider*.
. Copy the code provided.
+

.Example rhoas provider configuration
[source,shell]
----
$ terraform {
  required_providers {
    rhoas = {
      source = "redhat-developer/rhoas"
      version = "0.3.0"
    }
  }
}

provider "rhoas" {
  #configuration options
}
----

. Open your IDE of choice and paste the code into it. You can specify configuration options in the provider settings. For example, you can enter the offline token here using the `offline_token` field. However as noted previously, it is recommended to enter the offline token as an environment variable.
+

. Save the file as a .tf file.
. Open a terminal and enter the following command in the Terraform directory. This command initializes the working directory containing Terraform configuration files and installs any required plug-ins.
+
[source,shell]
----
$ terraform init
----
A success message displays that `Terraform has been successfully initialized!`

[id="con-using-terraform_{context}"]
=== Using the RHOAS Terraform provider

Resources are the most important element in the Terraform language. Each resource block describes one or more infrastructure objects. Terraform has error messages that help with minor typing errors by suggesting a resource close to what is typed. The next steps show what resources (either for single tasks or a combination of tasks) you can add to your Terraform configuration file to provision your Kafka instance.

[id"proc-creating-kafka-instance-terraform_{context}"]
[discrete]
=== Creating a Kafka instance

[role="_abstract"]
To manage a Kafka instance using Terraform, use the `rhoas_kafka` resource.

.Prerequisites

* You have an offline token that is accessible to Terraform.
* You have a Red Hat account.

.Procedure

. Open your IDE of choice and enter the example rhoas_kafka resource. This example uses the “instance”  identifier and creates a Kafka instance called `my-instance`. The only required fields are the `name`, `billing_model` and `plan` fields. The following values are set by default when you run `terraform apply`. All other information for the instance is provided by the Kafka APIs.
+
.. `cloud provider`: A list of available cloud providers can be obtained using `data.rhoas_cloud_providers`.
.. `region`: A list of available regions can be obtained using
`data.rhoas_cloud_providers_regions`

+
.Example `rhoas_kafka` resource
[source,shell]
----
$ terraform {
  required_providers {
    rhoas = {
      source  = "registry.terraform.io/redhat-developer/rhoas"
      version = "0.3.0"
    }
  }
}

provider "rhoas" {}

resource "rhoas_kafka" "my-instance" {
  name = "my-instance"
}

output "bootstrap_server_my-instance" {
  value = rhoas_kafka.my-instance.bootstrap_server_host
}
----
. Open a terminal and initilize Terraform by entering the following command.
+
[source,shell]
----
$ terraform init
----
. Apply the changes by entering the following command.
+
[source,shell]
----
$ terraform apply
----
. Terraform displays a message that `rhoas_kafka.my-instance` will be created. Enter *yes*.
. Verify that the instance has been created in the the table on the *Kafka Instances* page of the {product-kafka} {service-url-kafka}[web console^].

[id="proc-creating-service-account-terraform_{context}"]
[discrete]
=== Creating a service account

[role="_abstract"]
After creating your Kafka instance, you create a service account. To connect your applications or services to a Kafka instance in {product-kafka}, you must first create a service account with credentials.

Use the `rhoas_service_account` resource to manage a service account. Terraform populates the credentials when the resource is applied. This example uses the ‘srvaccnt’ identifier and creates a service account called my-service-account.

.Prerequisites

* You have an offline token that is accessible to Terraform.
* You have a running Kafka instance in {product-kafka}.

.Procedure

. In your IDE, enter the example `rhoas_service_account` resource to create a service account.
+

.Example `rhoas_service_account` resource
[source,shell]
----
$ resource "rhoas_service_account" "my-instance" {
  name        = "my-service-account"
  description = "<description of service account>"
}
----
. Apply the changes by entering the following command. Note that you do not need to run `terraform init` if following on from the Create a Kafka instance step.
+
[source,shell]
----
$ terraform apply
----
Terraform populates the generated service account credentials in the `client_id` and `client_secret` fields.
. Terraform displays a message that `rhoas_kafka.my-service-account` will be created. Enter *yes*.
. Verify that the service account has been created in the the table on the *Service Accounts* page of the {product-kafka} {service-url-kafka}[web console^].

[id="proc-creating-kafka-topic-terraform_{context}"]
[discrete]
=== Creating a Kafka topic

[role="_abstract"]
Use the `rhoas_topic` resource to create a Kafka topic with default values. This example uses the `topic` identifier and creates the `my-topic` Kafka topic. You set the Kafka ID directly in the resource.

.Prerequisites

* You have an offline token that is accessible to Terraform.
* You have created a Kafka instance.

.Procedure

. Use the following example to create the `my-topic` Kafka topic with default values.
+
.Example `rhoas_topic` resource with default values
[source,shell]
----
$ resource ‘rhoas_topic’ “topic” {
		name = “my-topic”
		partitions = 1
		Kafka_id = rhoas_kafka.instance.id
	}

----
+
. Apply the changes by entering the following command.
+
[source,shell]
----
$ terraform apply
----
+
. Verify that the topic is listed on the *Topics* page of the {product-kafka} {service-url-kafka}[web console^].


[id="proc-creating-acl-binding-terraform_{context}"]
[discrete]
=== Creating an ACL binding

[role="_abstract"]
Use the `rhoas_acl` resource to create a ACL binding. This example uses the `acl` identifier. You set the Kafka id directly in the resource along with the topic name. The following list is a description of each field in an ACL binding resource.

* `resource_type`: The type of resource you want access to. This example uses *“TOPIC”*.
* `resource_name`: the name of resource you want access to. This example uses the name that is passed when creating the topic.
* `principal`: the user that this binding applies to. This example uses the service account client ID.
* `operation`: The type of operation that is allowed for the given user on this resource.
* `permission`: Whether permission is given or taken away.


.Prerequisites

* You have an offline token that is accessible to Terraform.
* You have created a Kafka instance and a topic.
* You have created a service account and know the client ID.

.Procedure

. Use the following example to create an ACL binding.

+
.Example ACL binding resource
[source,shell]
----
$ resource "resource_acl" "acl" {
  kafka_id = rhoas_kafka.instance.id
  resource_type = "TOPIC"
  resource_name = "my-topic"
  pattern_type = "LITERAL"
  principal = rhoas_service_account.srvcaccnt.client_id
  operation = "ALL"
  permission = "ALLOW"
}

----
. Apply the changes by entering the following command.
+
[source,shell]
----
$ terraform apply
----

[id="proc-performing-all-actions_{context}"]
[discrete]
=== Performing all actions

[role="_abstract"]
The following example is a Terraform configuration file that puts together all the previous tasks. It provisions a Kafka instance, creates a service account and topics, and creates an ACL binding.

.Example all actions Terraform configuration file
[source,shell]
----
$ terraform {
  required_providers {
    rhoas = {
        source  = "registry.terraform.io/redhat-developer/rhoas"
        version = "0.3.0"
    }
  }
}

provider "rhoas" {
    offline_token = "..."
}

resource "rhoas_service_account" "srvcaccnt" {
  name = "service_account"
}

resource "rhoas_kafka" "instance" {
  name = "instance"
  plan = "developer.x1"
  billing_model = "standard"
  acl = [
    {
      principal = rhoas_service_account.srvcaccnt.client_id,
      resource_type = "TOPIC",
      resource_name = "topic-1",
      pattern_type = "LITERAL",
      operation_type = "ALL",
      permission_type = "ALLOW",
    },
  ]
}

resource "rhoas_topic" "topic-1" {
  kafka_id = rhoas_kafka.instance.id
  name = "topic-1"
  partitions = 1
}

resource "rhoas_topic" "topic-2" {
  kafka_id = rhoas_kafka.instance.id
  name = "topic-2"
  partitions = 1
}

resource "rhoas_acl" "acl" {
  kafka_id = rhoas_kafka.instance.id
  principal = rhoas_service_account.srvcaccnt.client_id
  resource_type = "TOPIC"
  resource_name = "topic-2"
  pattern_type = "LITERAL"
  operation_type = "ALL"
  permission_type = "ALLOW"
}

data "rhoas_kafka" "instance_data" {
  id = rhoas_kafka.instance.id
}

data "rhoas_service_account" "srvcaccnt_data" {
  id = rhoas_service_account.srvcaccnt.id
}
----

[id="ref-data-sources-terraform_{context}"]
[discrete]
=== Data Sources

[role="_abstract"]
Data sources in Terraform are used to obtain information about resources external to Terraform, defined by another separate Terraform configuration, or modified by functions using the data block. For example, the `rhoas_kafkas` data source provides a list of the Kafka instances accessible to your organization in {product-long-kafka}.

.List of available rhoas Data Sources
* https://registry.terraform.io/providers/redhat-developer/rhoas/latest/docs/data-sources/cloud_provider_regions[rhoas_cloud_provider_regions (Data source)^]
* https://registry.terraform.io/providers/redhat-developer/rhoas/latest/docs/data-sources/cloud_providers[rhoas_cloud_providers (Data source)^]
* https://registry.terraform.io/providers/redhat-developer/rhoas/latest/docs/data-sources/kafka[rhoas_kafka (Data source)^]
* https://registry.terraform.io/providers/redhat-developer/rhoas/latest/docs/data-sources/kafkas[rhoas_kafkas (Data source)^]
* https://registry.terraform.io/providers/redhat-developer/rhoas/latest/docs/data-sources/service_account[rhoas_service_account (Data source)^]
* https://registry.terraform.io/providers/redhat-developer/rhoas/latest/docs/data-sources/service_accounts[rhoas_service_accounts (Data source)^]
* https://registry.terraform.io/providers/redhat-developer/rhoas/latest/docs/data-sources/topic[rhoas_topic (Data source)^]


[role="_additional-resources"]
== Additional resources
* {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^]
* https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/7d28aec8-e146-44db-a4a5-fafc1f426ca5[Configuring topics in {product-long-kafka}^]
* {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^]
* {base-url}{access-mgmt-url-kafka}[Managing account access in {product-long-kafka}^]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
