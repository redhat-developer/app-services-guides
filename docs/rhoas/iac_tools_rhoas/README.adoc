////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//All OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/
:sso-token-url: https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
:cloud-console-url: https://console.redhat.com/
:service-accounts-url: https://console.redhat.com/application-services/service-accounts

//to avoid typos
:openshift: OpenShift
:openshift-dedicated: OpenShift Dedicated

//OpenShift Application Services CLI
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:command-ref-url-cli: commands
:installation-guide-url-cli: rhoas/rhoas-cli-installation/README.adoc
:service-contexts-url-cli: rhoas/rhoas-service-contexts/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:getting-started-rhoas-cli-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc
:message-browsing-url-kafka: kafka/message-browsing-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:getting-started-rhoas-cli-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:connectors: Connectors
:product-long-connectors: OpenShift Connectors
:product-connectors: Connectors
:product-version-connectors: 1
:service-url-connectors: https://console.redhat.com/application-services/connectors
:getting-started-url-connectors: connectors/getting-started-connectors/README.adoc
:getting-started-rhoas-cli-url-connectors: connectors/rhoas-cli-getting-started-connectors/README.adoc

//OpenShift API Designer
:product-long-api-designer: OpenShift API Designer
:product-api-designer: API Designer
:product-version-api-designer: 1
:service-url-api-designer: https://console.redhat.com/application-services/api-designer/
:getting-started-url-api-designer: api-designer/getting-started-api-designer/README.adoc

//OpenShift API Management
:product-long-api-management: OpenShift API Management
:product-api-management: API Management
:product-version-api-management: 1
:service-url-api-management: https://console.redhat.com/application-services/api-management/

////
END GENERATED ATTRIBUTES
////

[id="chap-using-iac-tools"]
= Automating deployment and management of Kafka instances in {product-long-kafka}
ifdef::context[:parent-context: {context}]
:context: using-iac-tools

[role="_abstract"]
As a system administrator of applications and services, you can use Infrastructure as Code (IaC) tools to automate deployment and management of architectures.

This guide describes how to use different IaC tools to provision and manage Kafka instances in {product-long-kafka}. To interact with Kafka instances, the IaC tools use two REST APIs that are available in {product-kafka} - a _management_ API and an _instance_ API.

ifndef::community[]
For more information about these APIs, see https://access.redhat.com/documentation/en-us/red_hat_openshift_application_services/1/guide/2409253a-45ee-470e-bdc9-5db4bfcf9d0f.
endif::[]

For more information on creating and setting up Kafka instances, see {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^].


//Additional line break to resolve mod docs generation error

[id="con-ansible_{context}"]
== Ansible

[role="_abstract"]
https://www.ansible.com/overview/how-ansible-works[Ansible^] is an open source configuration management and automation tool.

Ansible uses modules to execute tasks. The output of one module can be the input for the next one or they can be independent of each other.

You can combine one or more Ansible modules to make a play. Combine two or more plays to create an Ansible Playbook, a list of sequential tasks (written in YAML) that automatically executes when you run the `ansible-playbook` command. You can use an Ansible Playbook to describe the desired state of your infrastructure and then have Ansible provision it.

Ansible roles are a special kind of playbook that is fully self-contained and portable with the tasks, variables, configuration templates, and other supporting files that are needed to complete your chosen tasks. Multiple roles can exist inside a collection allowing easy sharing of content through Automation Hub and https://galaxy.ansible.com[Ansible Galaxy^].

//Additional line break to resolve mod docs generation error

[id="proc-install-rhoas-ansible-collection_{context}"]
=== Installing the RHOAS Ansible collection

[role="_abstract"]
You can use the https://galaxy.ansible.com/rhoas/rhoas[RHOAS Ansible collection] to fully manage your Kafka environment with Ansible. You can create, view, configure, and delete the following {product-long-kafka} resources:

* Kafka instances
* Service accounts
* Kafka topics
* Access Control Lists (ACLs)

.Prerequisites

* https://www.python.org/downloads/[Python 3.9^] or later is installed.
* https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html?extIdCarryOver=true&sc_cid=701f2000001Css5AAC[Ansible 2.9^] or later is installed.
* Some additional dependencies required by this collection are installed. To install these, enter the following commands:
+
[source,shell]
----
$ pip install rhoas-sdks
$ pip install python-dotenv
----
NOTE: Use the `pip install rhoas-sdks --force reinstall` command to ensure that the most up to date versions of the SDKs are installed.
+

* (Optional) This collection works best when used in a Python virtual environment. To create and activate a Python virtual environment, enter the following command:
+
[source,shell]
----
$ python3 -m venv rhoas/
. rhoas/bin/activate
----
+

.Procedure

. Open a terminal and enter the following command:
+
[source,shell]
----
$ ansible-galaxy collection install rhoas.rhoas
----
+
By default, the `ansible-galaxy collection install` command uses https://galaxy.ansible.com as the Galaxy server URL. The URL is specified as the value of the `GALAXY_SERVER` parameter in the `ansible.cfg` configuration file.
. Once installed, you can reference a collection content by its fully qualified collection name (FQCN). For example the `create_kafka` command in a task needs to be the FQCN of the module for collection development, for example
`rhoas.rhoas.create_kafka`.

[id="proc-using-rhoas-ansible-collection_{context}"]

=== Using the RHOAS Ansible collection

.Prerequisites
* You have a Red Hat account.
* You have an offline token used to authenticate the Ansible modules with the {product-long-rhoas} API.

[NOTE]
The offline token is a refresh token with no expiry and can be used by non-interactive processes to provide an access token for {product-long-rhoas} to authenticate the user. The token is an OpenShift offline token and you can find it at https://cloud.redhat.com/openshift/token.

* The collection requires the following two variables to work:
API_BASE_HOST:: The base host for the API. This is the base URL for the API. For example, https://api.openshift.com.
SSO_BASE_HOST:: The base host for the SSO. This is the base URL for the SSO. For example, https://sso.redhat.com/auth/realms/redhat-external.
You can define the environment variables shown in an environment variable (`.env`) file and save the file in the root directory. If you don't explicitly define these environment variables, the collection uses the URLs shown in the examples.

.Procedure

. (Optional) To find documentation for each module, open a terminal window, enter `ansible-doc rhoas.rhoas.<module_name>` and replace <module_name> with the name of the module in the terminal. The following example outputs the documentation for the `create_kafka` module. The documentation shows required parameters (denoted by `=`) and optional parameters (denoted by `-`) for the module, as well as examples of how to use the module and the output.
+
[source,shell]
----
$ ansible-doc rhoas.rhoas.create_kafka
----
. (Optional) You can run a module in the terminal to perform a single task. For example, the following command shows how to run a module that creates a Kafka instance.
+
.Example create_kafka module
[source,shell]
----
$ ansible localhost -m rhoas.rhoas.create_kafka -a 'name=unique-kafka-name billing_model=standard cloud_provider=aws plan="developer.x1" region="us-east-1" openshift_offline_token=<OFFLINE_TOKEN>'
----

+
. Open a terminal window.
. Create a new playbook file and save it as `rhoas_test.yml` in any directory.
. In your web browser, navigate to https://github.com/redhat-developer/app-services-ansible/blob/main/rhoas_test.yml to open the {product-long-rhoas} example playbook.

. Inspect the contents of the example playbook. In particular, observe that the playbook has modules for the following tasks:
+
* Creating and deleting a Kafka instance
* Creating and deleting a service account
* Creating Access Control List (ACL) permission bindings
* Creating, updating, and deleting a topic
+
[NOTE]
The playbook uses your offline token to authenticate with the Kafka Management API. If the token is not passed in as an argument per task, the module attempts to read it from the `_OFFLINE_TOKEN_` environment variable.

+
[NOTE]
====
The example playbook used in this section includes comments that indicate how to directly specify values rather than fetching them dynamically. For example, to specify a Kafka instance ID, a comment in the playbook states that you can include the following line:

[source, subs="+quotes"]
----
kafka_id: __<kafka_id>__
----
====
+
. Click on the copy symbol to copy the contents of the example playbook.
. Open the `rhoas_test.yml` file in your IDE and paste the example into it.
. Configure the `create_kafka_` module to provision a new Kafka instance. The only required fields for this module are the `name`, `billing_model` and `cloud provider` fields. You can specify configuration options such as `billing_cloud_account_id` and the `openshift_offline_token`. All other information for the instance is provided by the Kafka APIs. The default contents of this module in the OpenShift Application Services example playbook are shown below:
+
.Example `create_kafka` module
[source,yaml]
----
- name: Create kafka
    rhoas.rhoas.create_kafka:
      name: "kafka-name"
      instance_type: "x1"
      billing_model: "standard"
      cloud_provider: "aws"
      region: "us-east-1"
      plan: "developer.x1"
      billing_cloud_account_id: "123456789"
      openshift_offline_token: "OPENSHIFT_OFFLINE_TOKEN"
    register:
      kafka_req_resp
----
+
When you run the `create_kafka_` module, Ansible saves the output of that command in a variable in the `register` field. In the preceding example, Ansible saves the created Kafka instance as `_kafka_req_resp_`.

. Configure the `create_service_account` module to create a service account. The only required fields are the `name` and `short description` fields. Ansible populates the generated service account credentials in the `client_id` and `client_secret` fields once it creates the service account.
+
.Example `create_service_account` module
[source,yaml]
----
- name: Create Service Account
    create_service_account:
      name: "service-account-name"
      description: "This is a description of the service account"
      openshift_offline_token: "OPENSHIFT_OFFLINE_TOKEN"
    register:
      srvce_acc_resp_obj
----

. Configure the `create_kafka_acl_binding` module to create an ACL for the service account with some default values, and bind that ACL to the Kafka instance and service account. You can get the Kafka ID from the `_kafka_req_resp_` variable or enter it in the kafka_id field. You can get the service account ID from the `_srvce_acc_resp_obj_` variable. The following list is a description of each field that must have a value in an ACL binding module.
** `resource_name`: the name of resource you want access to. This example uses the name that is passed when creating the topic.
** `resource_type`: The type of resource you want access to. This example uses *Topic*.
** `pattern_type`: The type of pattern of the ACL.
** `operation_type`: The type of operation that is allowed for the given user on this module.
** `permission_type`: Whether permission is given or taken away.
+
.Example `create_kafka_acl_binding` module
[source,yaml]
----
- name: Create kafka ACL Service Binding
    rhoas.rhoas.create_kafka_acl_binding:
      kafka_id: "{{ kafka_req_resp.kafka_id }}"
      # To hardcode the kafka_id, uncomment and use the following line:
      # kafka_id: "KAFKA_ID"
      principal: " {{ srvce_acc_resp_obj['client_id'] }}"
      # To hardcode the principal_id, uncomment and use the following line:
      # principal: "PRINCIPAL_ID"
      resource_name: "topic-name"
      resource_type: "Topic"
      pattern_type: "PREFIXED"
      operation_type: "all"
      permission_type: "allow"
      openshift_offline_token: "OPENSHIFT_OFFLINE_TOKEN"
    register: kafka_acl_resp

----

. Configure the `create_kafka_topic` module to create a Kafka topic. The `kafka_id` field is a required field.
+
.Example `create_kafka_topic` module
[source,yaml]
----
- name: Create Kafka Topic
    create_kafka_topic:
      topic_name: "kafka-topic-name"
      kafka_id: "{{ kafka_req_resp.id }}"
      # To hardcode the kafka_id, uncomment and use the following line:
      # kafka_id: "KAFKA_ID"
      partitions: 1
      retention_period_ms: "86400000"
      retention_size_bytes: "1073741824"
      cleanup_policy: "compact"
      openshift_offline_token: "OPENSHIFT_OFFLINE_TOKEN"
    register:
      create_topic_res_obj
----
+
. Update the configuration of the first topic using the `update_kafka_topic` module. In the following example, the cleanup policy has been updated from compact to delete by replacing `"compact"` with `"delete"` in the `cleanup_policy` field. You can also update the `retention_period_ms` and `retention_size_bytes` fields.
+
.Example `update_kafka_topic` module
[source,yaml]
----
- name: Update Kafka Topic
    update_kafka_topic:
      topic_name: "kafka-topic-name"
      kafka_id: "{{ kafka_req_resp.id }}"
      # To hardcode the kafka_id, uncomment and use the following line:
      # kafka_id: "KAFKA_ID"
      partitions: 1
      retention_period_ms: "86400000"
      retention_size_bytes: "1073741824"
      cleanup_policy: "delete"
      openshift_offline_token: "OPENSHIFT_OFFLINE_TOKEN"
    register:
      update_topic_res_obj
----
+

. Configure the `delete_kafka_topic` module to delete both created topics.
+
.Example `delete_kafka_topic` module
[source,yaml]
----
- name: Delete Kafka Topic
   rhoas.rhoas.delete_kafka_topic:
     topic_name: "KAFKA_TOPIC_NAME"
      kafka_id: "{{ kafka_req_resp_obj['kafka_id'] }}"
      # To hardcode the kafka_id, uncomment and use the following line:
      # kafka_id: "KAFKA_ID"
     openshift_offline_token: "OPENSHIFT_OFFLINE_TOKEN"
----

. Configure the `delete_service_account_by_id` module to delete the service account.
+
.Example `delete_service_account_by_id` module
[source,yaml]
----
- name: Delete Service Account
   rhoas.rhoas.delete_service_account_by_id:
   # service_account_id: "service_account_id"
  service_account_id: "{{ srvce_acc_resp_obj['client_id'] }}"

  # openshift_offline_token: "OFFLINE_TOKEN"
----

. Deprovision and delete the {product-kafka} instance using the `delete_kafka_by_id` module.
+
.Example `delete_kafka_by_id` module
[source]
----
- name: Delete kafka instance by ID
    rhoas.rhoas.delete_kafka_by_id:
     kafka_id: "{{ kafka_req_resp_obj['kafka_id'] }}"
     openshift_offline_token: "offline_token"
----
. Save your changes.
. Open a terminal and enter the following command to run the example `rhoas_test` playbook:
+
[source, shell]
----
$ ansible-playbook rhoas_test.yml
----

As the playbook example used in this guide is intended to test the resources, it deletes all created resources when run. Therefore, some alteration of the playbook is necessary to keep the resources. For more information on writing playbooks, see the https://docs.ansible.com/ansible/latest/user_guide/playbooks_best_practices.html#playbooks-tips-and-tricks[Ansible documentation].

[id="con-terraform_{context}"]
== Terraform

link:https://www.terraform.io/[HashiCorp Terraform^] is an Infrastructure as Code tool that lets you build, change, and version infrastructure safely and efficiently through human-readable configuration files that you can version, reuse, and share. You can then use a consistent workflow to provision and manage all of your infrastructure throughout its lifecycle.

The link:https://registry.terraform.io/providers/redhat-developer/rhoas/latest[RHOAS Terraform^] provider is available in the official link:https://www.terraform.io/[Terraform provider registry^] and includes resources to interact with {product-long-rhoas}.

You can fully manage your Kafka environment through your Terraform system using the RHOAS Terraform provider. You can create, view, configure, deploy, and delete the following {product-kafka} resources:

* Kafka instances
* Service accounts
* Kafka topics
* Access Control Lists (ACLs)


[id="proc-install-rhoas-terraform-provider_{context}"]
=== Installing the RHOAS Terraform provider

.Prerequisites
* You have a Red Hat account.
* https://www.terraform.io/downloads[Terraform^] v1.3.4 or later is installed.

.Procedure
. In your web browser, open the RHOAS Terraform provider at https://registry.terraform.io/providers/redhat-developer/rhoas/latest.
. In the upper-right corner of Terraform Registry, click *Use Provider*.
+
A pane opens that shows the configuration you need to use the RHOAS Terraform provider.
. In the pane that opened, copy the configuration shown. The following lines show an example of the configuration.
+

.Example RHOAS provider configuration
[source,shell]
----
$ terraform {
  required_providers {
    rhoas = {
      source = "redhat-developer/rhoas"
      version = "0.4.0-alpha1"
    }
  }
}

provider "rhoas" {
  #configuration options
}
----

. In your IDE, open a new file and paste the configuration you copied. You can specify configuration options in the provider settings.
+

. Save the file as a Terraform configuration (`main.tf`) file in a directory called `Terraform`.
. Open a terminal and navigate to the Terraform directory.
+
[source,shell]
----
$ cd Terraform
----
. Enter the following command. This command initializes the working directory containing Terraform configuration files and installs any required plug-ins.
+
[source,shell]
----
$ terraform init
----
When the Terraform provider has been initialized, you see a confirmation message.

[id="proc-using-terraform_{context}"]
=== Using the RHOAS Terraform provider

Resources are the most important element in the Terraform language. Each resource block in a Terraform provider describes one or more infrastructure objects. For {product-long-kafka}, such infrastructure objects might include Kafka instances, service accounts, Access Control Lists (ACLs), and topics. The procedure that followS show what resources you can add to your Terraform configuration file to create a Kafka instance and its associated resources such as service accounts and topics.

.Prerequisites

* You have a Red Hat account.
* You have an offline token that authenticates the Terraform resources with the OpenShift Application Services API.

[NOTE]
====
The offline token is a refresh token with no expiry and can be used by non-interactive processes to provide an access token for {product-long-rhoas} to authenticate the user. The token is an OpenShift offline token and you can find it at https://cloud.redhat.com/openshift/token. As the offline token is a sensitive value that varies between environments it is best specified as an `_OFFLINE_TOKEN_` environment variable when running `terraform apply` in a terminal. To set this environment variable, enter the following command in a terminal window, replacing <offline_token> with the value of the offline token:
[source, subs="+quotes"]
----
export OFFLINE_TOKEN=<offline_token>
----
====

.Procedure

. Open the `main.tf` file in your IDE for editing.
. Enter the example rhoas_kafka resource. This example uses the `"my-instance"`  identifier and creates a Kafka instance called `my-instance`. The only required fields are the `name`, `billing_model` and `plan` fields. The following values are set by default when you run `terraform apply`.
+
.. `cloud provider`
.. `region`
+
All other information for the instance is provided by the Kafka APIs. The generated bootstrap server displays in the terminal as an output.
+
.Example `rhoas_kafka` resource
[source]
----
resource "rhoas_kafka" "my-instance" {
  name = "my-instance"
  plan = "developer.x1"
  billing_model = "standard"
}
  output "bootstrap_server_my-instance" {
    value = rhoas_kafka.my-instance.bootstrap_server_host
}

----
. After creating your Kafka instance, you create a service account. To connect your applications or services to a Kafka instance in {product-kafka}, you must first create a service account with credentials. In the `main.tf` file, enter the example `rhoas_service_account` resource to create a service account. This example uses the `"my-service-account"` identifier and creates a service account called my-service-account. The generated client ID displays in the terminal as an output.
+

.Example `rhoas_service_account` resource
[source]
----
resource "rhoas_service_account" "my-service-account" {
  name        = "my-service-account"
  description = "<description of service account>"
}

output "client_id" {
  value = rhoas_service_account.my-service-account.client_id
}

output "client_secret" {
  value     = rhoas_service_account.my-service-account.client_secret
  sensitive = true
}
----
+
The values for the client ID and client secret will be set owhen you run `terraform apply`.
. Enter the example `rhoas_topic` resource to create a Kafka topic with default values. This example uses the `topic` identifier and creates the `my-topic` Kafka topic. As you have already created the Kafka instance, Terraform can check dependencies for this new topic resource and knows the Kafka ID when you run this example resource.
+
.Example `rhoas_topic` resource with default values
[source]
----
resource "rhoas_topic" "topic" {
		name = "my-topic"
		partitions = 1
		kafka_id = rhoas_kafka.instance.id
	}

----
+
. Enter the `rhoas_acl` resource to create an ACL binding. This example uses the `"acl"` identifier. The following list is a description of each field in an ACL binding resource.

* `resource_type`: The type of resource you want access to. This example uses *“TOPIC”*.
* `resource_name`: the name of resource you want access to. This example uses the name that is passed when creating the topic.
* `principal`: the user that this binding applies to. This example uses the service account client ID.
* `operation_type`: The type of operation that is allowed for the given user on this resource.
* `permission_type`: Whether permission is given or taken away.

+
.Example `ACL binding` resource
[source]
----
resource "rhoas_acl" "acl" {
  kafka_id = rhoas_kafka.instance.id
  resource_type = "TOPIC"
  resource_name = "my-topic"
  pattern_type = "LITERAL"
  principal = rhoas_service_account.my-service-account.client_id
  operation_type = "ALL"
  permission_type = "ALLOW"
}

----
. Save your changes.
. Open a terminal window and enter the following command:
+
[source,shell]
----
$ terraform init
----
. Enter the following command:
+
[source,shell]
----
$ terraform apply
----
. Terraform displays a message that `rhoas_kafka.my-instance`, `rhoas_acl.acl`,`rhoas_service_account.my-service-account`, and `rhoas_acl.acl` will be created and displays all the field values for each resources, including fields that will be known after apply such as `client_id` and `client_secret`.
When you're ready to create your instance, service account, topic, and set your permissions, type *Yes*.
+
Running `terraform apply` also creates the Terraform state file. Terraform logs information about the resources it has created in this state file. This allows Terraform to know which resources are under its control and when to update and destroy them. The Terraform state file is named `terraform.tfstate` by default and is kept in the same directory where Terraform is run. Sensitive information such as the offline token, client ID, and client secret can be found in the `terraform.tfstate` file. Running `terraform apply` updates this file.
. Verify that the instance, service account, and topic have been created in the *Kafka Instances* and *Service Accounts* pages of the {product-kafka} {service-url-kafka}[web console^].
. (Optional) To delete the created resources, enter the following command:
+
[source,shell]
----
$ terraform destroy
----

[id="con-data-sources_{context}"]
=== Data Sources

[role="_abstract"]
In Terraform, you can use data sources to obtain information about resources external to Terraform, defined by another separate Terraform configuration, or modified by functions using the data block. Apply data sources in the same way that you add resources to the configuration file. The following `rhoas_kafkas` data source example provides a list of the Kafka instances accessible to your organization in {product-long-kafka}.

.Example `rhoas_kafkas` data source
[source]
----
terraform {
  required_providers {
    rhoas = {
      source  = "registry.terraform.io/redhat-developer/rhoas"
      version = "0.3"
    }
  }
}

provider "rhoas" {}

data "rhoas_kafkas" "all" {
}

output "all_kafkas" {
  value = data.rhoas_kafkas.all
}
----

[role="_additional-resources"]
== Additional resources
* {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^]
* https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/7d28aec8-e146-44db-a4a5-fafc1f426ca5[Configuring topics in {product-long-kafka}^]
* {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^]
* {base-url}{access-mgmt-url-kafka}[Managing account access in {product-long-kafka}^]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
