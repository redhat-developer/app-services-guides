////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//All OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:product-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/
:sso-token-url: https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
:cloud-console-url: https://console.redhat.com/
:service-accounts-url: https://console.redhat.com/application-services/service-accounts
:rh-sso-url: https://sso.redhat.com
:rh-customer-portal: Red Hat Customer Portal

//OpenShift
:openshift: OpenShift
:osd-name: OpenShift Dedicated
:osd-name-short: OpenShift Dedicated
:rosa-name: OpenShift Service on AWS
:rosa-name-short: OpenShift Service on AWS

//OpenShift Application Services CLI
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:command-ref-url-cli: commands
:installation-guide-url-cli: rhoas/rhoas-cli-installation/README.adoc
:service-contexts-url-cli: rhoas/rhoas-service-contexts/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:getting-started-rhoas-cli-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc
:message-browsing-url-kafka: kafka/message-browsing-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:getting-started-rhoas-cli-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:connectors: Connectors
:product-long-connectors: OpenShift Connectors
:product-connectors: Connectors
:product-version-connectors: 1
:service-url-connectors: https://console.redhat.com/application-services/connectors
:getting-started-url-connectors: connectors/getting-started-connectors/README.adoc
:getting-started-rhoas-cli-url-connectors: connectors/rhoas-cli-getting-started-connectors/README.adoc
:addon-url-connectors: https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01


//OpenShift API Designer
:product-long-api-designer: OpenShift API Designer
:product-api-designer: API Designer
:product-version-api-designer: 1
:service-url-api-designer: https://console.redhat.com/application-services/api-designer/
:getting-started-url-api-designer: api-designer/getting-started-api-designer/README.adoc

//OpenShift API Management
:product-long-api-management: OpenShift API Management
:product-api-management: API Management
:product-version-api-management: 1
:service-url-api-management: https://console.redhat.com/application-services/api-management/

////
END GENERATED ATTRIBUTES
////

[id="chap-connectors-rhoas-cli"]
= Managing {product-connectors} by using the {product-rhoas} CLI
ifdef::context[:parent-context: {context}]
:context: connectors-rhoas-cli

// Purpose statement for the assembly
[role="_abstract"]
As a developer of {product-long-connectors}, you can use the `rhoas` command-line interface (CLI) to create and configure connections between {product-long-kafka} and third-party systems.

You can use a *source* connector to send data from an external system to {product-kafka}. You can use a *sink* connector to send data from {product-kafka} to an external system.

For the example in this guide, you create a source connector that sends data from a simple data generator to a Kafka topic. You also create a sink connector that sends data from the Kafka topic to an HTTP site.

.Prerequisites
ifndef::community[]
* You have a {org-name} account.
endif::[]
* You've installed the latest version of the `rhoas` CLI as described in {base-url}{installation-guide-url-cli}[Installing and configuring the {product-rhoas}  CLI^].
* You've completed the following tasks:
+
*Note:* You can find detailed instructions for these tasks in {base-url}{getting-started-rhoas-cli-url-kafka}[Creating Kafka resources by using the {product-rhoas} CLI^].

** Create a Kafka instance.
[source,subs="+quotes"]
+
----
$ rhoas kafka create --name=my-kafka-instance
----

** Verify that the Kafka instance is in the *Ready* state.
+
[source,subs="+quotes"]
----
$ rhoas context status kafka
----

** Create a service account and copy the service account ID and secret. You must use a service account to connect and authenticate your {product-connectors} instances with your Kafka instance.
+
[source,subs="+quotes"]
----
$ rhoas service-account create --file-format=json --short-description=test-service-account
----

** For your Kafka instance, set the permissions for the service account to enable {connectors} instances (that are configured with the service account credentials) to produce and consume messages in any topic in the Kafka instance.
+
[source,subs="+quotes"]
----
$ rhoas kafka acl grant-access --producer --consumer --service-account=<service-acct-id> --topic all --group all
----

** Create a Kafka topic named `test-topic`. The Kafka topic stores messages sent by producers (data sources) and makes them available to consumers (data sinks).
+
[source,subs="+quotes"]
----
$ rhoas kafka topic create --name=test-topic
----

[id="proc-create-connector-namespace_{context}"]
== Creating a namespace to host your {connectors} instances
[role="_abstract"]

A {connectors} namespace hosts your {connectors} instances.

The namespace that you use depends on your OpenShift Dedicated environment.

If you're using the hosted preview environment:: You must create a preview namespace before you can create {connectors} instances. A preview namespace is active for 48 hours.

If you're using a Trial cluster in your own OpenShift Dedicated environment:: The namespace is created when you add the {product-connectors} service to your Trial cluster, as described in {addon-url-connectors}[Adding the Red Hat OpenShift {connectors} add-on to your OpenShift cluster^]. Your OpenShift Dedicated Trial cluster namespace is active for 60 days.

.Prerequisites

* You're logged in to `rhoas`.
+
[source]
----
$ rhoas login
----

.Procedure

. If you're using a Trial cluster in your own OpenShift Dedicated environment, skip to Step 2.
+
If you're using the {product-connectors} evaluation site, create an evaluation namespace.
+
[source,subs="+quotes"]
----
$ rhoas connector namespace create --name=eval-namespace
----

. Verify that your namespace is listed.
+
[source,subs="+quotes"]
----
$ rhoas connector namespace list
----

[id="proc-building-connector-configuration-cli_{context}"]
== Building connector configuration files

[role="_abstract"]
Before you can create a {connectors} instance, you must build a configuration file that is based on a supported connector type that is listed in the {product-connectors} catalog.

For this example, you want to create two types of connectors: a data generator (a source connector) and an HTTP sink connector.

You must build a configuration file for each connector type that you want to create. When you build a configuration file, the default file name is `connector.json`. Optionally, you can specify a different configuration file name.

.Prerequisites

* Your current local directory is the place where you want to save your {connectors} configuration files. For example, if you want to save your configuration files in a directory named `my-connectors`, ensure that the `my-connectors` directory is your current directory.
+
[source]
----
$ cd my-connectors
----

* You're logged in to `rhoas`.

* For the sink connector example, open the free https://webhook.site[Webhook.site^] page in a browser window. The `Webhook.site` page provides a unique URL that you can use for the example HTTP data sink.

.Procedure

. Decide which type of connector you want to create.

.. View a list of the supported connector types that are available in the {connectors} catalog. The default number of connector types listed is set to *10*. To see all {connectors} types, specify a limit value of *100*.
+
[source,subs="+quotes"]
----
$ rhoas connector type list --limit=100
----
// .. Filter the list to show only sink connectors:
// +
// [source,subs="+quotes"]
// ----
// rhoas connector type list --limit=70 --search=%sink%
// ----
//
// .. Filter the list to show only source connectors:
// +
// [source,subs="+quotes"]
// ----
// rhoas connector type list --limit=70 --search=%source%
// ----

.. For this example, find the data generator source connector by specifying "Generator" in the `search` flag.
+
[source,subs="+quotes"]
----
$ rhoas connector type list --search=%Generator%
----
+
The result is as follows:
+
[source,subs="+quotes"]
----
{
  "name": "Data Generator source",
  "id": "data_generator_0.1",
  "description": "A data generator (for development and testing purposes)."
}
----

.. For this example, find the HTTP sink connector by specifying "HTTP" in the `search` flag.
+
[source,subs="+quotes"]
----
$ rhoas connector type list --search=%HTTP%
----
+
The first result is the HTTP sink.
+
[source,subs="+quotes"]
----
{
  "name": "HTTP sink",
  "id": "http_sink_0.1",
  "description": "Send data to an HTTP endpoint."
}
----

. Build a configuration file for the `data_generator_0.1` connector type. Specify `test-generator` as the {connectors} instance name and `test-generator.json` as the configuration file name.
+
[source,subs="+quotes"]
----
$ rhoas connector build --name=test-generator --type=data_generator_0.1 --output-file=test-generator.json
----
+
*Note:* By default, the configuration file is in JSON format. Optionally, you can specify YAML format by adding `-o yaml` to the `connector build` command.

. Answer the prompts for configuration values.

.. For *Format*, press *ENTER* to accept the default (`application/octet-stream`).

.. For *Error handling method*, select `stop`. The {connectors} instance stops running if it encounters an error.

.. For *Topic Names*, type `test-topic`.

.. For *Content Type*, accept the default.

.. For *Message*, type `Hello World!`.

.. For *Period*, accept the default (`1000`).

. Build a configuration file for the `http_sink_0.1` connector type and specify `test-http` as the configuration file name.
+
[source,subs="+quotes"]
----
$ rhoas connector build --name=test-http --type=http_sink_0.1 --output-file=test-http.json
----

. Answer the prompts for configuration values.

.. For *Format*, press *ENTER* to accept the default (`application/octet-stream`).

.. For *Error handling method*, select `stop`.

.. For *Method*, accept the default (`POST`).

.. For *URL*, paste your unique URL that you copied from the https://webhook.site[Webhook.site^] page.

.. For *Topic Names*, type `test-topic`.

. Verify that the configuration files were built.
+
[source]
----
$ ls
----
+
The result shows the `test-generator.json` and `test-http.json` files.

. Optionally, you can edit a configuration file in an editor of your choice.
+
*Note:* To prevent saving sensitive data to disk, the values for the service account and the namespace are not included in the configuration file. You're prompted to specify those values when you create an {product-connectors} instance.

[id="proc-create-connector-instances_{context}"]
== Creating {connectors} instances
[role="_abstract"]

After you build a configuration file based on a connector type, you can use the configuration file to create a {connectors} instance.

For this example, you create two {connectors} instances: a data generator source {connectors} instance and an HTTP sink {connectors} instance.

.Prerequisites

* You have built configuration files based on each type of connector that you want to create.
* The configuration files are saved in your current directory.
* You have a {connectors} namespace.
* You have an {product-long-kafka} instance running and have a topic called `test-topic`.
* You have a service account created that has read and write access to the Kafka topic, and you know the credentials (ID and secret).

.Procedure

. Create a source {connectors} instance by specifying the source connector's configuration file. For example, the data generator configuration file is `test-generator.json`.
+
[source,subs="+quotes"]
----
$ rhoas connector create --file=test-generator.json
----

. Answer the prompts for details about the {connectors} instance.

.. For *Set the {connectors} namespace*, select your namespace from the list. For example, select `eval-namespace`.

.. For *Service Account Client ID*, type or paste your ID.

.. For *Service Account Client Secret*, type or paste your secret.
+
A message states "Successfully created the {connectors} instance".

. Wait until the status of the {connectors} instance is *Ready*.
+
To check the status:
+
[source,subs="+quotes"]
----
$ rhoas connector list
----

. Verify that your source {connectors} instance is producing messages.
+
[source,subs="+quotes"]
----
$ rhoas kafka topic consume --name=test-topic --partition=0 --wait
----

. Create a sink {connectors} instance by specifying the sink connector's configuration file. For example, the HTTP sink configuration file is `test-http.json`.
+
[source,subs="+quotes"]
----
$ rhoas connector create --file=test-http.json
----

. Answer the prompts for details about the {connectors} instance.

.. For *Set the {connectors} namespace*, select your namespace from the list. For example, select `eval-namespace`.

.. For *Service Account Client ID*, type or paste your ID.

.. For *Service Account Client Secret*, type or paste your secret.
+
A message states "Successfully created the {connectors} instance".

. Wait until the status of the {connectors} instance is *Ready*.
+
To check the status:
+
[source,subs="+quotes"]
----
$ rhoas connector list
----

. Verify that your sink {connectors} instance is receiving messages by viewing your https://webhook.site[Webhook.site^] page in a web browser.

[id="proc-commands-managing-connectors_{context}"]
== Managing {connectors} instances

[role="_abstract"]
The following `rhoas connector` help commands describe additional `rhoas connector` commands that you can use to manage your {connectors} instances:

* `rhoas connector namespace -h` for managing {connectors} namespaces
* `rhoas connector type -h` for viewing the available types of connectors
* `rhoas connector list -h`` for listing {connectors} instances
* `rhoas connector build -h` for building configuration files
* `rhoas connector create -h` for creating {connectors} instances

[role="_additional-resources"]
.Additional resources
* {base-url-cli}{command-ref-url-cli}[_CLI command reference (rhoas)_^]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
