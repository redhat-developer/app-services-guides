////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/

//OpenShift Application Services CLI
:rhoas-cli-base-url: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:rhoas-cli-ref-url: commands
:rhoas-cli-installation-url: rhoas/rhoas-cli-installation/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:rhoas-cli-getting-started-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:rhoas-cli-getting-started-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:product-long-connectors: OpenShift Connectors
:service-url-connectors: https://console.redhat.com/application-services/connectors
////
END GENERATED ATTRIBUTES
////

[id="chap-getting-started-connectors"]
= Getting started with {product-long-connectors}
ifdef::context[:parent-context: {context}]
:context: getting-started-connectors

// Purpose statement for the assembly
[role="_abstract"]
As a developer of applications and services, you can use {product-long-connectors} to create and configure connections between {product-long-kafka} and third-party systems.

In this example, you connect a data source (a data generator) that creates Kafka messages and a data sink (an HTTP endpoint) that consumes the Kafka messages.

// Condition out QS-only content so that it doesn't appear in docs.
// All QS anchor IDs must be in this alternate anchor ID format `[#anchor-id]` because the ascii splitter relies on the other format `[id="anchor-id"]` to generate module files.
ifdef::qs[]
[#description]
Learn how to create and set up connectors in {product-long-connectors}.

[#introduction]
Welcome to the quick start for {product-long-connectors}. 

In this quick start, you learn how to create a source connector and sink connector and send data to and from {product-kafka}. 

A *source* connector allows you to send data from an external system to {product-kafka}. A *sink* connector allows you to send data from {product-kafka} to an external system. 

endif::[]

ifndef::qs[]
== Overview

{product-long-kafka} is a cloud service that simplifies the process of running Apache Kafka. Apache Kafka is an open-source, distributed, publish-subscribe messaging system for creating fault-tolerant, real-time data feeds.  

You can use {product-long-connectors} to configure communication between {product-kafka} instances and external services and applications. {product-long-connectors} allow you to configure how data moves from one endpoint to another without writing code. 

The following diagram illustrates how data flows from a data source through a data source connector to a Kafka topic. And how data flows from a Kafka topic to a data sink through a data sink connector.

 [.screencapture] 
.{product-long-connectors} data flow
image::connectors-diagram.png[Illustration of data flow from data source through Kafka to data sink]

endif::[]

[id="proc-configuring-kafka-for-connectors_{context}"]
== Configuring the {product-kafka} instance for use with {product-long-connectors}

[role="_abstract"]
Configure your {product-kafka} instance for use with {product-long-connectors} by:

* Creating *Kafka topics* to store messages sent by producers (data sources) and make them available to consumers (data sinks). 
* Creating *service accounts* that allow you to connect and authenticate your Connectors with Kafka instances. 
* Setting up *access rules* for the service accounts that define how your Connectors can access and use the associated Kafka instance topics.

The number of Kafka topics and service accounts that you create, and the access rules for the service accounts, depend on your application. 

For this example, you create one Kafka topic, named *test*, one service account, and you define access for the service account.

ifndef::qs[]
.Prerequisites
* You're logged in to the  OpenShift Application Services web console at {service-url-connectors}[^].
* You've created a  {product-kafka} instance and the instance is in the *Ready* state.
For instructions on how to create a Kafka instance, see _{base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^]_. 
endif::[]

.Procedure
. Create a Kafka topic for your connectors:
.. In the OpenShift Application Services web console, select *Streams for Apache Kafka* > *Kafka Instances*.
.. Click the name of the Kafka instance that you want to add a topic to.
.. Select the *Topics* tab, and then click *Create topic*.
.. Type a unique name for your topic. For example, type *test-topic* for the *Topic Name*. 
.. Accept the default settings for partitions, message retention, and replicas.
. Create a service account for connectors: 
.. In the web console, select *Service Accounts*, and then click *Create service account*.
.. Type a unique service account name (for example, *test-service-acct* ) and then click *Create*.
.. Copy the generated *Client ID* and *Client Secret* to a secure location. You'll use these credentials to configure connections to this service account.
.. Select the *I have copied the client ID and secret* option, and then click *Close*.

. Set the level of access for your new service account in the Access Control List (ACL) of the Kafka instance:
.. Select *Streams for Apache Kafka* > *Kafka Instances*.
.. Click the name of the Kafka instance that you want the service account to access.
.. Click the *Access* tab to view the current ACL for the Kafka instance and then click *Manage access*.
.. From the *Account* drop-down menu, select the service account that you created in Step 2, and then click *Next*.
.. Under *Assign Permissions*, click *Add permission*.
.. From the drop-down menu, select *Consume from a topic*. Set all resource identifiers to `is` and all identifier values to `"*"`.
.. From the *Add permission* drop-down menu, select *Produce to a topic*. Set all resource identifiers to `is` and all identifier values to `"*"`.
+
The `is "*"` settings enable connectors that are configured with the service account credentials to produce and consume messages in any topic in the Kafka instance.

ifdef::qs[]
.Verification
* Did you create a topic for connectors?
* Did you create a service account and save the credentials to a secure location?
* Did you set the *Consume from a topic* and *Produce to a topic* permissions for the service account?
endif::[]


[id="proc-creating-source-connector_{context}"]
== Creating a Connectors instance for a data source

[role="_abstract"]
A *source* connector consumes events from an external data source and produces Kafka messages. 

For this example, you create an instance of the *Data Generator* source connector. 

You configure your connector to listen for events from the data source and produce a Kafka message for each event. 

The connector sends the messages at regular intervals to the Kafka topic that you created for Connectors.

ifndef::qs[]
.Prerequisites
* You're logged in to the  OpenShift Application Services web console at {service-url-connectors}[^].
* You configured a Kafka instance for Connectors as described in _{base-url}{getting-started-url-conectors}/proc-configuring-kafka-for-connectors_getting-started-connectors[Configuring the {product-kafka} instance for use with {product-long-connectors}^]_.

endif::[]

.Procedure
. In the OpenShift Application Services web console, select *Connectors* and then click *Create Connectors instance*.
. Select the connector that you want to use for a data source.
+
You can browse through the catalog of available connectors. You can also search for a particular connector by name, and filter for sink or source connectors.
+
For example, to find the *Data Generator* source connector, type *data* in the search box. The list filters to show only the *Data Generator Connector* card. 
+
Click the card to select the connector, and then click *Next*.

. For the *Kafka instance*, click the card for the {product-kafka} instance that you configured for Connectors, and then click *Next*.
+
NOTE: If you have not already configured a {product-kafka} instance for Connectors, you can create a new Kafka instance by clicking *Create Kafka instance*. You would also need to set up and define access for a service account as described in _Configuring the {product-kafka} instance for use with {product-long-connectors}_.

. On the *Namespace* page, click *Register eval namespace* to provision a namespace for hosting the Connectors instances that you create.

//. On the *Namespace* page, the namespace that you select depends on your OpenShift Dedicated environment.
//+
//If you are using a trial cluster in your own OpenShift Dedicated environment, select the card for the namespace that was created when you added the Connectors service to your trial cluster, as described in _https://access.redhat.com/documentation/en-us/red_hat_openshift_connectors/TBD[Adding the OpenShift Connectors service to an OpenShift Dedicated trial cluster^]_.
//need to update this link with correct URL
//+
//If you are using the evaluation OpenShift Dedicated environment, click *Register eval namespace* to provision a namespace for hosting the Connectors instances that you create.

. Click *Next*.

. Configure the core configuration for your connector:
.. Provide a name for the connector. 
.. Type the *Client ID* and *Client Secret* of the service account that you created for Connectors and then click *Next*.

. Provide connector-specific configuration. For the *Data Generator*, provide the following information:
.. *Data shape Format*: Accept the default, `application/octet-stream`.
.. *Topic Names*: Type the name of the topic that you created for Connectors. For example, type *test-topic*.
.. *Content Type*: Accept the default, `text/plain`.
.. *Message*: Type the content of the message that you want the Connector instance to send to the Kafka topic. For example, type `Hello World!`.
.. *Period*: Specify the interval (in milliseconds) at which you want the Connectors instance to send messages to the Kafka topic. For example, specify `10000`, to send a message every 10 seconds.

. Optionally, configure the error handling policy for your Connectors instance. 
+
The options are:
+
* *stop* - (the default) The Connectors instance shuts down when it encounters an error. 
* *log* - The Connectors instance sends errors to its log.
* *dead letter queue* - The Connectors instance sends messages that it cannot handle to a dead letter topic that you define for the Connectors Kafka instance.
+
For example, select *log*.

. Click *Next*.

. Review the summary of the configuration properties and then click *Create Connectors instance*.
+
Your Connectors instance is listed in the table of Connectors. After a couple of seconds, the status of your Connectors instance changes to the *Ready* state and it starts producing messages and sending them to its associated Kafka topic.
+
From the connectors table, you can stop, start, and delete your Connectors instance, as well as edit its configuration, by clicking the options icon (three vertical dots).

.Verification
ifdef::qs[]
* Did you create an instance of the Data Generator connector?
endif::[]

In the next procedure, you can verify that the source Connectors instance is sending messages as expected by creating a sink Connectors instance that consumes the messages.

[id="proc-creating-sink-connector_{context}"]
== Creating a Connectors instance for a data sink

[role="_abstract"]
A *sink* connector consumes messages from a Kafka topic and sends them to an external system. 

For this example, you use the *HTTP Sink* connector which consumes the Kafka messages (produced by the source Connectors instance) and sends the messages to an HTTP endpoint.

ifndef::qs[]
.Prerequisites
* You're logged in to the OpenShift Application Services web console at {service-url-connectors}[^].
* You created the source Connectors instance as described in _{base-url}{getting-started-url-conectors}/proc-creating-source-connector_getting-started-connectors[Creating a Connectors instance for a data source^]_.
* For the data sink example, open the free https://webhook.site[Webhook.site^] in a browser window. The Webhook.site page provides a unique URL that you copy for use as an HTTP data sink.
endif::[]

.Procedure
 
. In the OpenShift Application Services web console, select *Connectors* and then click *Create Connectors instance*. 

. Select the sink connector that you want to use:
.. For example, type *http* in the search field. The list of connectors filters to show the *HTTP Sink* connector. 
.. Click the *HTTP Sink connector* card and then click *Next*. 

. Select the {product-kafka} instance for the connector to work with. 
+
For example, select *test*  and then click *Next*.

. On the *Namespace* page, click the *eval namespace* that you created when you created the source connector. 

//. On the *Namespace* page, the namespace that you select depends on your OpenShift Dedicated environment.
//+
//If you are using a trial cluster on your own OpenShift Dedicated environment, select the card for the namespace that was created when you added the Connectors service to your trial cluster.
//+
//If you are using the evaluation OpenShift Dedicated environment, click the *eval namespace* that you created when you created the source connector.

. Click *Next*.

. Provide the core configuration for your connector:
.. Type a unique name for the connector. 
.. Type the *Client ID* and *Client Secret* of the service account that you created for Connectors and then click *Next*.

. Provide the connector-specific configuration for your connector. For the *HTTP sink connector*, provide the following information:

.. *Data shape Format*: Accept the default, `application/octet-stream`.
.. *Method*: Accept the default, `POST`.
.. *URL*: Type your unique URL from the link:https://webhook.site[webhook.site^].
.. *Topic Names*: Type the name of the topic that you used for the source Connectors instance. For example, type *test-topic*.

. Optionally, configure the error handling policy for your Connectors instance. For example, select *log* and then click *Next*.

. Review the summary of the configuration properties and then click *Create Connectors instance*.
+
Your Connectors instance is listed in the table of Connectors. 
+
After a couple of seconds, the status of your Connectors instance changes to the *Ready* state. It consumes messages from the associated Kafka topic and sends them to the data sink (for this example, the data sink is the HTTP URL that you provided).

.Verification

Open the browser tab to your custom URL for the link:https://webhook.site[webhook.site^] to see the HTTP POST calls with the `"Hello World!!"` messages (that you defined in the source connector).


ifdef::qs[]
[#conclusion]
Congratulations! You successfully completed the {product-long-connectors} Getting Started quick start.
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]