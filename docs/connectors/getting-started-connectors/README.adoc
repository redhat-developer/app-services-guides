////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/

//OpenShift Application Services CLI
:rhoas-cli-base-url: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:rhoas-cli-ref-url: commands
:rhoas-cli-installation-url: rhoas/rhoas-cli-installation/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:rhoas-cli-getting-started-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:rhoas-cli-getting-started-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:product-long-connectors: OpenShift Connectors
:service-url-connectors: https://console.redhat.com/application-services/connectors
////
END GENERATED ATTRIBUTES
////

[id="chap-getting-started-connectors"]
= Getting started with {product-long-connectors}
ifdef::context[:parent-context: {context}]
:context: getting-started-connectors

// Purpose statement for the assembly
[role="_abstract"]
As a developer of applications and services, you can use {product-long-connectors} to ...

// Condition out QS-only content so that it doesn't appear in docs.
// All QS anchor IDs must be in this alternate anchor ID format `[#anchor-id]` because the ascii splitter relies on the other format `[id="anchor-id"]` to generate module files.
ifdef::qs[]
[#description]
Learn how to create and set up connectors in {product-long-connectors}.

[#introduction]
Welcome to the quick start for {product-long-connectors}. In this quick start, you'll learn how to create a source connector and sink connector and send data to and from {product-kafka}. A source connector allows you to send data from an external system to {product-kafka}. A sink connector allows you to send data from {product-kafka} to an external system.

endif::[]

[id="proc-configuring-kafka-for-connectors_{context}"]
== Configuring the {product-kafka} instance for use with {product-long-connectors}

[role="_abstract"]
In this step you configure your {product-kafka} for use with {product-long-connectors}. This involves creating topics and setting up access rules for service accounts.

.Procedure
. First step
. Second step

.Verification
ifdef::qs[]
* Have you completed these steps?
endif::[]

[id="proc-creating-source-connector_{context}"]
== Creating a {product-long-connectors} Source Connector

[role="_abstract"]
In this step you create and configure a source connector. A source connector consumes events from an external system and produces Kafka messages. In this quick start you use the Data Generator Source connector which will produce a Kafka message with a configurable payload at regular intervals to a Kafka topic on your {product-kafka} instance.

.Procedure
. In the {service-url-connectors}[^] web console, go to *Connectors* and click *Create connector instance*. Follow the guided steps to define the connector details. Click *Next* to complete each step and click *Finish* to finish the setup.
. In the first step you select the connector you want to use. You can browse through the catalog of available connectors. You can also search for a particular connector by name, and filter for sink or source connectors.
+
Enter *data* in the search box. You should see only one connector, called *Data Generator Connector*, which is the source connector you use in this quick start. Click the connector box to select the connector, and click *Next* to complete the step.

. Select the {product-kafka} instance for the connector to work with. The name of the instance corresponds to the Kafka instance you configured in the previous step. Click on the Kafka instance box to select the instance and click *Next* to complete the step.
+
NOTE: From this screen you can also create a new Kafka instance by clicking the *Create kafka instance* button.

. Select the OpenShift Dedicated cluster to host the connector instance. Select the OSD cluster by clicking the box representing the cluster, and click *Next* to complete the step.

. In this step you configure the common configuration for your connector. Provide a unique name for the connector and select *Automatically create a service account for this connector*. As an alternative you can also provide the Client ID and Client Secret of an existing service account. If you did set up the {product-kafka} ACL as explained in the previous task of this quick start, the newly created service account has sufficient rights to produce messages to the topic associated with the connector. Click *Next* to complete the step.

. In this step you provide the connector specific configuration.
.. Leave the *Data shape Format* to `application/octet-stream`
.. *Topic Names*: Enter the name of the topic you created in the previous task of this quick start.
.. *Content Type*: Leave to `text/plain`.
.. *Message*: Enter the content of the message that you want to send to the topic, for example `Hello World!`.
.. *Period*: The interval in milliseconds at which events will be sent. Set this to `10000`, which will produce an event every 10 seconds.

. Finally configure the error handler for the connector. You can choose between *stop* (the connector shuts down in case of errors), *log* (the error is logged) or *dead letter queue* (the events that cannot be handled by the connector are sent to a dead letter topic of your Kafka instance).
Select `log`.

. The next screen shows the configuration properties of your connector. Click *Create connector* to proceed with the deployment of the connector.
+
After you complete the connector setup, the new connector is listed in the connectors table. After a couple of seconds, the connector moves to the *Ready* state. At this point the connector starts producing messages to the Kafka topic associated with the connector.
+
From the connectors table, you can stop, start and delete the connector, as well as edit the connector configuration by clicking the options icon (three vertical dots).

.Verification
ifdef::qs[]
* Have you completed these steps?
endif::[]

[id="proc-creating-sink-connector_{context}"]
== Creating a {product-long-connectors} Sink Connector

[role="_abstract"]
In this step you create and configure a sink connector. A sink connector consumes events from an Kafka topic and sends them to an external system. In this quickstart you use the HTTP Sink connector which consumes the messages produced by the source connector configured in the previous step and calls a HTTP endpoint with the message payload.

.Procedure
. Before continuing with the setup of the source connector, you need to setup a HTTP endpoint for the sink connector. One way to do so is to use the free link:https://webhook.site[webhook.site^] service.
In a new browser tab, navigate to the link:https://webhook.site[webhook.site^], where you will be given a unique URL that you can leverage as HTTP sink for the connector.
. In the {service-url-connectors}[^] web console, go to *Connectors* and click *Create connector instance*. Follow the guided steps to define the connector details. Click *Next* to complete each step and click *Finish* to finish the setup.
. In the first step you select the connector you want to use. You can browse through the catalog of available connectors. You can also search for a particular connector by name, and filter for sink or source connectors.
+
Enter *http* in the search box. You should see only one connector, called *HTTP Sink*, which is the sink connector you use in this quick start. Click the connector box to select the connector, and click *Next* to complete the step.

. Select the {product-kafka} instance for the connector to work with. The name of the instance corresponds to the Kafka instance you configured in the previous step. Click on the Kafka instance box to select the instance and click *Next* to complete the step.

. Select the OpenShift Dedicated cluster to host the connector instance. Select the OSD cluster by clicking the box representing the cluster, and click *Next* to complete the step.

. In this step you configure the common configuration for your connector. Provide a unique name for the connector and select *Automatically create a service account for this connector*. As an alternative you can also provide the Client ID and Client Secret of an existing service account. If you did set up the {product-kafka} ACL as explained in the previous task of this quick start, the newly created service account has sufficient rights to consume messages from the topic associated with the connector. Click *Next* to complete the step.

. In this step you provide the connector specific configuration.
.. Leave the *Data shape Format* to `application/octet-stream`
.. *Method*: Leave to `POST`.
.. *URL*: Enter your unique URL from link:https://webhook.site[webhook.site^].
.. *Topic Names*: Enter the name of the topic you created in the first task of this quick start. Use the same topic as for the data sink connector.

. Finally configure the error handler for the connector. You can choose between *stop* (the connector shuts down in case of errors), *log* (the error is logged) or *dead letter queue* (the events that cannot be handled by the connector are sent to a dead letter topic of your Kafka instance).
Select `log`.

. The next screen shows the configuration properties of your connector. Click *Create connector* to proceed with the deployment of the connector.
+
After you complete the connector setup, the new connector is listed in the connectors table. After a couple of seconds, the connector moves to the *Ready* state. At this point the connector starts consuming messages from the Kafka topic associated with the connector and sending them to the HTTP sink.

. In the browser tab pointing to link:https://webhook.site[webhook.site^] you should see the HTTP POST calls from the connector with the message contents as defined in the source connector.



.Verification
ifdef::qs[]
* Have you completed these steps?
endif::[]

ifdef::qs[]
[#conclusion]
Congratulations! You successfully completed the {product-long-connectors} Getting Started quick start.
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
