////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/

//OpenShift Application Services CLI
:rhoas-cli-base-url: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:rhoas-cli-ref-url: commands
:rhoas-cli-installation-url: rhoas/rhoas-cli-installation/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:rhoas-cli-getting-started-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:rhoas-cli-getting-started-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:product-long-connectors: OpenShift Connectors
:service-url-connectors: https://console.redhat.com/application-services/connectors
////
END GENERATED ATTRIBUTES
////

[id="chap-getting-started-connectors"]
= Getting started with {product-long-connectors}
ifdef::context[:parent-context: {context}]
:context: getting-started-connectors

// Purpose statement for the assembly
[role="_abstract"]
As a developer of applications and services, you can use {product-long-connectors} to create and configure connections between OpenShift Streams for Apache Kafka and third-party systems.

In this quick start example, you connect a data source (Data Generator) that generates Kafka messages and a data sink (an HTTP endpoint) that consumes the Kafka messages.

// Condition out QS-only content so that it doesn't appear in docs.
// All QS anchor IDs must be in this alternate anchor ID format `[#anchor-id]` because the ascii splitter relies on the other format `[id="anchor-id"]` to generate module files.
ifdef::qs[]
[#description]
Learn how to create and set up connectors in {product-long-connectors}.

[#introduction]
Welcome to the quick start for {product-long-connectors}. 

In this quick start, you learn how to create a source connector and sink connector and send data to and from {product-kafka}. 

A *source* connector allows you to send data from an external system to {product-kafka}. A *sink* connector allows you to send data from {product-kafka} to an external system. 

endif::[]

ifndef::qs[]
== Overview

{product-long-kafka} is a cloud service that simplifies the process of running Apache Kafka. Apache Kafka is an open-source, distributed, publish-subscribe messaging system for creating fault-tolerant, real-time data feeds.  

You can use {product-connectors} to configure communication between {product-kafka} instances and external services and applications. {product-connectors} allow you to configure how data moves from one endpoint to another without writing code. 

The following diagram illustrates how data flows from a source of data through a data source connector to a Kafka topic. And how data flows from a kafka topic to a  data sink connector to a data sink.

image::connectors-diagram.png[Illustration of data flow from data source through Kafka to data sink]


endif::[]

[id="proc-configuring-kafka-for-connectors_{context}"]
== Configuring the {product-kafka} instance for use with {product-long-connectors}

[role="_abstract"]
Configure your {product-kafka} for use with {product-long-connectors} by creating  Kafka topics, creating service accounts, and setting up access rules for the service accounts. The number of Kafka topics, service accounts that you create, and the access rules for the service accounts depend on your application. 

Kakfa topics store messages sent by producers (data sources) and make them available to consumers (data sinks). Service accounts allow you to connect and authenticate your Connectors with Kafka instances. Access rules for service accounts define how your Connectors accounts can access and use the associated Kafka instance and topics.

For this quick start, you create one Kafka topic, named *test*, one service account, and you provide all permissions on the service account.

ifndef::qs[]
.Prerequisites
* You're logged in to the {service-url-connectors}[^] web console.
* You've created a  {product-kafka} instance and the instance is in the *Ready* state.
For instructions on how to create a Kafka instance, see _Getting started with OpenShift Streams for Apache Kafka_. 
endif::[]

.Procedure
. Create a topic for connectors:
.. In the {service-url-connectors}[^] web console, select *Streams for Apache Kafka* > *Kafka Instances* and then click the name of the Kafka instance that you want to add a topic to.
.. Select the *Topics* tab, and then click *Create topic*.
.. Type a unique name for your topic. For the quick start example, type *test-topic* for the *Topic Name*. Accept the default settings for partitions and message retention, and replicas.
. Create a service account for connectors: 
.. In the web console, select *Service Accounts*, and then click *Create service account*.
.. Type a unique service account name (for example, *test-service-acct* ) and then click *Create*.
.. Copy the generated *Client ID* and *Client Secret* to a secure location. You'll use these credentials to configure connections to this service account.
.. Select the *I have copied the client ID and secret* option, and then click *Close*.

. Set the level of access for your new service account in the Access Control List (ACL) of the Kafka instance:
.. Select *Streams for Apache Kafka* > *Kafka Instances*, click the name of the Kafka instance that you want the service account to access.
.. Click the *Access* tab to view the current ACL for this instance and then click *Manage access*.
.. From the *Account* drop-down menu, select the service account that you previously created, and then click *Next*.
.. Under *Assign Permissions*, use the drop-down menu to select the *Consume from a topic* and the *Produce to a topic* permission options, and then set all resource identifiers and values to `is "*"`. 
+
The `is "*"` settings mean that the service account has rights to produce and receive messages for the Kafka topic that you created for Connectors. 

.Verification
ifdef::qs[]
* Have you completed these steps?
endif::[]

[id="proc-creating-source-connector_{context}"]
== Creating a Connectors instance for a data source

[role="_abstract"]
A source connector consumes events from an external data source and produces Kafka messages. For this quick start, you create an instance of the Data Generator Source connector. You configure the connector to listen for events from the data source and produce a Kafka message with a configurable payload for each event. The connector sends the messages at regular intervals to a Kafka topic on your {product-kafka} instance.


ifndef::qs[]
.Prerequisites
* You're logged in to the {service-url-connectors}[^] web console.
* You configured a Kafka instance for Connectors as described in _Configuring the {product-kafka} instance for use with {product-long-connectors}_.
endif::[]

.Procedure
. In the {service-url-connectors}[^] web console, select *Connectors* and then click *Create Connectors instance*.
. Select the connector that you want to use for a data source. You can browse through the catalog of available connectors. You can also search for a particular connector by name, and filter for sink or source connectors.
+
For example, to find the source connector for this quick start, type *data* in the search box. The list filters to show only the *Data Generator Connector* card, which is the source connector for this quick start. Click the card to select the connector, and then click *Next*.

. Select the {product-kafka} instance that you configured for Connectors. Click the Kafka instance's card and then click *Next*.
+
NOTE: If you have not already configured a {product-kafka} instance for Connectors, you can create a new Kafka instance by clicking *Create kafka instance*.

. Select the namespace to host your Connectors instance and then click *Next*.

. Configure the core configuration for your connector:
.. Provide a unique name for the connector. 
.. Type the *Client ID* and *Client Secret* of the service account that you created for Connectors and then click *Next*.

. Provide connector-specific configuration:
.. *Data shape Format*: Accept the default, `application/octet-stream`.
.. *Topic Names*: Type the name of the topic that you created for Connectors (for the quick start example, type *test-topic*).
.. *Content Type*: Accept the default, `text/plain`.
.. *Message*: Type the content of the message that you want to send to the topic, for the quick start example, type `Hello World!`.
.. *Period*: Specify the interval (in milliseconds) at which you want the Connectors instance to sends messages to the Kafka topic. For the quick start example, specify `10000`, to send a message every 10 seconds.

. Optionally, configure the error handling policy for your Connectors instance. For the quick start, select *log* (the Connectors instance sends errors to its log).  +
+
Other options are *stop* (the Connectors instance shuts down in case of errors), or *dead letter queue* (the Connectors instance sends messages that it cannot handle to a dead letter topic that you define for the Connectors Kafka instance). 

.. Click *Next*.

.. Review the summary of the configuration properties of your Connectors instance and then click *Create Connectors instance* to deploy it.
+
Your Connectors instance is listed in the table of Connectors. After a couple of seconds, the status of your Connectors instance changes to the *Ready* state and it starts producing messages and sending them to its associated Kafka topic.
+
From the connectors table, you can stop, start and delete your Connectors instance, as well as edit its configuration by clicking the options icon (three vertical dots).
+
In the next procedure, you can verify that the source Connectors instance is sending messages as expected by creating a sink Connectors instance that consumes the messages.

.Verification
ifdef::qs[]
* Have you completed these steps?
endif::[]

[id="proc-creating-sink-connector_{context}"]
== Creating a Connectors instance for a data sink

[role="_abstract"]
A sink connector consumes messages from a Kafka topic and sends them to an external system. In this quick start, you use the *HTTP Sink* connector which consumes the Kakfa messages (produced by the source Connectors instance) and sends the message payloads to an HTTP endpoint.

ifndef::qs[]
.Prerequisites
* You're logged in to the {service-url-connectors}[^] web console.
* You created the source Connectors instance as described in _Creating a Connectors instance for a data source_).
* You have a unique URL from the https://webhook.site[webhook.site].
endif::[]

.Procedure
 
. In the {service-url-connectors}[^] web console, select *Connectors* and then click *Create Connectors instance*. 

. Select the sink connector that you want to use:
.. For this quick start, type *http* in the search field. The list of connectors filters to show one connector, called *HTTP Sink*, which is the sink connector to use for this quick start. 
.. Click the *HTTP Sink connector* card and then click *Next*. 

. Select the {product-kafka} instance for the connector to work with. For the quick start, select *test*  and then click *Next*.

. Select the namespace to host your Connectors instance and then click *Next*.

. Configure the core configuration for your connector:
.. Provide a unique name for the connector. 
.. Type the *Client ID* and *Client Secret* of the service account that you created for Connectors and then click *Next*.

. Provide connector-specific configuration:
.. *Data shape Format*: Accept the default, `application/octet-stream`.
.. *Method*: Accept the default, `POST`.
.. *URL*: Enter your unique URL from link:https://webhook.site[webhook.site^].
.. *Topic Names*: Type the name of the topic that you used for the source Connectors instance (for the quick start example, type *test-topic*).

. Optionally, configure the error handling policy for your Connectors instance. For the quick start, select *log*  and then click *Next*.

.. Review the summary of the configuration properties of your Connectors instance and then click *Create Connectors instance* to deploy it.
+
Your Connectors instance is listed in the table of Connectors. After a couple of seconds, the status of your Connectors instance changes to the *Ready* state and it consuming messages from the associated Kafka topic and sends them to the data sink (for the quick start, the data sink is the HTTP url that you provided).

. Open the browser tab to link:https://webhook.site[webhook.site^] to see the HTTP POST calls with the message contents defined in the source connector.
// what are example messages?


.Verification
ifdef::qs[]
* Have you completed these steps?
endif::[]

ifdef::qs[]
[#conclusion]
Congratulations! You successfully completed the {product-long-connectors} Getting Started quick start.
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]