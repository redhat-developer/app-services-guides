////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//All OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/
:sso-token-url: https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
:cloud-console-url: https://console.redhat.com/
:service-accounts-url: https://console.redhat.com/application-services/service-accounts

//to avoid typos
:openshift: OpenShift
:openshift-dedicated: OpenShift Dedicated

//OpenShift Application Services CLI
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:command-ref-url-cli: commands
:installation-guide-url-cli: rhoas/rhoas-cli-installation/README.adoc
:service-contexts-url-cli: rhoas/rhoas-service-contexts/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:getting-started-rhoas-cli-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc
:message-browsing-url-kafka: kafka/message-browsing-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:getting-started-rhoas-cli-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:connectors: Connectors
:product-long-connectors: OpenShift Connectors
:product-connectors: Connectors
:product-version-connectors: 1
:service-url-connectors: https://console.redhat.com/application-services/connectors
:getting-started-url-connectors: connectors/getting-started-connectors/README.adoc
:getting-started-rhoas-cli-url-connectors: connectors/rhoas-cli-getting-started-connectors/README.adoc

//OpenShift API Designer
:product-long-api-designer: OpenShift API Designer
:product-api-designer: API Designer
:product-version-api-designer: 1
:service-url-api-designer: https://console.redhat.com/application-services/api-designer/
:getting-started-url-api-designer: api-designer/getting-started-api-designer/README.adoc

//OpenShift API Management
:product-long-api-management: OpenShift API Management
:product-api-management: API Management
:product-version-api-management: 1
:service-url-api-management: https://console.redhat.com/application-services/api-management/

////
END GENERATED ATTRIBUTES
////

[id="chap-getting-started-connectors"]
= Getting started with {product-long-connectors}
ifdef::context[:parent-context: {context}]
:context: getting-started-connectors

// Purpose statement for the assembly
[role="_abstract"]
As a developer of applications and services, you can use {product-long-connectors} to create and configure connections between {product-long-kafka} and third-party systems.

In this example, you connect a data source (a data generator) that creates Kafka messages and a data sink (an HTTP endpoint) that consumes the Kafka messages.

// Condition out QS-only content so that it doesn't appear in docs.
// All QS anchor IDs must be in this alternate anchor ID format `[#anchor-id]` because the ascii splitter relies on the other format `[id="anchor-id"]` to generate module files.
ifdef::qs[]
[#description]
====
Learn how to configure connections between {product-long-kafka} and third-party systems by using {product-long-connectors}.
====

[#introduction]
====
Welcome to the quick start for {product-long-connectors}.

In this quick start, you learn how to create a source connector and sink connector and send data to and from {product-kafka}.

A *source* connector allows you to send data from an external system to {product-kafka}. 

A *sink* connector allows you to send data from {product-kafka} to an external system.
====
endif::[]


[id="proc-verifying-prerequisites-for-connectors_{context}"]
== Verifying the prerequisites for using {product-long-connectors}

[role="_abstract"]

Before you use {product-connectors}, you must complete the following prerequisites: 

* Determine which {openshift} environment to use for deploying your {product-connectors} instances.

* Configure {product-long-kafka} for use with {product-connectors}.

*Determining which {openshift} environment to use for deploying your {connectors} instances*

For Service Preview, you have two choices:

* *The hosted evaluation environment*

** The {connectors} instances are hosted on a multitenant {openshift-dedicated} cluster that is owned by Red Hat.
** You can create four {connectors} instances at a time.
** The evaluation environment applies 48-hour expiration windows, as described in https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/8190dc9e-249c-4207-bd69-096e5dd5bc64[Red Hat {openshift} {connectors} Service Preview evaluation guidelines^].

* *Your own trial environment*

** You have access to your own {openshift-dedicated} trial environment.
** You can create an unlimited number of {connectors} instances.
** Your {openshift-dedicated} trial cluster expires after 60 days.
** A cluster administrator must install the {product-connectors} add-on as described in https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01[Adding the Red Hat {openshift} {connectors} add-on to your {openshift-dedicated} trial cluster^].

*Configuring {product-long-kafka} for use with {product-connectors}*

ifndef::qs[]
Complete the steps in _{base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^]_ to set up the following components:
endif::[]

ifdef::qs[]
Complete the steps in the link:https://console.redhat.com/application-services/learning-resources?quickstart=getting-started[Getting started with {product-long-kafka}] quick start to set up the following components:
endif::[]

* A *Kafka instance* that you can use for {product-connectors}.
* A *Kafka topic* to store messages sent by data sources and make the messages available to data sinks.
* A *service account* that allows you to connect and authenticate your {connectors} instances with your Kafka instance.
* *Access rules* for the service account that define how your {connectors} instances can access and use the topics in your Kafka instance.

ifdef::qs[]
.Procedure
Make sure that you have set up the prerequisite components.

.Verification
* Is the Kafka instance listed in the Kafka instances table and is the Kafka instance in the *Ready* state?
* Is your service account created in the *Service Accounts* page?
* Did you save your service account credentials to a secure location?
* Are the permissions for your service account listed in the *Access* page of the Kafka instance?
* Is the Kafka topic that you created for {connectors} listed in the topics table of the Kafka instance?
* If you plan to use a 60-day {openshift-dedicated} trial cluster to deploy your {product-connectors} instances, has a cluster administrator added the {product-connectors} add-on to your trial cluster?

endif::[]

ifndef::qs[]
.Verification
* Verify that the Kafka instance is listed in the Kafka instances table and that the state of the Kafka instance is shown as *Ready*.
* Verify that your service account was successfully created in the *Service Accounts* page.
* Verify that you saved your service account credentials to a secure location.
* Verify that the permissions for your service account are listed in the *Access* page of the Kafka instance.
* Verify that the Kafka topic that you created for {product-connectors} is listed in the Kafka instance's topics table.
* If you plan to use a 60-day {openshift-dedicated} trial cluster to deploy your {product-connectors} instances, verify that a cluster administrator added the {product-connectors} add-on to your trial cluster.

endif::[]


[id="proc-creating-source-connector_{context}"]
== Creating a {connectors} instance for a data source

[role="_abstract"]
A *source* connector consumes events from an external data source and produces Kafka messages.

For this example, you create an instance of the *Data Generator* source connector.

You configure your connector to listen for events from the data source and produce a Kafka message for each event.

The connector sends the messages at regular intervals to the Kafka topic that you created for your {connectors} instances.

ifndef::qs[]
.Prerequisites
* You're logged in to the {product-long-connectors} web console at {service-url-connectors}[^].
endif::[]

.Procedure
. In the {product-long-connectors} web console, select *{connectors}* and then click *Create {connectors} instance*.
. Select the connector that you want to use for connecting to a data source.
+
You can browse through the catalog of available connectors. You can also search for a particular connector by name, and filter for sink or source connectors.
+
For example, to find the *Data Generator* source connector, type *data* in the search box. The list filters to show only the *Data Generator Connector* card.
+
Click the card to select the connector, and then click *Next*.

. For *Kafka instance*, click the card for the {product-kafka} instance that you configured for {connectors}, and then click *Next*.

. On the *Namespace* page, the namespace that you select depends on your {openshift-dedicated} environment. The namespace is the deployment space that hosts your {connectors} instances.
+
If you're using a trial cluster in your own {openshift-dedicated} environment, select the card for the namespace that was created when a system administrator added the {connectors} service to your trial cluster, as described in https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01[Adding the Red Hat {openshift} {connectors} add-on to your {openshift-dedicated} trial cluster^].
+
If you're using the evaluation {openshift-dedicated} environment, click *Register eval namespace* to provision a namespace for hosting the {connectors} instances that you create.

. Click *Next*.

. Configure the core configuration for your {connectors} instance:
.. Type a name for your {connectors} instance.
.. Type the *Client ID* and *Client Secret* of the service account that you created for {connectors} and then click *Next*.
. Provide connector-specific configuration. For the *Data Generator*, provide the following information:
.. *Data shape Format*: Accept the default, `application/octet-stream`.
.. *Topic Names*: Type the name of the topic that you created for {connectors}. For example, type *test-topic*.
.. *Content Type*: Accept the default, `text/plain`.
.. *Message*: Type the content of the message that you want the {connectors} instance to send to the Kafka topic. For example, type `Hello World!`.
.. *Period*: Specify the interval (in milliseconds) at which you want the {connectors} instance to send messages to the Kafka topic. For example, specify `10000`, to send a message every 10 seconds.

. Optionally, configure the error handling policy for your {connectors} instance.
+
The options are:
+
* *stop*: (the default) The {connectors} instance shuts down when it encounters an error.
* *log*: The {connectors} instance sends errors to its log.
* *dead letter queue*: The {connectors} instance sends messages that it cannot handle to a dead letter topic that you define for the {connectors} Kafka instance.
+
For example, accept the default *stop* option.

. Click *Next*.

. Review the summary of the configuration properties and then click *Create {connectors} instance*.
+
Your {connectors} instance is listed in the table of {connectors}. After a couple of seconds, the status of your {connectors} instance changes to the *Ready* state and it starts producing messages and sending them to its associated Kafka topic.
+
From the {connectors} table, you can stop, start, and delete your {connectors} instance, as well as edit its configuration, by clicking the options icon (three vertical dots).

.Verification
ifdef::qs[]
* Does your source {connectors} instance generate messages?
endif::[]
ifndef::qs[]
* Verify that your source {connectors} instance generates messages.
endif::[]

.. In the {product-long-rhoas} web console, select *Streams for Apache Kafka* > *Kafka Instances*.
.. Click the Kafka instance that you created for connectors.
.. Click the *Topics* tab and then click the topic that you specified for your source {connectors} instance.
.. Click the *Messages* tab to see a list of `Hello World!` messages.


[id="proc-creating-sink-connector_{context}"]
== Creating a {connectors} instance for a data sink

[role="_abstract"]
A *sink* connector consumes messages from a Kafka topic and sends them to an external system.

For this example, you use the *HTTP Sink* connector which consumes the Kafka messages (produced by the source {connectors} instance) and sends the messages to an HTTP endpoint.

ifndef::qs[]
.Prerequisites
* You're logged in to the {product-long-connectors} web console at {service-url-connectors}[^].
* You created the source {connectors} instance as described in _Creating a {connectors} instance for a data source_.
* For the data sink example, open the free https://webhook.site[webhook.site^] in a browser window. The `webhook.site` page provides a unique URL that you copy for use as an HTTP data sink.
endif::[]

.Procedure

. In the {product-long-connectors} web console, click *Create {connectors} instance*.

. Select the sink connector that you want to use:
.. For example, type *http* in the search field. The list of {connectors} filters to show the *HTTP Sink* connector.
.. Click the *HTTP Sink connector* card and then click *Next*.

. Select the {product-kafka} instance for the connector to work with.
+
For example, select *test* and then click *Next*.

. On the *Namespace* page, the namespace that you select depends on your {openshift-dedicated} environment. The namespace is the deployment space that hosts your {connectors} instances.
+
If you're using a trial cluster on your own {openshift-dedicated} environment, select the card for the namespace that was created when you added the {connectors} service to your trial cluster.
+
If you're using the evaluation {openshift-dedicated} environment, click the *eval namespace* that you created when you created the source connector.

. Click *Next*.

. Provide the core configuration for your connector:
.. Type a unique name for the connector.
.. Type the *Client ID* and *Client Secret* of the service account that you created for {connectors} and then click *Next*.

. Provide the connector-specific configuration for your {connectors} instance. For the *HTTP sink connector*, provide the following information:

.. *Data shape Format*: Accept the default, `application/octet-stream`.
.. *Method*: Accept the default, `POST`.
.. *URL*: Type your unique URL from the link:https://webhook.site[webhook.site^].
.. *Topic Names*: Type the name of the topic that you used for the source {connectors} instance. For example, type *test-topic*.

. Optionally, configure the error handling policy for your {connectors} instance. For example, select *log* and then click *Next*.

. Review the summary of the configuration properties and then click *Create {connectors} instance*.
+
Your {connectors} instance is listed in the table of {connectors}.
+
After a couple of seconds, the status of your {connectors} instance changes to the *Ready* state. It consumes messages from the associated Kafka topic and sends them to the data sink (for this example, the data sink is the HTTP URL that you provided).

.Verification
ifdef::qs[]
* Open a web browser tab to your custom URL for the link:https://webhook.site[webhook.site^]. Do you see HTTP POST calls with `"Hello World!!"` messages?

endif::[]

ifndef::qs[]
* Verify that you see HTTP POST calls with `"Hello World!!"` messages. Open a web browser tab to your custom URL for the link:https://webhook.site[webhook.site^].
endif::[]



ifdef::qs[]
[#conclusion]
====
Congratulations! You successfully completed the {product-long-connectors} Getting Started quick start.
====
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
