////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//All OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:product-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/
:sso-token-url: https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
:cloud-console-url: https://console.redhat.com/
:service-accounts-url: https://console.redhat.com/application-services/service-accounts
:rh-sso-url: https://sso.redhat.com

//OpenShift
:openshift: OpenShift
:osd-name: OpenShift Dedicated
:osd-name-short: OpenShift Dedicated
:rosa-name: OpenShift Service for AWS
:rosa-name-short: OpenShift Service for AWS

//OpenShift Application Services CLI
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:command-ref-url-cli: commands
:installation-guide-url-cli: rhoas/rhoas-cli-installation/README.adoc
:service-contexts-url-cli: rhoas/rhoas-service-contexts/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:getting-started-rhoas-cli-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc
:message-browsing-url-kafka: kafka/message-browsing-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:getting-started-rhoas-cli-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:connectors: Connectors
:product-long-connectors: OpenShift Connectors
:product-connectors: Connectors
:product-version-connectors: 1
:service-url-connectors: https://console.redhat.com/application-services/connectors
:getting-started-url-connectors: connectors/getting-started-connectors/README.adoc
:getting-started-rhoas-cli-url-connectors: connectors/rhoas-cli-getting-started-connectors/README.adoc

//OpenShift API Designer
:product-long-api-designer: OpenShift API Designer
:product-api-designer: API Designer
:product-version-api-designer: 1
:service-url-api-designer: https://console.redhat.com/application-services/api-designer/
:getting-started-url-api-designer: api-designer/getting-started-api-designer/README.adoc

//OpenShift API Management
:product-long-api-management: OpenShift API Management
:product-api-management: API Management
:product-version-api-management: 1
:service-url-api-management: https://console.redhat.com/application-services/api-management/

////
END GENERATED ATTRIBUTES
////

[id="chap-getting-started-connectors"]
= Getting started with {product-long-connectors}
ifdef::context[:parent-context: {context}]
:context: getting-started-connectors

// Purpose statement for the assembly
[role="_abstract"]
As a developer of applications and services, you can use {product-long-connectors} to create and configure connections between {product-long-kafka} and third-party systems.

In this example, you connect a data source (a data generator) that creates Kafka messages and a data sink (an HTTP endpoint) that consumes the Kafka messages.

// Condition out QS-only content so that it doesn't appear in docs.
// All QS anchor IDs must be in this alternate anchor ID format `[#anchor-id]` because the ascii splitter relies on the other format `[id="anchor-id"]` to generate module files.

ifndef::qs[]
.Example flow of messages from a data source to a data sink
image::{imagesdir}/connectors-getting-started-connectors/connectors-example-diagram.png[Image of data flowing from a data source to a data sink]
endif::[]

ifdef::qs[]
[#description]
====
Configure connections between an {product-kafka} instance and third-party systems.
====

[#introduction]
====
Welcome to the quick start for {product-long-connectors}.

In this quick start, you learn how to create a source connector and sink connector and send data to and from {product-kafka}.

A _source_ connector allows you to send data from an external system to {product-kafka}.

A _sink_ connector allows you to send data from {product-kafka} to an external system.
====
endif::[]


[id="proc-verifying-prerequisites-for-connectors_{context}"]
== Verifying the prerequisites for using {product-long-connectors}

[role="_abstract"]

Before you use {product-connectors}, you must complete the following prerequisites:

* Determine which {openshift} environment to use for your _{connectors} namespace_. The {connectors} namespace is where your {product-connectors} instances are deployed.

* Configure {product-long-kafka} for use with {product-connectors}.

*Determining which {openshift} environment to use for your {connectors} namespace*

You have three choices:

* *The hosted preview environment*

** The {connectors} instances are hosted on a multitenant {osd-name-short} cluster that is owned by Red Hat.
** You can create four {connectors} instances at a time.
** The preview environment applies 48-hour expiration windows, as described in https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/8190dc9e-249c-4207-bd69-096e5dd5bc64[Red Hat {openshift} {connectors} Preview guidelines^].

* *Your own {osd-name} Trial environment*

** You have access to your own {osd-name} Trial environment.
** You can create an unlimited number of {connectors} instances.
** Your {osd-name-short} Trial cluster expires after 60 days.
** A cluster administrator must install the {product-connectors} add-on as described in https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01[Adding the Red Hat {openshift} {connectors} add-on to your {openshift} cluster^].

* *Your own {rosa-name} cluster*

** You have access to your own {rosa-name} (ROSA) environment.
** You can create {connectors} instances depending on your subscription, as described in https://access.redhat.com/articles/6990631[Red Hat OpenShift Connectors Tiers^].
** A cluster administrator must install the {product-connectors} add-on as described in https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01[Adding the Red Hat {openshift} {connectors} add-on to your {openshift} cluster^].

*Configuring {product-long-kafka} for use with {product-connectors}*

ifndef::qs[]
Complete the steps in {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^] to set up the following components:
endif::[]

ifdef::qs[]
Complete the steps in the link:https://console.redhat.com/application-services/learning-resources?quickstart=getting-started[Getting started with {product-long-kafka}] quick start to set up the following components:
endif::[]

* A _Kafka instance_ that you can use for {product-connectors}. For this example, the Kafka instance is `test-connect`.
* A _Kafka topic_ to store messages sent by data sources and make the messages available to data sinks. For this example, the Kafka topic is `test-topic`.
* A _service account_ that allows you to connect and authenticate your {connectors} instances with your Kafka instance.
* _Access rules_ for the service account that define how your {connectors} instances can access and use the topics in your Kafka instance.

ifdef::qs[]
.Procedure
Make sure that you have set up the prerequisite components.

.Verification
* Is the Kafka instance listed on the *Kafka Instances* page and is the Kafka instance in the *Ready* state?
* Is your service account created on the *Service Accounts* page?
* Did you save your service account credentials to a secure location?
* Are the permissions for your service account listed on the *Access* page of the Kafka instance?
* Is the Kafka topic that you created for {connectors} listed on the *Topics* page of the Kafka instance?
* If you plan to use your own {openshift} cluster ({osd-name-short} Trial or ROSA) to deploy your {product-connectors} instances, has a cluster administrator added the {product-connectors} add-on to your Trial cluster?

endif::[]

ifndef::qs[]
.Verification
* Verify that the Kafka instance is listed on the *Kafka Instances* page and that the state of the Kafka instance is shown as *Ready*.
* Verify that your service account was successfully created on the *Service Accounts* page.
* Verify that you saved your service account credentials to a secure location.
* Verify that the permissions for your service account are listed on the *Access* page of the Kafka instance.
* Verify that the Kafka topic that you created for {product-connectors} is listed on the *Topics* page of the Kafka instance.
* If you plan to use your own {openshift} cluster ({osd-name-short} Trial or ROSA) to deploy your {product-connectors} instances, verify that a cluster administrator added the {product-connectors} add-on to your Trial cluster.

endif::[]


[id="proc-creating-source-connector_{context}"]
== Creating a {connectors} instance for a data source

[role="_abstract"]
A _source_ connector consumes events from an external data source and produces Kafka messages.

You configure your {connectors} instance to listen for events from the data source and produce a Kafka message for each event. Your {connectors} instance sends the messages at regular intervals to the Kafka topic that you created for {connectors}.

For this example, you create an instance of the Data Generator source connector. The Data Generator is provided for development and testing purposes. You specify the text for a message and how often to send the message.

.Prerequisites

* If you want to use a dead letter queue (DLQ) to handle any messaging errors, create a Kafka topic for the DLQ.

ifndef::qs[]
* You're logged in to the {product-long-connectors} web console at {service-url-connectors}[^].
endif::[]

.Procedure
. In the {product-long-connectors} web console, click *Create a {connectors} instance*.
. Select the connector that you want to use for connecting to a data source.
+
You can browse through the catalog of available connectors. You can also search for a particular connector by name, and filter for sink or source connectors.
+
For example, to find the Data Generator source connector, type `data` in the search box. The list is filtered to show only the *Data Generator source* card.
+
Click the card to select the connector, and then click *Next*.

. On the *Kafka Instance* page, click the card for the {product-kafka} instance that you configured for {connectors}. For example, click the *test-connect* card.
+
Click *Next*.

. On the *Deployment* page, the namespace that you select depends on your {openshift} environment.
+
If you're using your own {openshift} environment, select the card for the namespace that was created when a cluster administrator added the {connectors} service to your cluster, as described in https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01[Adding the Red Hat {openshift} {connectors} add-on to your {openshift} cluster^].
+
If you're using the hosted preview environment, click *Create preview namespace* to provision a namespace for hosting the {connectors} instances that you create.
+
Click *Next*.

. Specify the core configuration for your {connectors} instance:
.. Type a name for your {connectors} instance. For example, type `hello world generator`.
.. In the *Client ID* and *Client Secret* fields, type the credentials for the service account that you created for {connectors} and then click *Next*.
. Provide connector-specific configuration. For the Data Generator, provide the following information:
.. *Topic Name*: Type the name of the Kafka topic that you created for {connectors}. For example, type `test-topic`.
.. *Content Type*: Accept the default, `text/plain`.
.. *Message*: Type the content of the message that you want the {connectors} instance to send to the Kafka topic. For example, type `Hello World!!`.
.. *Period*: Specify the interval (in milliseconds) at which you want the {connectors} instance to send messages to the Kafka topic. For example, to send a message every 10 seconds, specify `10000`.
.. *Data Shape Produces Format*: Accept the default, `application/octet-stream`.
+
Click *Next*.

. Select one of the following error handling policies for your {connectors} instance:
+
* *Stop*: If a message fails to send, the {connectors} instance stops running and changes its status to the *Failed* state. You can view the error message.
* *Ignore*: If a message fails to send, the {connectors} instance ignores the error and continues to run. No error message is logged.
* *Dead letter queue*: If a message fails to send, the {connectors} instance sends error details to the Kafka topic that you created for the DLQ.
+
Click *Next*.

. Review the summary of the configuration properties and then click *Create {connectors} instance*.
+
Your {connectors} instance is listed on the *{connectors} Instances* page. After a couple of seconds, the status of your {connectors} instance changes to the *Ready* state and it starts producing messages and sending them to its associated Kafka topic.
+
From the *{connectors} Instances* page, you can stop, start, duplicate, and delete your {connectors} instance, as well as edit its configuration, by clicking the options icon (three vertical dots).

.Verification
ifdef::qs[]
* Does your source {connectors} instance generate messages?
endif::[]
ifndef::qs[]
* Verify that your source {connectors} instance generates messages.
endif::[]

.. In the {product-long-rhoas} web console, select *Streams for Apache Kafka* > *Kafka Instances*.
.. Click the Kafka instance that you created for connectors. For example, click *test-connect*.
.. Click the *Topics* tab and then click the topic that you specified for your source {connectors} instance. For example, click *test-topic*.
.. Click the *Messages* tab to see a list of `Hello World!!` messages.


[id="proc-creating-sink-connector_{context}"]
== Creating a {connectors} instance for a data sink

[role="_abstract"]
A _sink_ connector consumes messages from a Kafka topic and sends them to an external system.

For this example, you use the *HTTP Sink* connector which consumes the Kafka messages (produced by your Data Generator source {connectors} instance) and sends the messages to an HTTP endpoint.

.Prerequisites

ifndef::qs[]
* You're logged in to the {product-long-connectors} web console at {service-url-connectors}[^].
endif::[]
* You created a Data Generator source {connectors} instance.
* For the data sink example, open the free https://webhook.site[webhook.site^] in a browser window. The `webhook.site` page provides a unique URL that you copy for use as an HTTP data sink.
* If you want to use a dead letter queue (DLQ) to handle any messaging errors, create a Kafka topic for the DLQ.

.Procedure

. In the {product-long-connectors} web console, click *Create {connectors} instance*.

. Select the sink connector that you want to use:
.. For example, type `http` in the search field. The list of {connectors} is filtered to show the *HTTP sink* connector.
.. Click the *HTTP sink* card and then click *Next*.

. On the *Kafka Instance* page, select the {product-kafka} instance for the connector to work with. For example, select *test-connect*.
+
Click *Next*.

. On the *Deployment* page, the namespace that you select depends on your {openshift} environment.
+
If you're using your own {openshift} environment, select the card for the namespace that was created when a cluster administrator added the {connectors} service to your cluster.
+
If you're using the hosted preview environment, click the *preview namespace* that you provisioned when you created the source connector.
+
Click *Next*.

. Provide the core configuration for your connector:
.. Type a unique name for the connector. For example, type `hello world receiver`.
.. In the *Client ID* and *Client Secret* fields, type the credentials for the service account that you created for {connectors} and then click *Next*.

. Provide the connector-specific configuration for your HTTP sink {connectors} instance:
.. *Topic Names*: Type the name of the topic that you used for the source {connectors} instance. For example, type `test-topic`.
.. *Method*: Accept the default, `POST`.
.. *URL*: Type your unique URL from the link:https://webhook.site[webhook.site^].
.. *Data Shape Consumes Format*: Accept the default, `application/octet-stream`.
+
Click *Next*.

. Select an error handling policy for your {connectors} instance. For example, select *Stop*.
+
Click *Next*.

. Review the summary of the configuration properties and then click *Create {connectors} instance*.
+
Your {connectors} instance is added to the *{connectors} Instances* page.
+
After a couple of seconds, the status of your {connectors} instance changes to the *Ready* state. It consumes messages from the associated Kafka topic and sends them to the data sink (for this example, the data sink is the HTTP URL that you provided).

.Verification
ifdef::qs[]
* Open a web browser tab to your custom URL for the link:https://webhook.site[webhook.site^]. Do you see HTTP POST calls with `"Hello World!!"` messages?

endif::[]

ifndef::qs[]
* Verify that you see HTTP POST calls with `"Hello World!!"` messages. Open a web browser tab to your custom URL for the link:https://webhook.site[webhook.site^].
endif::[]



ifdef::qs[]
[#conclusion]
====
Congratulations! You successfully completed the {product-long-connectors} Getting Started quick start.
====
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
