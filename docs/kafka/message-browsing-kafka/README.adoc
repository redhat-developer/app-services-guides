////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//All OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/
:sso-token-url: https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
:cloud-console-url: https://console.redhat.com/
:service-accounts-url: https://console.redhat.com/application-services/service-accounts

//to avoid typos
:openshift: OpenShift
:openshift-dedicated: OpenShift Dedicated

//OpenShift Application Services CLI
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:command-ref-url-cli: commands
:installation-guide-url-cli: rhoas/rhoas-cli-installation/README.adoc
:service-contexts-url-cli: rhoas/rhoas-service-contexts/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:getting-started-rhoas-cli-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc
:message-browsing-url-kafka: kafka/message-browsing-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:getting-started-rhoas-cli-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:connectors: Connectors
:product-long-connectors: OpenShift Connectors
:product-connectors: Connectors
:product-version-connectors: 1
:service-url-connectors: https://console.redhat.com/application-services/connectors
:getting-started-url-connectors: connectors/getting-started-connectors/README.adoc
:getting-started-rhoas-cli-url-connectors: connectors/rhoas-cli-getting-started-connectors/README.adoc

//OpenShift API Designer
:product-long-api-designer: OpenShift API Designer
:product-api-designer: API Designer
:product-version-api-designer: 1
:service-url-api-designer: https://console.redhat.com/application-services/api-designer/
:getting-started-url-api-designer: api-designer/getting-started-api-designer/README.adoc

//OpenShift API Management
:product-long-api-management: OpenShift API Management
:product-api-management: API Management
:product-version-api-management: 1
:service-url-api-management: https://console.redhat.com/application-services/api-management/

////
END GENERATED ATTRIBUTES
////

[id="chap-browsing-messages"]
= Browsing messages in the {product-long-kafka} web console
ifdef::context[:parent-context: {context}]
:context: browsing-messages

// Purpose statement for the assembly
[role="_abstract"]

As a developer or administrator, you can use the {product-long-kafka} web console to view and inspect messages for a Kafka topic. You might use this functionality to verify that a client is producing messages to the expected topic partition or that messages have the expected content.

When you select a topic in the web console, you can use the *Messages* tab to view a list of messages for that topic. You can filter the list of messages in the following ways:

* Specify a partition and see messages sent to the partition.
* Specify a partition and offset and see messages sent to the partition from that offset.
* Specify a partition and a timestamp (that is, date and time) value. See messages sent to the partition from that date and time.
* Specify a partition and a Unix epoch timestamp value and see messages sent to the partition from that epoch timestamp value.

//Additional line break to resolve mod docs generation error.

[id="proc-browsing-messages-for-a-topic_{context}"]
== Browsing messages for a topic

The following procedure shows how to filter and inspect a list of messages for a topic in the {product-long-kafka} web console.

.Prerequisites

* You have a Kafka instance with a topic that contains some messages. To learn how to create your _first_ Kafka instance and topic and then send messages to the topic that will appear on the *Messages* page, see the following guides:
+
** {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^]
** {base-url}{kafka-bin-scripts-url-kafka}[Configuring and connecting Kafka scripts with {product-long-kafka}^]

.Procedure

. In the {product-kafka} web console, click *Kafka Instances* in the left navigation menu.
. On the *Kafka Instances* page, click a Kafka instance.
. In your Kafka instance, click the *Topics* tab.
. In the topics table, click a Kafka topic that you want to inspect.
. In your topic, click the *Messages* tab.
+
By default, the *Messages* page shows messages in *partition 0* of your topic. You can change this partition value, as described later in the procedure.
+
The messages table includes columns for the following topic and message properties:
+
--
* Partition
* Offset
* Timestamp (date and time)
* Key
* Headers
* Value
--

. To see complete data for a message, click *Show more* in the *Value*, *Key*, or *Headers* column.
+
A *Message* pane opens to show the complete message data. This pane also shows the epoch timestamp value.
+
[NOTE]
--
If you're using a schema in {product-long-registry} with your topic, the *Messages* page does not deserialize messages that a producer application serialized to conform to that schema. To view such messages, you must configure a consumer application to use a Kafka deserializer. For more information, see https://access.redhat.com/documentation/en-us/red_hat_integration/2021.q3/html-single/service_registry_user_guide/index#configuring-kafka-client-serdes[Configuring Kafka serializers/deserializers in Java clients^].

Similarly, if a message is encoded (for example, in a format such as UTF-8 or Base64), the *Messages* page does not decode the message.
--

. To copy the full message value or header data, click the copy icon next to the data in the *Message* pane.

. To see messages for a different topic partition, select a new value in the *Partition* list.
+
NOTE: If you have many partitions, you can filter the values shown in the *Partition* list by typing a value in the field.

. To further refine the list of messages in the table, use the filter controls at the top of the *Messages* page.
+
--
* To filter messages by topic partition and offset, perform the following actions:
... In the *Partition* field, select a topic partition.
... Click the arrow next to `Latest messages` and select `Offset` from the list.
... In the *Specify offset* field, type an offset value.
... To apply your filter settings, click the search (magnifying glass) icon.

* To filter messages by topic partition and date and time, perform the following actions:
... In the *Partition* field, select a topic partition.
... Click the arrow next to `Latest messages` and select `Timestamp` from the list.
+
Additional selection tools appear.
... Use the additional selection tools to set date and time values. Alternatively, type a date and time value in the format shown in the field.
... To apply your filter settings, click the search (magnifying glass) icon.

* To filter messages by topic partition and epoch timestamp, perform the following actions:
... In the *Partition* field, select a topic partition.
... Click the arrow next to `Latest messages` and select `Epoch timestamp` from the list.
... In the *Epoch timestamp* field, type or paste an epoch timestamp value.
+
NOTE: You can easily convert a human-readable date and time to an epoch value using a https://www.epochconverter.com/[timestamp conversion tool^].
... To apply your filter settings, click the search (magnifying glass) icon.

--
+
Based on your filter settings, the *Messages* page automatically reloads the list of messages in the table.


ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
