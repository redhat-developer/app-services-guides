////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//All OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:product-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/
:sso-token-url: https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
:cloud-console-url: https://console.redhat.com/
:service-accounts-url: https://console.redhat.com/application-services/service-accounts
:rh-sso-url: https://sso.redhat.com

//OpenShift
:openshift: OpenShift
:osd-name: OpenShift Dedicated
:osd-name-short: OpenShift Dedicated
:rosa-name: OpenShift Service for AWS
:rosa-name-short: OpenShift Service for AWS

//OpenShift Application Services CLI
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:command-ref-url-cli: commands
:installation-guide-url-cli: rhoas/rhoas-cli-installation/README.adoc
:service-contexts-url-cli: rhoas/rhoas-service-contexts/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:getting-started-rhoas-cli-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc
:message-browsing-url-kafka: kafka/message-browsing-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:getting-started-rhoas-cli-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:connectors: Connectors
:product-long-connectors: OpenShift Connectors
:product-connectors: Connectors
:product-version-connectors: 1
:service-url-connectors: https://console.redhat.com/application-services/connectors
:getting-started-url-connectors: connectors/getting-started-connectors/README.adoc
:getting-started-rhoas-cli-url-connectors: connectors/rhoas-cli-getting-started-connectors/README.adoc
:addon-url-connectors: https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01


//OpenShift API Designer
:product-long-api-designer: OpenShift API Designer
:product-api-designer: API Designer
:product-version-api-designer: 1
:service-url-api-designer: https://console.redhat.com/application-services/api-designer/
:getting-started-url-api-designer: api-designer/getting-started-api-designer/README.adoc

//OpenShift API Management
:product-long-api-management: OpenShift API Management
:product-api-management: API Management
:product-version-api-management: 1
:service-url-api-management: https://console.redhat.com/application-services/api-management/

////
END GENERATED ATTRIBUTES
////

[id="chap-kafka-bin-scripts"]
= Configuring and connecting Kafka scripts with {product-long-kafka}
ifdef::context[:parent-context: {context}]
:context: using-kafka-bin-scripts

// Purpose statement for the assembly
[role="_abstract"]
As a developer of applications and services, you can use Kafka scripts to produce and consume messages for Kafka instances in {product-long-kafka}. This is a useful way to test and debug your Kafka instances.
The Kafka scripts are a set of shell scripts that are included with the https://kafka.apache.org/downloads[Apache Kafka distribution^].

ifndef::community[]
NOTE: The Kafka scripts are part of the open source community version of Apache Kafka. The scripts are not a part of {product-kafka} and are therefore not supported by Red Hat.
endif::[]

When you download and extract the Apache Kafka distribution, the `bin/` directory of the distribution (or the `bin\windows\` directory if you're using Windows) contains a set of shell scripts that enable you to interact with your Kafka instance.
With the scripts, you can produce and consume messages using your Kafka instances. You can also perform various other operations against the Kafka APIs to administer topics, consumer groups, and other resources.

NOTE: The command examples in this quick start show how to use the Kafka scripts on Linux and macOS. If you're using Windows, use the Windows versions of the scripts. For example, instead of the `__<Kafka-distribution-dir>__/bin/kafka-console-producer.sh` script, use the `__<Kafka-distribution-dir>__\bin\windows\kafka-console-producer.bat` script.

.Prerequisites
* You have a running Kafka instance in {product-kafka} (see {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^]).
* You have a command-line terminal application.
* https://adoptopenjdk.net/[JDK^] 11 or later is installed. (The latest LTS version of OpenJDK is recommended.)
* You've downloaded the latest supported binary version of the https://kafka.apache.org/downloads[Apache Kafka distribution^]. You can check your Kafka version using the following command:
+
[source]
----
$ ./kafka-console-producer.sh --version
----

ifdef::qs[]
[#description]
====
Use Kafka scripts to interact with a Kafka instance in {product-long-kafka}.
====

[#introduction]
====
Welcome to the quick start for {product-long-kafka} with Kafka scripts. In this quick start, you'll learn how to use the Kafka scripts to produce and consume messages for your Kafka instances in {product-kafka}.
====
endif::[]

[id="proc-configuring-kafka-bin-scripts_{context}"]
== Configuring the Kafka scripts to connect to a Kafka instance

[role="_abstract"]
To enable the Kafka scripts to access a Kafka instance, you must configure a connection using the generated credentials for your {product-long-rhoas} service account. In this task, you create a configuration file that specifies these credential values.

.Prerequisites
* You have the generated credentials for your service account. To reset the credentials, use the {service-accounts-url}[Service Accounts^] page.
* You've set permissions for your service account to access resources in the Kafka instance. To verify the current permissions, select your Kafka instance in the {product-long-kafka} web console and click the *Access* tab. To learn more about setting permissions, see {base-url}{access-mgmt-url-kafka}[Managing account access in {product-long-kafka}^].

.Procedure

. In your Kafka distribution, navigate to the `config/` directory.

. Create a file called `{property-file-name}`.

. In the `{property-file-name}` file, set the SASL connection mechanism and the Kafka instance client credentials as shown in the following configuration. Replace the values with your own credential information. {product-kafka}  supports the SASL/OAUTHBEARER mechanism for authentication, which is the recommended authentication mechanism to use.
+
--
.Setting server and credential values
[source,subs="+quotes,attributes"]
----
sasl.mechanism=OAUTHBEARER
security.protocol=SASL_SSL

sasl.oauthbearer.token.endpoint.url= {sso-token-url}

sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
  scope="openid" \
  clientId="<client-id>" \
  clientSecret="<client-secret>" ;

sasl.login.callback.handler.class=org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler
----


--
. Save the file. You'll use this file in the next task to connect to your Kafka instance and produce messages.

[id="proc-producing-messages-kafka-bin-scripts_{context}"]
== Producing messages using Kafka scripts

[role="_abstract"]
In this task, you use the `kafka-console-producer` script to produce messages to a Kafka topic.

.Prerequisites

* You have a running Kafka instance in {product-long-kafka} (see {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^]).
* You have the bootstrap server endpoint for your Kafka instance. To get the server endpoint, select your Kafka instance in the {product-long-kafka} web console, select the options icon (three vertical dots), and click *Connection*.
* You've created the `{property-file-name}` file to store your service account credentials.

.Procedure
. On the command line, navigate to the `bin/` directory of your Kafka distribution.
. Use the `kafka-topics` script to create a Kafka topic, as shown in the following example. The example creates a topic called `my-other-topic` with the default settings. Replace _<bootstrap_server>_ with the bootstrap server endpoint for your own Kafka instance.
+
NOTE: For trial Kafka instances, the default value for the replication factor is 1.
+

--
.Using the `kafka-topics` script to create a Kafka topic
[source,subs="+quotes,+attributes"]
----
$ ./kafka-topics.sh --create --topic my-other-topic --partitions 1 --replication-factor 3 --bootstrap-server __<bootstrap_server>__ --command-config ../config/{property-file-name}
----
--
+
ifndef::community[]
NOTE: If you try to create a topic with a number of partitions that would cause the partition limit of the Kafka instance to be exceeded, you see an error message indicating this. For more information about partition limits for Kafka instances, see https://access.redhat.com/articles/5979061[{product-long-kafka} service limits].
endif::[]

. Start the `kafka-console-producer` script, as shown in the following example. The example uses the SASL/OAUTHBEARER authentication mechanism with the credentials that you saved in the `{property-file-name}` file. The command prepares a producer to send messages to the `my-other-topic` topic that you previously created.
+
.Starting the `kafka-console-producer` script
+
[source,subs="+quotes,+attributes"]
----
$ ./kafka-console-producer.sh --topic my-other-topic --bootstrap-server "__<bootstrap_server>__" --producer.config ../config/{property-file-name}
----

. With the `kafka-console-producer` script running, enter messages that you want to produce to the Kafka topic.
+
.Example messages to produce to the Kafka topic
+
[source]
----
>First message
>Second message
>Third message
----

. Keep the producer running so that you can use it again later, when you create a consumer.

.Verification
ifdef::qs[]
* Is the `kafka-console-producer` script still running without any errors in the terminal?
endif::[]
ifndef::qs[]
* Verify that the `kafka-console-producer` script is still running without any errors in the terminal.
endif::[]

[id="proc-consuming-messages-kafka-bin-scripts_{context}"]
== Consuming messages using Kafka scripts

[role="_abstract"]
In this task, you use the `kafka-console-consumer` script to consume the messages that you previously produced with the `kafka-console-producer` script.

.Prerequisites

* You used the `kafka-console-producer` script to produce example messages to a topic.

.Procedure
. Open a second terminal window or tab, separate from the producer.
. On the command line, navigate to the `bin/` directory of your Kafka distribution.

. Start the `kafka-console-consumer` script, as shown in the following example. The example uses the SASL/OAUTHBEARER authentication mechanism with the credentials that you saved in the `{property-file-name}` file. The command consumes and displays messages from the `my-other-topic` topic.
+
--
.Starting the `kafka-console-consumer` script

[source,subs="+quotes,+attributes"]
----
$ ./kafka-console-consumer.sh --topic my-other-topic --bootstrap-server "__<bootstrap_server>__" --from-beginning --consumer.config ../config/{property-file-name}
----

You see output like the following example:

[source]
----
First message
Second message
Third message
----
--

. If your producer is still running in a separate terminal, continue entering messages in the producer terminal and observe the messages being consumed in the consumer terminal.

NOTE: You can also use the {product-long-kafka} web console to browse messages in the Kafka topic. For more information, see {base-url}{message-browsing-url-kafka}[Browsing messages in the {product-long-kafka} web console^].

.Verification
ifdef::qs[]
* Is the `kafka-console-consumer` script running without any errors in the terminal?
* Did the `kafka-console-consumer` script display the messages from the `my-other-topic` example topic?
endif::[]
ifndef::qs[]
. Verify that the `kafka-console-consumer` script is running without any errors in the terminal.
. Verify that the `kafka-console-consumer` script displays the messages from the `my-other-topic` example topic.
endif::[]


ifdef::qs[]
[#conclusion]
====
Congratulations! You successfully completed the {product-kafka} Kafka scripts quick start, and are now ready to produce and consume messages in the service.
====
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
