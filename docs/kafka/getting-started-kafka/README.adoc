////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//All OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:product-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/
:sso-token-url: https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
:cloud-console-url: https://console.redhat.com/
:service-accounts-url: https://console.redhat.com/application-services/service-accounts
:rh-sso-url: https://sso.redhat.com

//OpenShift
:openshift: OpenShift
:osd-name: OpenShift Dedicated
:osd-name-short: OpenShift Dedicated
:rosa-name: OpenShift Service for AWS
:rosa-name-short: OpenShift Service for AWS

//OpenShift Application Services CLI
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:command-ref-url-cli: commands
:installation-guide-url-cli: rhoas/rhoas-cli-installation/README.adoc
:service-contexts-url-cli: rhoas/rhoas-service-contexts/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:getting-started-rhoas-cli-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc
:message-browsing-url-kafka: kafka/message-browsing-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:getting-started-rhoas-cli-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:connectors: Connectors
:product-long-connectors: OpenShift Connectors
:product-connectors: Connectors
:product-version-connectors: 1
:service-url-connectors: https://console.redhat.com/application-services/connectors
:getting-started-url-connectors: connectors/getting-started-connectors/README.adoc
:getting-started-rhoas-cli-url-connectors: connectors/rhoas-cli-getting-started-connectors/README.adoc

//OpenShift API Designer
:product-long-api-designer: OpenShift API Designer
:product-api-designer: API Designer
:product-version-api-designer: 1
:service-url-api-designer: https://console.redhat.com/application-services/api-designer/
:getting-started-url-api-designer: api-designer/getting-started-api-designer/README.adoc

//OpenShift API Management
:product-long-api-management: OpenShift API Management
:product-api-management: API Management
:product-version-api-management: 1
:service-url-api-management: https://console.redhat.com/application-services/api-management/

////
END GENERATED ATTRIBUTES
////

[id="chap-getting-started"]
= Getting started with {product-long-kafka}
ifdef::context[:parent-context: {context}]
:context: getting-started

// Purpose statement for the assembly
[role="_abstract"]
As a developer of applications and services, you can use {product-long-kafka} to create and set up Kafka instances and connect your applications and services to these instances. {product-kafka} is a managed cloud service that enables you to add Kafka data-streaming functionality in your applications without having to install, configure, run, and maintain your own Kafka clusters.

//For more overview information about {product-kafka}, see [variablized link to overview here https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/].


ifndef::community[]
.Prerequisites
* You have a {org-name} account.
//* You have a subscription to {product-long-kafka}. For more information about signing up, see *<@SME: Where to link?>*.
endif::[]

// Condition out QS-only content so that it doesn't appear in docs.
// All QS anchor IDs must be in this alternate anchor ID format `[#anchor-id]` because the ascii splitter relies on the other format `[id="anchor-id"]` to generate module files.
ifdef::qs[]
[#description]
====
Create and set up your first Apache Kafka instance in {product-long-kafka}.
====

[#introduction]
====
Welcome to the quick start for {product-long-kafka}. In this quick start, you'll learn how to create and inspect a Kafka instance, create a service account to connect an application or service to the instance, and create a topic in the instance.
====
endif::[]

[id="proc-creating-kafka-instance_{context}"]
== Creating a Kafka instance in {product-kafka}

[role="_abstract"]
Use the {product-long-kafka} web console to create and configure a Kafka instance for your applications or services. A Kafka instance in {product-kafka} includes a Kafka cluster, bootstrap server, and the configuration details needed for producers and consumers to connect to the instance.

ifndef::qs[]
.Prerequisites
* You're logged in to the {product-kafka} web console at {service-url-kafka}[^].
* You're familiar with the different Kafka instance sizes that you can create in {product-kafka}, and the capacities that correspond to those sizes. For more information about the capacities and service limits of your Kafka instance, see https://access.redhat.com/articles/5979061[Red Hat OpenShift Streams for Apache Kafka Service Limits].
endif::[]

.Procedure
. On the *Kafka Instances* page of the {product-kafka} {service-url-kafka}[web console^], click *Create Kafka instance*.
. Complete the *Create a Kafka instance* form to define the following instance details. Some instance details currently have only one option.
* *Instance name*: Enter a unique name for the Kafka instance, such as `my-first-kafka-instance`.
* *Cloud provider*: For a production Kafka instance, accept the default value of *Amazon Web Services*. If you're creating a trial instance, you also have the option to select *Google Cloud Platform*.
* *Cloud region*: Select a cloud provider region from the list.
* *Availability zones*: The selection for availability zones is preconfigured as *Multi* for a production instance and *Single* for a trial instance.
* *Size*: If you've purchased a subscription for {product-kafka}, use the slider to select an instance size. In {product-kafka}, you can create Kafka instances of 1 or 2 streaming units depending on the capacity required. If you're creating a trial instance, the instance size is preconfigured.
+

////
//For post preview, when more options are available.
. In the *Streams for Apache Kafka* page of the web console, click *Create Kafka instance* and define the following instance details. Some values currently have only one option.
* *Instance name*: Enter a unique name for the instance, such as `my-first-kafka-instance`.
* *Cloud provider*: Select `Amazon Web Services`.
* *Cloud region*: Select `US East, N. Virginia`.
* *Availability zones*: Select `Multi`.
////

+

. Click *Create instance* to start the creation process for your Kafka instance.
+
--
The new Kafka instance is listed on the *Kafka Instances* page. Typically, you need to wait a few minutes for the instance creation process to finish.

ifdef::qs[]
When the instance has a status of *Ready*, you can start using the instance. You can click the options icon (three vertical dots) to view instance and connection details, change the instance owner, or delete the instance.
endif::[]
ifndef::qs[]
When the instance has a status of *Ready*, you can start using the instance. As shown in the following figure, you can click the options icon (three vertical dots) to view instance and connection details, change the instance owner, or delete the instance.
[.screencapture]
.Kafka instance options menu
image::sak-kafka-instance-options.png[Image of Kafka instance options menu]
endif::[]

NOTE: Although you can see Kafka instances created by other users in your organization, you might not be able to manage or connect to those instances. Only the instance owner or users with permissions to access the instance can edit or delete the instance, access the associated service account and topics, or connect to the instance.
--

.Verification
ifdef::qs[]
* Is the new Kafka instance listed on the *Kafka Instances* page?
* Is the status of the new Kafka instance shown as *Ready*?
endif::[]
ifndef::qs[]
. Verify that the new Kafka instance is listed on the *Kafka Instances* page.
. Verify that the status of the new Kafka instance is shown as *Ready*.
endif::[]

[id="proc-creating-service-account_{context}"]
== Creating a service account to connect to a Kafka instance in {product-long-kafka}

[role="_abstract"]
To connect your applications or services to a Kafka instance in {product-long-kafka}, you need to create a service account that's associated with the instance. You then need to save the generated service account credentials, the authentication token endpoint, and the bootstrap server endpoint for the instance to a secure location. You'll use these details when you configure an application to connect to the Kafka instance.

.Prerequisites
* You have a running Kafka instance in {product-kafka}.

.Procedure
. On the *Kafka Instances* page of the {product-kafka} {service-url-kafka}[web console^], select the options icon (three vertical dots) for your Kafka instance and click *Connection*.
. On the *Connection* page, copy the *Bootstrap server* endpoint to a secure location. You'll specify this endpoint when configuring a connection to the Kafka instance.
+
ifdef::qs[]
The remainder of this task describes how to create a service account and copy the generated credentials.
If you want to use the credentials of an _existing_ service account, you can skip to the next task.
endif::[]
ifndef::qs[]
The remainder of this section describes how to create a service account and copy the generated credentials.
If you want to use the credentials of an _existing_ service account, you can skip to the next section.
endif::[]

. Open the {service-accounts-url}[Service Accounts^] page in the *Application Services* section of the Red Hat Hybrid Cloud Console.
. Click *Create service account* to set up the account that you'll use to access this Kafka instance.
. Enter a short description such as `my-service-account` and click *Create*.
. Copy the generated *Client ID* and *Client secret* values to a secure location. You'll specify these credentials when configuring a connection to the Kafka instance.
+
IMPORTANT: The generated credentials are displayed only once, so ensure that you've successfully and securely saved the copied credentials before closing the credentials window.

. After you save the generated credentials to a secure location, select the confirmation check box in the credentials window and close the window.
. Under *Authentication method*, copy the SASL/OAUTHBEARER *Token endpoint URL* value to a secure location. This is the endpoint that you'll use with your service account credentials to authenticate the connection to the Kafka instance.
+
NOTE: SASL/PLAIN authentication is also available for tools and libraries that don't support SASL/OAUTHBEARER, but SASL/OAUTHBEARER is recommended whenever possible. With SASL/PLAIN authentication, you use only service account credentials to authenticate the connection to the Kafka instance.

. (Optional) To review, reset, or delete the service account, use the {service-accounts-url}[Service Accounts^] page.

.Verification
ifdef::qs[]
* Did you save the bootstrap server, client credentials, and authentication token endpoint to a secure location?
* Did you verify that your service account was successfully created in the *Service Accounts* page?
endif::[]
ifndef::qs[]
. Verify that the bootstrap server, client credentials, and authentication token endpoint are saved to a secure location.
. Verify that your service account was successfully created on the *Service Accounts* page.
endif::[]

[id="proc-setting-service-account-permissions_{context}"]
== Setting permissions for a service account in a Kafka instance in {product-kafka}

[role="_abstract"]
After you create a service account to connect to a Kafka instance, you must also set the appropriate level of access for that new account in an Access Control List (ACL) for the Kafka instance. {product-long-kafka} uses ACLs provided by Kafka that enable you to manage how other user accounts and service accounts are permitted to interact with the Kafka resources that you create.

.Prerequisites
* You have a running Kafka instance in {product-kafka}.
* You've created a service account that you want to allow to access the running Kafka instance.

.Procedure
.  In the {product-long-kafka} {service-url-kafka}[web console^], select *Kafka Instances* and then click the name of the Kafka instance that you want the service account to access.
. Click the *Access* tab to view the current ACL for this instance.
. To modify the ACL, click *Manage access*.
. In the *Manage access* dialog box, use the *Account* list to select the service account that you previously created, and click *Next*.
. Under *Assign Permissions*, use the list to select the *Consume from a topic* and the *Produce to a topic* permission options, and set all resource identifiers to `Is` and all identifier values to `*`.
+
--
These settings result in the following ACL permissions for the new service account:

.Example ACL permissions for a new service account
[cols="25%,25%,25%,25%"]
|===
h|Resource type
h|Resource identifier and value
h|Access type
h|Operation

|`Topic`

(For consuming)
|`Is` = `*`
|`Allow`
|`Read`, `Describe`

|`Consumer group`

(For consuming)
|`Is` = `*`
|`Allow`
|`Read`

|`Topic`

(For producing)
|`Is` = `*`
|`Allow`
|`Write`, `Create`, `Describe`
|===

The permissions shown in the table enable applications associated with the service account to create topics in the Kafka instance, to produce and consume messages in any topic in the instance, and to use any consumer group.

NOTE: Alternatively, you can click *Add permission* to create individual permissions as needed. For example, you can create one `Topic` entry and one `Consumer group` entry, both with `Allow` access to `All` operations. This enables both consuming and producing for the specified topic in a single entry, and enables all permissions for the consumer group in another single entry. But you must configure these entries individually without using the predefined permission options.

--
. After you add these permissions for the service account, click *Save* to finish.

.Verification
ifdef::qs[]
* Are the new permissions for the service account listed on the *Access* page of the Kafka instance?
endif::[]
ifndef::qs[]
* Verify that the new permissions for the service account are listed on the *Access* page of the Kafka instance.

[role="_additional-resources"]
.Additional resources
* {base-url}{access-mgmt-url-kafka}[Managing account access in {product-long-kafka}^]
* link:https://kafka.apache.org/documentation/#security_authz[Authorization and ACLs^] in the Apache Kafka documentation
endif::[]

[id="proc-creating-kafka-topic_{context}"]
== Creating a Kafka topic in {product-kafka}

[role="_abstract"]
After you create a Kafka instance, you can create Kafka topics to start producing and consuming messages in your applications and services.

.Prerequisites
* You have a running Kafka instance in {product-long-kafka}.

.Procedure

. In the {product-long-kafka} {service-url-kafka}[web console^], select *Kafka Instances* and then click the name of the Kafka instance that you want to add a topic to.
. Select the *Topics* tab.
ifdef::qs[]
. Click *Create topic* and follow the guided steps to define the topic details.
endif::[]
ifndef::qs[]
. Click *Create topic* and follow the guided steps to define the topic details, as shown in the figure.
+
[.screencapture]
.Guided steps to define topic details
image::sak-create-topic.png[Image of wizard to create a topic]
endif::[]
+
--
You must specify the following topic properties:

* *Topic name*: Enter a unique topic name, such as `my-first-kafka-topic`.
* *Partitions*: Set the number of partitions for this topic. This example sets the partitions value to `1`. Partitions are distinct lists of messages within a topic and enable parts of a topic to be distributed over multiple brokers in the cluster. A topic can contain one or more partitions, enabling producer and consumer loads to be scaled.
* *Message retention*: Set the message retention time and size to the relevant value and increment. This example sets the retention time to `A week` and the retention size to `Unlimited`. Message retention time is the amount of time that messages are retained in a topic before they are deleted or compacted, depending on the cleanup policy. Retention size is the maximum total size of all log segments in a partition before they are deleted or compacted.
* *Replicas*: Replicas are copies of partitions in a topic. For this release of {product-kafka}, the replica values are preconfigured. For a standard Kafka instance, the number of partition replicas for the topic is set to `3` and the minimum number of follower replicas that must be in sync with a partition leader is set to `2`. For a trial Kafka instance, the number of replicas and the minimum in-sync replica factor are both set to `1`. Partition replicas are distributed over multiple brokers in the cluster to ensure topic availability if a broker fails. When a follower replica is in sync with a partition leader, the follower replica can become the new partition leader if needed.

After you complete the topic setup, the new topic is listed on the *Topics* page. You can now start producing and consuming messages to and from this topic using applications that you connect to the Kafka instance.

ifndef::community[]
NOTE: If you try to create a topic with a number of partitions that would cause the partition limit of the Kafka instance to be exceeded, you see an error message indicating this. For more information about partition limits for Kafka instances, see https://access.redhat.com/articles/5979061[{product-long-kafka} service limits].
endif::[]

NOTE: If the topic creation is unsuccessful and you see a `400 Bad Request` error message, try to create your topic again later. This situation might occur, for example, if your selected cloud provider has a temporary availability problem that affects your Kafka instance.
--

ifdef::qs[]
. (Optional) To edit or delete the topic, click the options icon (three vertical dots) next to the topic name.
endif::[]
ifndef::qs[]
. (Optional) To edit or delete the topic, click the options icon (three vertical dots) next to the topic name, as shown in the figure.
endif::[]

.Verification
ifdef::qs[]
* Is the new Kafka topic listed on the *Topics* page?
endif::[]
ifndef::qs[]
* Verify that the new Kafka topic is listed on the *Topics* page.
endif::[]

[role="_additional-resources"]
== Additional resources
* https://console.redhat.com/application-services/streams/overview[Purchase a subscription to {product-long-kafka}]
* https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/7d28aec8-e146-44db-a4a5-fafc1f426ca5[Configuring topics in {product-long-kafka}^]
* {base-url}{getting-started-rhoas-cli-url-kafka}[Getting started with the rhoas CLI for {product-long-kafka}^]
* {base-url-cli}{command-ref-url-cli}[CLI command reference (rhoas)^]
* {base-url}{kafkacat-url-kafka}[Configuring and connecting Kcat with {product-long-kafka}^]
* {base-url}{kafka-bin-scripts-url-kafka}[Configuring and connecting Kafka scripts with {product-long-kafka}^]
* {base-url}{quarkus-url-kafka}[Using Quarkus applications with Kafka instances in {product-long-kafka}^]

ifdef::qs[]
[#conclusion]
====
Congratulations! You successfully completed the {product-kafka} Getting Started quick start, and are now ready to use the service.

You can use either Kcat or the Kafka scripts to check that you can connect with your Kafka instance.
====
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
