////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//All OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:product-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/
:sso-token-url: https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
:cloud-console-url: https://console.redhat.com/
:service-accounts-url: https://console.redhat.com/application-services/service-accounts
:rh-sso-url: https://sso.redhat.com

//OpenShift
:openshift: OpenShift
:osd-name: OpenShift Dedicated
:osd-name-short: OpenShift Dedicated
:rosa-name: OpenShift Service for AWS
:rosa-name-short: OpenShift Service for AWS

//OpenShift Application Services CLI
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:command-ref-url-cli: commands
:installation-guide-url-cli: rhoas/rhoas-cli-installation/README.adoc
:service-contexts-url-cli: rhoas/rhoas-service-contexts/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:getting-started-rhoas-cli-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc
:message-browsing-url-kafka: kafka/message-browsing-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:getting-started-rhoas-cli-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:connectors: Connectors
:product-long-connectors: OpenShift Connectors
:product-connectors: Connectors
:product-version-connectors: 1
:service-url-connectors: https://console.redhat.com/application-services/connectors
:getting-started-url-connectors: connectors/getting-started-connectors/README.adoc
:getting-started-rhoas-cli-url-connectors: connectors/rhoas-cli-getting-started-connectors/README.adoc
:addon-url-connectors: https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01


//OpenShift API Designer
:product-long-api-designer: OpenShift API Designer
:product-api-designer: API Designer
:product-version-api-designer: 1
:service-url-api-designer: https://console.redhat.com/application-services/api-designer/
:getting-started-url-api-designer: api-designer/getting-started-api-designer/README.adoc

//OpenShift API Management
:product-long-api-management: OpenShift API Management
:product-api-management: API Management
:product-version-api-management: 1
:service-url-api-management: https://console.redhat.com/application-services/api-management/

////
END GENERATED ATTRIBUTES
////

[id="chap-using-nodejs"]
= Using Node.js applications with Kafka instances in {product-long-kafka}
ifdef::context[:parent-context: {context}]
:context: using-nodejs

// Purpose statement for the assembly
[role="_abstract"]
As a developer of applications and services, you can connect Node.js applications to Kafka instances in {product-long-kafka}. https://nodejs.org/en/about/[Node.js^] is a server-side JavaScript runtime that is designed to build scalable network applications. Node.js provides an I/O model that is based on events and non-blocking operations, which enables efficient applications.

In this quick start, you'll use the {product-kafka} web console to collect connection information for a Kafka instance. Then you'll manually configure a connection from an example Node.js application to the Kafka instance and start producing and consuming messages.

NOTE: When you've completed this quick start and understand the required connection configuration for a Kafka instance, you can use the {product-long-rhoas} command-line interface (CLI) to generate this type of configuration in a more automated way. To learn more, see {base-url}{service-contexts-url-cli}[Connecting client applications to {product-long-rhoas} using the rhoas CLI^].

.Prerequisites
ifndef::community[]
* You have a {org-name} account.
endif::[]
* You have a running Kafka instance in {product-kafka} (see {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^]).
* You have a command-line terminal application.
* https://github.com/git-guides/[Git^] is installed.
* You have an IDE such as https://www.jetbrains.com/idea/download/[IntelliJ IDEA^], https://www.eclipse.org/downloads/[Eclipse^], or https://code.visualstudio.com/Download[Visual Studio Code^].
* https://nodejs.org/en/download/[Node.js 14^] or later is installed.

NOTE: The example Node.js application in this quick start uses the https://kafka.js.org/[KafkaJS^] client by default. If you want to use the https://github.com/blizzard/node-rdkafka[node-rdkafka^] client, you must install some development tools locally on your computer or use a container runtime such as Podman or Docker to run a specified container image and configure a development environment. To learn more, see the https://github.com/nodeshift-starters/reactive-example/tree/node-rdkafka#node-rdkafka-and-kafkajs[documentation] for the example Node.js application.


// Condition out QS-only content so that it doesn't appear in docs.
// All QS anchor IDs must be in this alternate anchor ID format `[#anchor-id]` because the ascii splitter relies on the other format `[id="anchor-id"]` to generate module files.
ifdef::qs[]
[#description]
====
Manually connect a Node.js application to a Kafka instance and then produce and consume messages.
====

[#introduction]
====
Welcome to the quick start for {product-long-kafka} with Node.js. In this quick start, you'll use the web console to collect connection information for a Kafka instance in {product-kafka}. Then you'll manually configure a connection from an example https://nodejs.org/en/about/[Node.js^] application to the Kafka instance and start producing and consuming messages.
====
endif::[]


[id="proc-importing-nodejs-sample-code_{context}"]
== Importing the Node.js sample code

[role="_abstract"]
For this quick start, you'll use sample code from the Nodeshift Application Starters https://github.com/nodeshift-starters/reactive-example[reactive-example^] repository in GitHub. After you understand the concepts and tasks in this quick start, you can use your own Node.js applications with {product-long-kafka} in the same way.

.Procedure
. On the command line, clone the Nodeshift Application Starters https://github.com/nodeshift-starters/reactive-example[reactive-example^] repository from GitHub.
+
[source,subs="+attributes"]
----
$ git clone https://github.com/nodeshift-starters/reactive-example.git
----
. In your IDE, open the `reactive-example` directory of the repository that you cloned.

ifdef::qs[]
.Verification
* Is the Node.js example application accessible in your IDE?
endif::[]

[id="proc-configuring-nodejs_{context}"]
== Configuring the Node.js example application to connect to a Kafka instance

[role="_abstract"]
To enable your Node.js application to access a Kafka instance, you must configure a connection by specifying the following details:

* The bootstrap server endpoint for your Kafka instance
* The generated credentials for your {product-long-kafka} service account
* The Simple Authentication and Security Layer (SASL) mechanism that the client will use to authenticate with the Kafka instance

In this task, you'll create a new configuration file called `rhoas.env`. In the file, you'll set the required bootstrap server and client credentials as environment variables.

.Prerequisites
ifndef::qs[]
* You have the bootstrap server endpoint for your Kafka instance. To get the server endpoint, select your Kafka instance in the {product-kafka} {service-url-kafka}[web console^], select the options icon (three vertical dots), and click *Connection*.
* You have the generated credentials for your service account. To reset the credentials, use the {service-accounts-url}[Service Accounts^] page in the *Application Services* section of the Red Hat Hybrid Cloud Console.
endif::[]

.Procedure

. In your IDE, create a new file. Save the file with the name `rhoas.env`, at the root level of the `reactive-example` directory for the cloned repository.

. In the `rhoas.env` file, set the SASL authentication mechanism and the Kafka instance client credentials as shown in the following configuration. Replace the client ID and client secret values with your own credential information. The configuration uses SASL/OAUTHBEARER authentication, which is the recommended authentication mechanism to use in {product-kafka}.
+
.Setting environment variables in the rhoas.env file
[source,subs="+attributes,+quotes"]
----
KAFKA_HOST=__<bootstrap_server>__
RHOAS_SERVICE_ACCOUNT_CLIENT_ID=__<client_id>__
RHOAS_SERVICE_ACCOUNT_CLIENT_SECRET=__<client_secret>__
KAFKA_SASL_MECHANISM=oauthbearer
RHOAS_TOKEN_ENDPOINT_URL={sso-token-url}
----
ifdef::qs[]
+
The values are described as follows:
+
--
* *bootstrap_server*: The bootstrap server endpoint for your Kafka instance. To access this information for a Kafka instance in {product-kafka}, select the options menu (three vertical dots). Click *Connection*.
* *client_id*: A client credential generated when you create a service account in {product-kafka}. You're prompted to copy and store this credential when you create the service account.
* *client_secret*: A client credential generated when you create a service account in {product-kafka}. You're prompted to copy and store this credential when you create the service account.
--
endif::[]

. Save the `rhoas.env` file.

ifdef::qs[]
.Verification
* Did you set environment variables for the Kafka instance?
endif::[]

[id="proc-creating-countries-topic_{context}"]
== Creating a Kafka topic in {product-kafka}

[role="_abstract"]
The Node.js application in this quick start uses a Kafka topic called `countries` to produce and consume messages. In this task, you'll create the `countries` topic in your Kafka instance.

.Prerequisites
* You have a running Kafka instance in {product-long-kafka}.

.Procedure
. In the {product-kafka} {service-url-kafka}[web console^], select *Kafka Instances* and then click the name of the Kafka instance that you want to add a topic to.
. Select the *Topics* tab.
. Click *Create topic* and follow the guided steps to define the topic details.
+
--
You must specify the following topic properties:

* *Topic name*: For this quick start, enter `countries` as the topic name.
* *Partitions*: Set the number of partitions for the topic. For this quick start, set the value to `1`.
* *Message retention*: Set the message retention time and size. For this quick start, set the retention time to `A week` and the retention size to `Unlimited`.
* *Replicas*: For this release of {product-kafka}, the replica values are preconfigured. The number of partition replicas for the topic is set to `3` and the minimum number of follower replicas that must be in sync with a partition leader is set to `2`. For a trial Kafka instance, the number of replicas and the minimum in-sync replica factor are both set to `1`.

After you complete the setup, the new topic appears on the *Topics* page. You can now run the Node.js application to start producing and consuming messages.
--

.Verification
ifdef::qs[]
* Is the `countries` topic listed on the *Topics* page?
endif::[]
ifndef::qs[]
* Verify that the `countries` topic is listed on the *Topics* page.
endif::[]

[id="proc-running-nodejs-example-application_{context}"]
== Running the Node.js example application

[role="_abstract"]
After you configure your Node.js application to connect to a Kafka instance, and you create the required Kafka topic, you're ready to run the application.

In this task, you'll run the following components of the Node.js application:

* A `producer-backend` component that generates random country names and sends these names to the Kafka topic
* A `consumer-backend` component that consumes the country names from the Kafka topic

.Prerequisites
* You've configured the Node.js example application to connect to a Kafka instance.
* You've created the `countries` topic.
* You've set permissions for your service account to produce and consume messages in the `countries` topic. For the Node.js application in this example, the consumer group you must specify in your permissions is called `consumer-test`. To learn how to configure access permissions for a Kafka instance, see {base-url}{access-mgmt-url-kafka}[Managing account access in {product-long-kafka}^].

.Procedure
. On the command line, navigate to the `reactive-example` directory of the repository that you cloned.
+
[source]
----
$ cd reactive-example
----

. Navigate to the directory for the consumer component. Use Node Package Manager (npm) to install the dependencies for this component.
+
.Installing dependencies for the consumer component
[source]
----
$ cd consumer-backend
$ npm install
----

. Run the consumer component.
+
[source]
----
$ node consumer.js
----
+
You see the Node.js application run and connect to the Kafka instance. However, because you haven't yet run the producer component, the consumer has no country names to display.
+
If the application fails to run, review the error log in the command-line window and address any problems. Also, review the steps in this quick start to ensure that the application and Kafka topic are configured correctly.

. Open a second command-line window or tab.

. On the second command line, navigate to the `reactive-example` directory of the repository that you cloned.
+
[source]
----
$ cd reactive-example
----

. Navigate to the directory for the producer component. Use Node Package Manager to install the dependencies for this component.
+
.Installing dependencies for the producer component
[source]
----
$ cd producer-backend
$ npm install
----

. Run the producer component.
+
[source]
----
$ node producer.js
----
+
When the producer component runs, you see output like that shown in the following example:
+
.Example output from the producer component
[source]
----
Ghana
Réunion
Guatemala
Luxembourg
Mayotte
Syria
United Kingdom
Bolivia
Haiti
----
+
As shown in the example, the producer component runs and generates messages that represent country names.

. Switch back to the first command-line window.
+
You now see that the consumer component displays the same country names generated by the producer, and in the same order, as shown in the following example:
+
.Example output from the consumer component
[source]
----
Ghana
Réunion
Guatemala
Luxembourg
Mayotte
Syria
United Kingdom
Bolivia
Haiti
----
+
The output from both components confirms that they successfully connected to the Kafka instance. The components are using the Kafka topic that you created to produce and consume messages.
+
NOTE: You can also use the {product-long-kafka} web console to browse messages in the Kafka topic. For more information, see {base-url}{message-browsing-url-kafka}[Browsing messages in the {product-long-kafka} web console^].

. In your IDE, in the `producer-backend` directory of the repository that you cloned, open the `producer.js` file.
+
Observe that the producer component is configured to process environment variables from the `rhoas.env` file that you created. The component used the bootstrap server endpoint and client credentials stored in this file to connect to the Kafka instance.

. In the `consumer-backend` directory, open the `consumer.js` file.
+
Observe that the consumer component is also configured to process environment variables from the `rhoas.env` file that you created.

ifdef::qs[]
.Verification
* Did the producer component run and start generating random country names?
* Did the consumer component run and display the same country names generated by the producer, and in the same order?
endif::[]

ifdef::qs[]
[#conclusion]
====
Congratulations! You successfully completed the {product-kafka} Node.js quick start. You're now ready to use your own Node.js applications with {product-kafka}.
====
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
