////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//All OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:product-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/
:sso-token-url: https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
:cloud-console-url: https://console.redhat.com/
:service-accounts-url: https://console.redhat.com/application-services/service-accounts
:rh-sso-url: https://sso.redhat.com

//OpenShift
:openshift: OpenShift
:osd-name: OpenShift Dedicated
:osd-name-short: OpenShift Dedicated
:rosa-name: OpenShift Service for AWS
:rosa-name-short: OpenShift Service for AWS

//OpenShift Application Services CLI
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:command-ref-url-cli: commands
:installation-guide-url-cli: rhoas/rhoas-cli-installation/README.adoc
:service-contexts-url-cli: rhoas/rhoas-service-contexts/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:getting-started-rhoas-cli-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc
:message-browsing-url-kafka: kafka/message-browsing-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:getting-started-rhoas-cli-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:connectors: Connectors
:product-long-connectors: OpenShift Connectors
:product-connectors: Connectors
:product-version-connectors: 1
:service-url-connectors: https://console.redhat.com/application-services/connectors
:getting-started-url-connectors: connectors/getting-started-connectors/README.adoc
:getting-started-rhoas-cli-url-connectors: connectors/rhoas-cli-getting-started-connectors/README.adoc

//OpenShift API Designer
:product-long-api-designer: OpenShift API Designer
:product-api-designer: API Designer
:product-version-api-designer: 1
:service-url-api-designer: https://console.redhat.com/application-services/api-designer/
:getting-started-url-api-designer: api-designer/getting-started-api-designer/README.adoc

//OpenShift API Management
:product-long-api-management: OpenShift API Management
:product-api-management: API Management
:product-version-api-management: 1
:service-url-api-management: https://console.redhat.com/application-services/api-management/

////
END GENERATED ATTRIBUTES
////

[id="chap-monitoring-metrics"]
= Monitoring metrics in {product-long-kafka}
ifdef::context[:parent-context: {context}]
:context: monitoring-metrics

// Purpose statement for the assembly
[role="_abstract"]
As a developer or administrator, you can view metrics in {product-long-kafka} to visualize the performance and data usage for Kafka instances and topics that you have access to. You can view metrics directly in the {product-kafka} web console, or use the metrics API endpoint provided by {product-kafka} to import the data into your own metrics monitoring tool, such as Prometheus.

//Additional line break to resolve mod docs generation error, not sure why. Leaving for now. (Stetson, 20 May 2021)

[id="ref-supported-metrics_{context}"]
== Supported metrics in {product-kafka}

[role="_abstract"]
{product-long-kafka} supports the following metrics for Kafka instances and topics. In the {product-kafka} web console, the *Dashboard* page of a Kafka instance displays a subset of these metrics. To learn more about the limits associated with both trial and production Kafka instance types, see https://access.redhat.com/articles/5979061[Red Hat OpenShift Streams for Apache Kafka Service Limits].


Cluster metrics::
+
--
`kafka_namespace:haproxy_server_bytes_in_total:rate5m`:: Number of incoming bytes per second for the cluster in the last five minutes. This ingress metric represents all the data that producers are sending to topics in the cluster.
+
The Kafka instance type determines the maximum incoming byte rate.

`kafka_namespace:haproxy_server_bytes_out_total:rate5m`:: Number of outgoing bytes per second for the cluster in the last five minutes. This egress metric represents all the data that consumers are receiving from topics in the cluster.
+
The Kafka instance type determines the maximum outgoing byte rate.

`kafka_namespace:kafka_server_socket_server_metrics_connection_count:sum`:: Number of current client connections to the cluster. Kafka clients use persistent connections to interact with brokers in the cluster. For example, a consumer holds a connection to each broker it is receiving data from and a connection to its group coordinator.
+
The Kafka instance type determines the maximum number of active connections.

`kafka_namespace:kafka_server_socket_server_metrics_connection_creation_rate:sum`:: Number of client connection creations per second for the cluster. Kafka clients use persistent connections to interact with brokers in the cluster. A constant high number of connection creations might indicate a client issue.
+
The Kafka instance type determines the maximum connection creation rate.

`kafka_topic:kafka_topic_partitions:count`:: Number of topics in the cluster. This metric does not include internal Kafka topics, such as `\__consumer_offsets` and `__transaction_state`.

`kafka_topic:kafka_topic_partitions:sum`:: Number of partitions across all topics in the cluster. This metric does not include partitions from internal Kafka topics, such as `\__consumer_offsets` and `__transaction_state`.
+
The Kafka instance type determines the maximum number of partitions.

`kas_broker_partition_log_size_bytes_top50`:: Sizes, in bytes, of the fifty largest topic partitions on each broker in the cluster. The total amount of storage being used by all topic partitions on a broker is shown by the `kafka_broker_quota_totalstorageusedbytes` broker metric. The total usage for a broker must stay below the `kafka_broker_quota_softlimitbytes` value to avoid throttling of producers. 

`kas_topic_partition_log_size_bytes`:: Size, in bytes, of each topic partition on each broker in the cluster. The total amount of storage being used by all topic partitions on a broker is shown by the `kafka_broker_quota_totalstorageusedbytes` broker metric. The total usage for a broker must stay below the `kafka_broker_quota_softlimitbytes` value to avoid throttling of producers.
--

Broker metrics::
+
--
`kafka_broker_quota_softlimitbytes`:: Maximum amount of storage, in bytes, for this broker before producers are throttled. When this limit is reached, the broker starts throttling producers to prevent them from sending additional data.
+
The Kafka instance type determines the maximum storage in the broker.

`kafka_broker_quota_totalstorageusedbytes`:: Amount of storage, in bytes, that is currently used by partitions in the broker. The storage usage depends on the number and retention configurations of the partitions. This metric must stay below the `kafka_broker_quota_softlimitbytes` value.

`kafka_controller_kafkacontroller_global_partition_count`:: Number of partitions in the cluster. Only the broker that is the current controller in the cluster reports this metric. Any other brokers report a value of `0`. This count includes partitions from internal Kafka topics, such as `\__consumer_offsets` and `__transaction_state`. This metric is similar to the `kafka_topic:kafka_topic_partitions:sum` cluster metric.

`kafka_controller_kafkacontroller_offline_partitions_count`:: Number of partitions in the cluster that are currently offline. Offline partitions cannot be used by clients for producing or consuming data. Only the broker that is the current controller in the cluster reports this metric. Any other brokers report `0`.

`kubelet_volume_stats_available_bytes`:: Amount of disk space, in bytes, that is available in the broker.

`kubelet_volume_stats_used_bytes`:: Amount of disk space, in bytes, that is currently used in the broker. This metric is similar to the `kafka_broker_quota_totalstorageusedbytes` broker metric.
--

Topic metrics::
+
--
`kafka_server_brokertopicmetrics_bytes_in_total`:: Number of incoming bytes to topics in the instance.

`kafka_server_brokertopicmetrics_bytes_out_total`:: Number of outgoing bytes from topics in the instance.

`kafka_server_brokertopicmetrics_messages_in_total`:: Number of messages per second received by one or more topics in the instance.

`kafka_topic:kafka_server_brokertopicmetrics_bytes_in_total:rate5m`:: Number of incoming bytes to topics in the instance in the last five minutes.

`kafka_topic:kafka_server_brokertopicmetrics_bytes_out_total:rate5m`:: Number of outgoing bytes from topics in the instance in the last five minutes.

`kafka_topic:kafka_server_brokertopicmetrics_messages_in_total:rate5m`:: Number of messages per second received by one or more topics in the instance in the last five minutes.

`kafka_topic:kafka_log_log_size:sum`:: Log size, in bytes, of each topic and replica across all brokers in the cluster.
--

[id="proc-viewing-metrics_{context}"]
== Viewing metrics for a Kafka instance in {product-kafka}

[role="_abstract"]
After you produce and consume messages in your services using methods such as link:https://kafka.apache.org/downloads[Kafka^] scripts, link:https://github.com/edenhill/kcat[Kcat^], or a link:https://quarkus.io/[Quarkus^] application, you can return to the Kafka instance in the web console and use the *Dashboard* page to view metrics for the instance and topics. The metrics help you understand the performance and data usage for your Kafka instance and topics.

.Prerequisites
* You have access to a running Kafka instance in {product-kafka} that contains topics. For more information about access management in {product-kafka}, see {base-url}{access-mgmt-url-kafka}[Managing account access in {product-long-kafka}^].

.Procedure
* In the *Kafka Instances* page of the web console, click the name of the Kafka instance and select the *Dashboard* tab.
+
When you create a Kafka instance and add new topics, the *Dashboard* page is initially empty. After you start producing and consuming messages in your services, you can return to this page to view related metrics. For example, to use Kafka scripts to produce and consume messages, see {base-url}{kafka-bin-scripts-url-kafka}[Configuring and connecting Kafka scripts with {product-long-kafka}^].

NOTE: In some cases, after you start producing and consuming messages, you might need to wait several minutes for the latest metrics to appear. You might also need to wait until your instance and topics contain enough data for metrics to appear.



[id="proc-configuring-metrics-prometheus_{context}"]
== Configuring metrics monitoring for a Kafka instance in Prometheus

[role="_abstract"]
As an alternative to viewing metrics for a Kafka instance in the {product-long-kafka} web console, you can export your metrics to https://prometheus.io/docs/introduction/overview/[Prometheus^] and integrate the metrics with your own metrics monitoring platform. {product-kafka} provides a `kafkas/{id}/metrics/federate` API endpoint that you can configure as a scrape target for Prometheus to use to collect and store metrics. You can then access the metrics in the https://prometheus.io/docs/visualization/browser/[Prometheus expression browser^] or in a data-graphing tool such as https://prometheus.io/docs/visualization/grafana/[Grafana^].

This procedure follows the https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file[Configuration File^] method defined by Prometheus for integrating third-party metrics. If you use the Prometheus Operator in your monitoring environment, you can also follow the https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/additional-scrape-config.md#additional-scrape-configuration[Additional Scrape Configuration^] method.

.Prerequisites
* You have access to a running Kafka instance that contains topics in {product-kafka}. For more information about access management in {product-kafka}, see {base-url}{access-mgmt-url-kafka}[Managing account access in {product-long-kafka}^].
* You have the ID and the SASL/OAUTHBEARER token endpoint for the Kafka instance. To relocate the Kafka instance ID and the token endpoint, select your Kafka instance in the {product-kafka} web console, select the options menu (three vertical dots), and click *Connection*.
* You have the generated credentials for your service account that has access to the Kafka instance. To reset the credentials, use the {service-accounts-url}[Service Accounts^] page in the *Application Services* section of the Red Hat Hybrid Cloud Console.
* You've installed a Prometheus instance in your monitoring environment. For installation instructions, see https://prometheus.io/docs/prometheus/latest/getting_started/[Getting Started^] in the Prometheus documentation.

.Procedure
. In your Prometheus configuration file, add the following information. Replace the variable values with your own Kafka instance and service account information.
+
--
The `<kafka_instance_id>` is the ID of the Kafka instance. The `<client_id>` and `<client_secret>` are the generated credentials for your service account that you copied previously. The `<token_url>` is the SASL/OAUTHBEARER token endpoint for the Kafka instance.

.Required information for Prometheus configuration file
[source,yaml,subs="+quotes"]
----
- job_name: "kafka-federate"
  static_configs:
  - targets: ["api.openshift.com"]
  scheme: "https"
  metrics_path: "/api/kafkas_mgmt/v1/kafkas/__<kafka_instance_id>__/metrics/federate"
  oauth2:
    client_id: "__<client_id>__"
    client_secret: "__<client_secret>__"
    token_url: "__<token_url>__"
----

The new scrape target becomes available after the configuration has reloaded.
--
. View your collected metrics in the Prometheus expression browser at `http://__<host>__:__<port>__/graph`, or integrate your Prometheus data source with a data-graphing tool such as Grafana. For information about Prometheus metrics in Grafana, see https://prometheus.io/docs/visualization/grafana/[Grafana Support for Prometheus^] in the Grafana documentation.
+
--
If you use Grafana with your Prometheus instance, you can import the predefined https://grafana.com/grafana/dashboards/15835[{product-long-kafka} Grafana dashboard^] to set up your metrics display. For import instructions, see https://grafana.com/docs/grafana/v7.5/dashboards/export-import/#importing-a-dashboard[Importing a dashboard^] in the Grafana documentation.
--

When you create a Kafka instance and add new topics, the metrics are initially empty. After you start producing and consuming messages in your services, you can return to your monitoring tool to view related metrics. For example, to use Kafka scripts to produce and consume messages, see {base-url}{kafka-bin-scripts-url-kafka}[Configuring and connecting Kafka scripts with {product-long-kafka}^].

NOTE: In some cases, after you start producing and consuming messages, you might need to wait several minutes for the latest metrics to appear. You might also need to wait until your instance and topics contain enough data for metrics to appear.

[NOTE]
====
If you use the Prometheus Operator in your monitoring environment, you can alternatively create a `kafka-federate.yaml` file as an additional scrape configuration in your Prometheus custom resource as shown in the following example commands. For more information about this method, see https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/additional-scrape-config.md#additional-scrape-configuration[Additional Scrape Configuration^] in the Prometheus documentation.

.Example `kafka-federate.yaml` file
[source,yaml,subs="+quotes"]
----
- job_name: "kafka-federate"
  static_configs:
  - targets: ["api.openshift.com"]
  scheme: "https"
  metrics_path: "/api/kafkas_mgmt/v1/kafkas/__<kafka_instance_id>__/metrics/federate"
  oauth2:
    client_id: "__<client_id>__"
    client_secret: "__<client_secret>__"
    token_url: "__<token_url>__"
----

.Example command to create and apply a Kubernetes secret
[source,subs="+quotes"]
----
kubectl create secret generic additional-scrape-configs --from-file=__<~/kafka-federate.yaml>__ --dry-run -o yaml \
kubectl apply -f - -n __<namespace>__
----

.Example Prometheus custom resource with new secret
[source,subs="+quotes"]
----
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
    ...
spec:
    ...
    additionalScrapeConfigs:
        name: additional-scrape-configs
        key: kafka-federate.yaml
----

====

[id="proc-configuring-prometheus-alerts_{context}"]
== Configuring Prometheus alerts for Kafka instance limits

.Prerequisites
* You have successfully configured metrics monitoring for a Kafka instance in Prometheus.
* You use the Prometheus Operator in your monitoring environment.
* You can define https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/[alerting rules^] in Prometheus and can deploy an https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/alerting.md/[Alertmanager cluster^] in Prometheus Operator.


.Procedure
. Create a `PrometheusRule` custom resource with alerts defined for the https://access.redhat.com/articles/5979061[capacity of your Kafka instance].
. Apply the `PrometheusRule` to the cluster that you are federating the metrics to.

.Example `PrometheusRule` custom resource for a Kafka broker storage limit alert
[source,subs="+quotes"]
----
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
spec:
  groups:
    - name: limits
      rules:
        - alert: KafkaBrokerStorageFillingUp
          expr: predict_linear(kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-(.+)-kafka-[0-9]+"}[1h], 4 * 3600)
          labels:
            severity: <SOME_SEVERITY>
          annotations:
            summary: 'Broker PersistentVolume is filling up.'
            description: 'Based on recent sampling, the Broker PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is expected to fill up within four days.
----




[role="_additional-resources"]
.Additional resources
* {base-url}{getting-started-url-kafka}[Getting started with {product-long-kafka}^]
* {base-url}{getting-started-rhoas-cli-url-kafka}[Getting started with the `rhoas` CLI for {product-long-kafka}^]
* {base-url-cli}{command-ref-url-cli}[CLI command reference (rhoas)^]
* https://prometheus.io/docs/prometheus/latest/getting_started/[Getting Started^] in the Prometheus documentation
* https://prometheus.io/docs/visualization/grafana/[Grafana Support for Prometheus^]
* https://grafana.com/docs/grafana/latest/datasources/prometheus/[Prometheus Data Source^] in the Grafana documentation

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
