////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////


:community:
:imagesdir: ./images
:product-version: 1
:product-long: Application Services
:product: App Services
// Placeholder URL, when we get a HOST UI for the service we can put it here properly
:service-url: https://console.redhat.com/beta/application-services/streams/
:property-file-name: app-services.properties
:rhoas-version: 0.27.0

// Other upstream project names
:samples-git-repo: https://github.com/redhat-developer/app-services-guides

//URL components for cross refs
:base-url: https://github.com/redhat-developer/app-services-guides/blob/main/
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:getting-started-url: getting-started/README.adoc
:kafka-bin-scripts-url: kafka-bin-scripts/README.adoc
:kafkacat-url: kafkacat/README.adoc
:quarkus-url: quarkus/README.adoc
:rhoas-cli-url: rhoas-cli/README.adoc
:rhoas-cli-ref-url: commands
:topic-config-url: topic-configuration/README.adoc
:consumer-config-url: consumer-configuration/README.adoc
:service-binding-url: service-discovery/README.adoc

////
END GENERATED ATTRIBUTES
////

[id="chap-using-nodejs"]
= Using Node.js applications with Kafka instances in {product-long}
ifdef::context[:parent-context: {context}]
:context: using-nodejs

[IMPORTANT]
====
{product-long} is currently available for Development Preview. Development Preview releases provide early access to a limited set of features that might not be fully tested and that might change in the final GA version. Users should not use Development Preview software in production or for business-critical workloads. Limited documentation is available for Development Preview releases and is typically focused on fundamental user goals.
====

// Purpose statement for the assembly
[role="_abstract"]
As a developer of applications and services, you can connect Node.js applications to Kafka instances in {product-long}. https://nodejs.org/en/about/[Node.js^] is a server-side JavaScript runtime that is designed to build scalable network applications. Node.js provides an I/O model that is based on events and non-blocking operations, which enables efficient applications.

.Prerequisites
ifndef::community[]
* You have a Red Hat account.
endif::[]
* You have a Kafka instance in {product} and the instance is in the *Ready* state. To learn how to create a Kafka instance, see link:{base-url}{getting-started-url}[Getting started with {product-long}].
* https://github.com/git-guides/[Git^] is installed.
* You have an IDE such as https://www.jetbrains.com/idea/download/[IntelliJ IDEA^], https://www.eclipse.org/downloads/[Eclipse^], or https://code.visualstudio.com/Download[VSCode^].
* https://nodejs.org/en/download/[Node.js 14^] is installed. The https://github.com/blizzard/node-rdkafka[node-rdkafka^] client can't run on later versions.

[NOTE]
====
The example Node.js application in this quick start uses the https://kafka.js.org/[KafkaJS^] client by default. If you want to use the https://github.com/blizzard/node-rdkafka[node-rdkafka^] client, you must install some development tools locally on your computer, or use Docker to run a specified container image and configure a development environment. To learn more, see the https://github.com/nodeshift-starters/reactive-example/tree/node-rdkafka#node-rdkafka-and-kafkajs[documentation] for the example Node.js application.
====

// Condition out QS-only content so that it doesn't appear in docs.
// All QS anchor IDs must be in this alternate anchor ID format `[#anchor-id]` because the ascii splitter relies on the other format `[id="anchor-id"]` to generate module files.
ifdef::qs[]
[#description]
Learn how to use Node.js applications to produce and consume messages using a Kafka instance in {product-long}.

[#introduction]
Welcome to the quick start for {product-long} with Node.js. In this quick start, you'll learn how to use the https://nodejs.org/en/about/[Node.js^] runtime to produce messages to and consume messages from your Kafka instances in {product}.
endif::[]


[id="proc-importing-nodejs-sample-code_{context}"]
== Importing the Node.js sample code

For this quick start, you'll use sample code from the Nodeshift Application Starters https://github.com/nodeshift-starters/reactive-example[reactive-example^] repository in GitHub. After you understand the concepts and tasks in this quick start, you can use your own Node.js applications with {product} in the same way.

.Procedure
. On the command line, clone the Nodeshift Application Starters https://github.com/nodeshift-starters/reactive-example[reactive-example^] repository from GitHub.
+
.Cloning the reactive-example repository
[source,subs="+attributes"]
----
$ git clone https://github.com/nodeshift-starters/reactive-example.git
----
. In your IDE, open the `reactive-example` directory of the repository that you cloned.

ifdef::qs[]
.Verification
* Is the Node.js example application accessible in your IDE?
endif::[]

[id="proc-configuring-nodejs_{context}"]
== Configuring the Node.js example application to connect to a Kafka instance

To enable your Node.js application to access a Kafka instance, you must configure a connection by specifying the following details:

* The bootstrap server endpoint for your Kafka instance
* The generated credentials for your {product} service account
* The Simple Authentication and Security Layer (SASL) mechanism that the client will use to authenticate with the Kafka instance

In this task, you'll create a new configuration file called `.env`. In this file, you'll set the required bootstrap server and client credentials as environment variables.

.Prerequisites
ifndef::qs[]
* You have the bootstrap server endpoint and the generated credentials for your service account. You copied this information previously for the Kafka instance in {product} by selecting the options menu (three vertical dots) and clicking *Connection*.
endif::[]

.Procedure

. In your IDE, create a new file. Save the file with the name `.env`, at the root level of the `reactive-example` directory for the cloned repository.

. In the `.env` file, add the lines shown in the example. These lines set the bootstrap server and client credentials as environment variables to be used by the Node.js application.
+
.Setting environment variables in the .env file
[source,subs="+quotes"]
----
KAFKA_BOOTSTRAP_SERVER=__<bootstrap_server>__
KAFKA_CLIENT_ID=__<client_id>__
KAFKA_CLIENT_SECRET=__<client_secret>__
KAFKA_SASL_MECHANISM=plain
----
+
In the preceding example, replace the values in angle brackets (`< >`) with your own bootstrap server and client credential information.
ifdef::qs[]
+
The values are described as follows:
+
--
* *bootstrap_server*: The bootstrap server endpoint for your Kafka instance. To access this information for a Kafka instance in {product}, select the options menu (three vertical dots). Click *Connection*.
* *client_id*: A client credential generated when you create a service account in {product}. You're prompted to copy and store this credential when you create the service account.
* *client_secret*: A client credential generated when you create a service account in {product}. You're prompted to copy and store this credential when you create the service account.
--
endif::[]
+
In this case, observe that the Node.js application uses the `SASL/PLAIN` authentication method (that is, the value of `KAFKA_SASL_MECHANISM` is set to `plain`). This means that the application uses only the client ID and client secret to authenticate with the Kafka instance. The application doesn't require an authentication token.

. Save the `.env` file.

ifdef::qs[]
.Verification
* Did you set environment variables for the Kafka instance?
endif::[]

[id="proc-creating-countries-topic_{context}"]
== Creating a Kafka topic in {product}

The Node.js application in this quick start uses a Kafka topic called `countries` to produce and consume messages. In this task, you'll create the topic in your Kafka instance.

.Prerequisites
* You've created a Kafka instance in {product} and the instance is in the *Ready* state.

.Procedure
. In the {product} web console, go to *Streams for Apache Kafka* > *Kafka Instances* and click the name of the Kafka instance that you want to add a topic to.
. Click *Create topic* and follow the guided steps to define the topic details. Click *Next* to complete each step and click *Finish* to complete the setup.
+
[.screencapture]
.Guided steps to define topic details
image::sak-create-countries-topic.png[Image of wizard to create a topic]

* *Topic name*: Enter `countries` as the topic name.
* *Partitions*: Set the number of partitions for this topic. This example sets the partition to `1` for a single partition. Partitions are distinct lists of messages within a topic and enable parts of a topic to be distributed over multiple brokers in the cluster. A topic can contain one or more partitions, enabling producer and consumer loads to be scaled.
* *Message retention*: Set the message retention time and size to the relevant value and increment. This example sets the retention time to `7 days` and the retention size to `Unlimited`. Message retention time is the amount of time that messages are retained in a topic before they are deleted or compacted, depending on the cleanup policy. Retention size is the maximum total size of all log segments in a partition before they are deleted or compacted.
* *Replicas*: For this release of {product}, the replicas are preconfigured. The number of partition replicas for the topic is set to `3` and the minimum number of follower replicas that must be in sync with a partition leader is set to `2`. Replicas are copies of partitions in a topic. Partition replicas are distributed over multiple brokers in the cluster to ensure topic availability if a broker fails. When a follower replica is in sync with a partition leader, the follower replica can become the new partition leader if needed.
+
After you complete the topic setup, the new Kafka topic is listed in the topics table for your Kafka instance. You can now run the Node.js application to start producing and consuming messages.

.Verification
ifdef::qs[]
* Is the `countries` topic listed in the topics table?
endif::[]
ifndef::qs[]
* Verify that the `countries` topic is listed in the topics table.
endif::[]

[id="proc-running-nodejs-example-application_{context}"]
== Running the Node.js example application

After you configure your Node.js application to connect to a Kafka instance, and you create the required Kafka topic, you're ready to run the application.

In this task, you'll run the following components of the Node.js application:

* A `producer-backend` component that generates random country names and sends these names to the Kafka topic.
* A `consumer-backend` component that consumes the country names from the Kafka topic.

.Prerequisites
* You've configured the Node.js example application to connect to a Kafka instance.
* You've created the `countries` Kafka topic.

.Procedure
. On the command line, navigate to the `reactive-example` directory of the repository that you cloned.
+
.Navigating to the reactive-example directory
[source]
----
$ cd reactive-example
----

. Navigate to the directory for the consumer component. Use Node Package Manager (npm) to install the dependencies for this component.
+
.Installing dependencies for the consumer component
[source]
----
$ cd consumer-backend
$ npm install
----

. Run the consumer component.
+
.Running the consumer component
[source]
----
$ node consumer.js
----
+
You should see the Node.js application start to run and connect to the Kafka instance. However, because you haven't yet run the producer component, the consumer has no country names to display.
+
If the application fails to run, review the error log in the command-line window and address any problems. Also, review the steps in this quick start to ensure that the application and Kafka topic are configured correctly.

. Open a second command-line window or tab.

. On the second command line, navigate to the `reactive-example` directory of the repository that you cloned.
+
.Navigating to the reactive-example directory
[source]
----
$ cd reactive-example
----

. Navigate to the directory for the producer component. Use Node Package Manager to install the dependencies for this component.
+
.Installing dependencies for the producer component
[source]
----
$ cd producer-backend
$ npm install
----

. Run the producer component.
+
.Running the producer component
[source]
----
$ node producer.js
----
+
You should see output like that shown in the example.
+
.Example output from the producer component
[source]
----
$ node producer.js
Ghana
Réunion
Guatemala
Luxembourg
Mayotte
Syria
United Kingdom
Bolivia
Haiti
----
+
As shown in the example, the producer component starts to run and generate messages that represent country names.

. Switch back to the first command-line window that you opened.
+
You should now see that the consumer component displays the same country names generated by the producer, and in the same order, as shown in the example.
+
.Example output from the consumer component
[source]
----
$ node consumer.js
Ghana
Réunion
Guatemala
Luxembourg
Mayotte
Syria
United Kingdom
Bolivia
Haiti
----
+
The output from both components confirms that they successfully connected to the Kafka instance. The components are using the Kafka topic that you created to produce and consume messages.

. In your IDE, in the `producer-backend` directory of the repository that you cloned, open the `producer.js` file.
+
Observe that the producer component is configured to process environment variables from the `.env` file that you created. The values of these environment variables are the bootstrap server endpoint and client credentials that the component used to connect to the Kafka instance.

. In the `consumer-backend` directory, open the `consumer.js` file.
+
Observe that the consumer component is also configured to process environment variables from the `.env` file that you created.

ifdef::qs[]
.Verification
* Did the producer component run and start generating random country names?
* Did the consumer component run and display the same country names generated by the producer, and in the same order?
endif::[]

ifdef::qs[]
[#conclusion]
Congratulations! You successfully completed the {product} Node.js quick start. You're now ready to use your own Node.js applications with {product}.
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
